diff --git a/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp b/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
index 39fba6a25..6f244b455 100644
--- a/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
+++ b/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp
@@ -25,9 +25,13 @@
 #include "llvm/Transforms/Vectorize/LoopVectorizationLegality.h"
 #include <algorithm>
 #include <optional>
+#include <string>
+#include "llvm/ADT/SmallString.h"
 using namespace llvm;
 using namespace llvm::PatternMatch;
-
+extern SmallString<256> ttilog;
+#define STR(a) std::to_string(a.getValue().value())
+#define STRI(a) std::to_string(a)
 #define DEBUG_TYPE "aarch64tti"
 
 static cl::opt<bool> EnableFalkorHWPFUnrollFix("enable-falkor-hwpf-unroll-fix",
@@ -4286,8 +4290,42 @@ InstructionCost AArch64TTIImpl::getShuffleCost(
         {TTI::SK_Reverse, MVT::nxv4i1, 1},
         {TTI::SK_Reverse, MVT::nxv2i1, 1},
     };
-    if (const auto *Entry = CostTableLookup(ShuffleTbl, Kind, LT.second))
+    if (const auto *Entry = CostTableLookup(ShuffleTbl, Kind, LT.second)){
+      if (Kind == TTI::SK_Reverse){
+      llvm::outs() << "MVT: " << LT.second << "\n";
+      InstructionCost Cost = LT.first * Entry->Cost;
+      InstructionCost EntryCost = Entry->Cost;
+      if (Cost.isValid()) {
+        ttilog += "-> ShuffleCost(" + STR(Cost) + ") = LT.first("+STR(LT.first) + ") * (" + STR(EntryCost) + ") -> ASM: [";
+        switch (LT.second.SimpleTy) {
+          case MVT::v2i32:
+          case MVT::v2f32:
+          case MVT::v4f16:
+          case MVT::v4i16:
+          case MVT::v8i8:
+            ttilog += "REV64";
+            break;
+          case MVT::v4i32:
+          case MVT::v4f32:
+          case MVT::v8f16:
+          case MVT::v8i16:
+          case MVT::v16i8:
+            ttilog += "REV64, EXT";
+            break;
+          case MVT::v2i64:
+          case MVT::v2f64:
+            ttilog += "EXT";
+            break;
+          default:
+            ttilog += "Unknown";
+        }
+        ttilog += "]";
+      }
+        return Cost;
+
+      }
       return LT.first * Entry->Cost;
+    }
   }
 
   if (Kind == TTI::SK_Splice && isa<ScalableVectorType>(Tp))
diff --git a/llvm/lib/Target/RISCV/RISCVISelLowering.cpp b/llvm/lib/Target/RISCV/RISCVISelLowering.cpp
index 823fb4284..803cf9c82 100644
--- a/llvm/lib/Target/RISCV/RISCVISelLowering.cpp
+++ b/llvm/lib/Target/RISCV/RISCVISelLowering.cpp
@@ -90,8 +90,8 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
   if ((ABI == RISCVABI::ABI_ILP32F || ABI == RISCVABI::ABI_LP64F) &&
       !Subtarget.hasStdExtF()) {
     errs() << "Hard-float 'f' ABI can't be used for a target that "
-                "doesn't support the F instruction set extension (ignoring "
-                          "target-abi)\n";
+              "doesn't support the F instruction set extension (ignoring "
+              "target-abi)\n";
     ABI = Subtarget.is64Bit() ? RISCVABI::ABI_LP64 : RISCVABI::ABI_ILP32;
   } else if ((ABI == RISCVABI::ABI_ILP32D || ABI == RISCVABI::ABI_LP64D) &&
              !Subtarget.hasStdExtD()) {
@@ -158,8 +158,8 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
       MVT::nxv8bf16, MVT::nxv16bf16, MVT::nxv32bf16};
   static const MVT::SimpleValueType F32VecVTs[] = {
       MVT::nxv1f32, MVT::nxv2f32, MVT::nxv4f32, MVT::nxv8f32, MVT::nxv16f32};
-  static const MVT::SimpleValueType F64VecVTs[] = {
-      MVT::nxv1f64, MVT::nxv2f64, MVT::nxv4f64, MVT::nxv8f64};
+  static const MVT::SimpleValueType F64VecVTs[] = {MVT::nxv1f64, MVT::nxv2f64,
+                                                   MVT::nxv4f64, MVT::nxv8f64};
 
   if (Subtarget.hasVInstructions()) {
     auto addRegClassForRVV = [this](MVT VT) {
@@ -311,8 +311,8 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
   }
 
   if (!Subtarget.hasStdExtM()) {
-    setOperationAction({ISD::SDIV, ISD::UDIV, ISD::SREM, ISD::UREM},
-                       XLenVT, Expand);
+    setOperationAction({ISD::SDIV, ISD::UDIV, ISD::SREM, ISD::UREM}, XLenVT,
+                       Expand);
     if (RV64LegalI32 && Subtarget.is64Bit())
       setOperationAction({ISD::SDIV, ISD::UDIV, ISD::SREM, ISD::UREM}, MVT::i32,
                          Promote);
@@ -365,7 +365,6 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
                            ? Promote
                            : Expand);
 
-
   if (Subtarget.hasVendorXCVbitmanip()) {
     setOperationAction(ISD::BITREVERSE, XLenVT, Legal);
   } else {
@@ -439,13 +438,12 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
       ISD::SETUGE, ISD::SETULT, ISD::SETULE, ISD::SETUNE, ISD::SETGT,
       ISD::SETGE,  ISD::SETNE,  ISD::SETO,   ISD::SETUO};
 
-  static const unsigned FPOpToExpand[] = {
-      ISD::FSIN, ISD::FCOS,       ISD::FSINCOS,   ISD::FPOW,
-      ISD::FREM};
+  static const unsigned FPOpToExpand[] = {ISD::FSIN, ISD::FCOS, ISD::FSINCOS,
+                                          ISD::FPOW, ISD::FREM};
 
-  static const unsigned FPRndMode[] = {
-      ISD::FCEIL, ISD::FFLOOR, ISD::FTRUNC, ISD::FRINT, ISD::FROUND,
-      ISD::FROUNDEVEN};
+  static const unsigned FPRndMode[] = {ISD::FCEIL,  ISD::FFLOOR,
+                                       ISD::FTRUNC, ISD::FRINT,
+                                       ISD::FROUND, ISD::FROUNDEVEN};
 
   if (Subtarget.hasStdExtZfhminOrZhinxmin())
     setOperationAction(ISD::BITCAST, MVT::i16, Custom);
@@ -502,10 +500,9 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
 
     setOperationAction(ISD::FNEARBYINT, MVT::f16,
                        Subtarget.hasStdExtZfa() ? Legal : Promote);
-    setOperationAction({ISD::FREM, ISD::FPOW, ISD::FPOWI,
-                        ISD::FCOS, ISD::FSIN, ISD::FSINCOS, ISD::FEXP,
-                        ISD::FEXP2, ISD::FEXP10, ISD::FLOG, ISD::FLOG2,
-                        ISD::FLOG10},
+    setOperationAction({ISD::FREM, ISD::FPOW, ISD::FPOWI, ISD::FCOS, ISD::FSIN,
+                        ISD::FSINCOS, ISD::FEXP, ISD::FEXP2, ISD::FEXP10,
+                        ISD::FLOG, ISD::FLOG2, ISD::FLOG10},
                        MVT::f16, Promote);
 
     // FIXME: Need to promote f16 STRICT_* to f32 libcalls, but we don't have
@@ -672,9 +669,9 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
 
     // RVV intrinsics may have illegal operands.
     // We also need to custom legalize vmv.x.s.
-    setOperationAction({ISD::INTRINSIC_WO_CHAIN, ISD::INTRINSIC_W_CHAIN,
-                        ISD::INTRINSIC_VOID},
-                       {MVT::i8, MVT::i16}, Custom);
+    setOperationAction(
+        {ISD::INTRINSIC_WO_CHAIN, ISD::INTRINSIC_W_CHAIN, ISD::INTRINSIC_VOID},
+        {MVT::i8, MVT::i16}, Custom);
     if (Subtarget.is64Bit())
       setOperationAction({ISD::INTRINSIC_W_CHAIN, ISD::INTRINSIC_VOID},
                          MVT::i32, Custom);
@@ -685,38 +682,89 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
     setOperationAction({ISD::INTRINSIC_W_CHAIN, ISD::INTRINSIC_VOID},
                        MVT::Other, Custom);
 
-    static const unsigned IntegerVPOps[] = {
-        ISD::VP_ADD,         ISD::VP_SUB,         ISD::VP_MUL,
-        ISD::VP_SDIV,        ISD::VP_UDIV,        ISD::VP_SREM,
-        ISD::VP_UREM,        ISD::VP_AND,         ISD::VP_OR,
-        ISD::VP_XOR,         ISD::VP_SRA,         ISD::VP_SRL,
-        ISD::VP_SHL,         ISD::VP_REDUCE_ADD,  ISD::VP_REDUCE_AND,
-        ISD::VP_REDUCE_OR,   ISD::VP_REDUCE_XOR,  ISD::VP_REDUCE_SMAX,
-        ISD::VP_REDUCE_SMIN, ISD::VP_REDUCE_UMAX, ISD::VP_REDUCE_UMIN,
-        ISD::VP_MERGE,       ISD::VP_SELECT,      ISD::VP_FP_TO_SINT,
-        ISD::VP_FP_TO_UINT,  ISD::VP_SETCC,       ISD::VP_SIGN_EXTEND,
-        ISD::VP_ZERO_EXTEND, ISD::VP_TRUNCATE,    ISD::VP_SMIN,
-        ISD::VP_SMAX,        ISD::VP_UMIN,        ISD::VP_UMAX,
-        ISD::VP_ABS, ISD::EXPERIMENTAL_VP_REVERSE, ISD::EXPERIMENTAL_VP_SPLICE,
-        ISD::VP_SADDSAT,     ISD::VP_UADDSAT,     ISD::VP_SSUBSAT,
-        ISD::VP_USUBSAT,     ISD::VP_CTTZ_ELTS,   ISD::VP_CTTZ_ELTS_ZERO_UNDEF,
-        ISD::EXPERIMENTAL_VP_SPLAT};
-
-    static const unsigned FloatingPointVPOps[] = {
-        ISD::VP_FADD,        ISD::VP_FSUB,        ISD::VP_FMUL,
-        ISD::VP_FDIV,        ISD::VP_FNEG,        ISD::VP_FABS,
-        ISD::VP_FMA,         ISD::VP_REDUCE_FADD, ISD::VP_REDUCE_SEQ_FADD,
-        ISD::VP_REDUCE_FMIN, ISD::VP_REDUCE_FMAX, ISD::VP_MERGE,
-        ISD::VP_SELECT,      ISD::VP_SINT_TO_FP,  ISD::VP_UINT_TO_FP,
-        ISD::VP_SETCC,       ISD::VP_FP_ROUND,    ISD::VP_FP_EXTEND,
-        ISD::VP_SQRT,        ISD::VP_FMINNUM,     ISD::VP_FMAXNUM,
-        ISD::VP_FCEIL,       ISD::VP_FFLOOR,      ISD::VP_FROUND,
-        ISD::VP_FROUNDEVEN,  ISD::VP_FCOPYSIGN,   ISD::VP_FROUNDTOZERO,
-        ISD::VP_FRINT,       ISD::VP_FNEARBYINT,  ISD::VP_IS_FPCLASS,
-        ISD::VP_FMINIMUM,    ISD::VP_FMAXIMUM,    ISD::VP_LRINT,
-        ISD::VP_LLRINT,      ISD::EXPERIMENTAL_VP_REVERSE,
-        ISD::EXPERIMENTAL_VP_SPLICE, ISD::VP_REDUCE_FMINIMUM,
-        ISD::VP_REDUCE_FMAXIMUM, ISD::EXPERIMENTAL_VP_SPLAT};
+    static const unsigned IntegerVPOps[] = {ISD::VP_ADD,
+                                            ISD::VP_SUB,
+                                            ISD::VP_MUL,
+                                            ISD::VP_SDIV,
+                                            ISD::VP_UDIV,
+                                            ISD::VP_SREM,
+                                            ISD::VP_UREM,
+                                            ISD::VP_AND,
+                                            ISD::VP_OR,
+                                            ISD::VP_XOR,
+                                            ISD::VP_SRA,
+                                            ISD::VP_SRL,
+                                            ISD::VP_SHL,
+                                            ISD::VP_REDUCE_ADD,
+                                            ISD::VP_REDUCE_AND,
+                                            ISD::VP_REDUCE_OR,
+                                            ISD::VP_REDUCE_XOR,
+                                            ISD::VP_REDUCE_SMAX,
+                                            ISD::VP_REDUCE_SMIN,
+                                            ISD::VP_REDUCE_UMAX,
+                                            ISD::VP_REDUCE_UMIN,
+                                            ISD::VP_MERGE,
+                                            ISD::VP_SELECT,
+                                            ISD::VP_FP_TO_SINT,
+                                            ISD::VP_FP_TO_UINT,
+                                            ISD::VP_SETCC,
+                                            ISD::VP_SIGN_EXTEND,
+                                            ISD::VP_ZERO_EXTEND,
+                                            ISD::VP_TRUNCATE,
+                                            ISD::VP_SMIN,
+                                            ISD::VP_SMAX,
+                                            ISD::VP_UMIN,
+                                            ISD::VP_UMAX,
+                                            ISD::VP_ABS,
+                                            ISD::EXPERIMENTAL_VP_REVERSE,
+                                            ISD::EXPERIMENTAL_VP_SPLICE,
+                                            ISD::VP_SADDSAT,
+                                            ISD::VP_UADDSAT,
+                                            ISD::VP_SSUBSAT,
+                                            ISD::VP_USUBSAT,
+                                            ISD::VP_CTTZ_ELTS,
+                                            ISD::VP_CTTZ_ELTS_ZERO_UNDEF,
+                                            ISD::EXPERIMENTAL_VP_SPLAT};
+
+    static const unsigned FloatingPointVPOps[] = {ISD::VP_FADD,
+                                                  ISD::VP_FSUB,
+                                                  ISD::VP_FMUL,
+                                                  ISD::VP_FDIV,
+                                                  ISD::VP_FNEG,
+                                                  ISD::VP_FABS,
+                                                  ISD::VP_FMA,
+                                                  ISD::VP_REDUCE_FADD,
+                                                  ISD::VP_REDUCE_SEQ_FADD,
+                                                  ISD::VP_REDUCE_FMIN,
+                                                  ISD::VP_REDUCE_FMAX,
+                                                  ISD::VP_MERGE,
+                                                  ISD::VP_SELECT,
+                                                  ISD::VP_SINT_TO_FP,
+                                                  ISD::VP_UINT_TO_FP,
+                                                  ISD::VP_SETCC,
+                                                  ISD::VP_FP_ROUND,
+                                                  ISD::VP_FP_EXTEND,
+                                                  ISD::VP_SQRT,
+                                                  ISD::VP_FMINNUM,
+                                                  ISD::VP_FMAXNUM,
+                                                  ISD::VP_FCEIL,
+                                                  ISD::VP_FFLOOR,
+                                                  ISD::VP_FROUND,
+                                                  ISD::VP_FROUNDEVEN,
+                                                  ISD::VP_FCOPYSIGN,
+                                                  ISD::VP_FROUNDTOZERO,
+                                                  ISD::VP_FRINT,
+                                                  ISD::VP_FNEARBYINT,
+                                                  ISD::VP_IS_FPCLASS,
+                                                  ISD::VP_FMINIMUM,
+                                                  ISD::VP_FMAXIMUM,
+                                                  ISD::VP_LRINT,
+                                                  ISD::VP_LLRINT,
+                                                  ISD::EXPERIMENTAL_VP_REVERSE,
+                                                  ISD::EXPERIMENTAL_VP_SPLICE,
+                                                  ISD::VP_REDUCE_FMINIMUM,
+                                                  ISD::VP_REDUCE_FMAXIMUM,
+                                                  ISD::EXPERIMENTAL_VP_SPLAT};
 
     static const unsigned IntegerVecReduceOps[] = {
         ISD::VECREDUCE_ADD,  ISD::VECREDUCE_AND,  ISD::VECREDUCE_OR,
@@ -953,16 +1001,33 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
         ISD::STRICT_FDIV, ISD::STRICT_FSQRT, ISD::STRICT_FMA};
 
     // TODO: support more vp ops.
-    static const unsigned ZvfhminPromoteVPOps[] = {
-        ISD::VP_FADD,        ISD::VP_FSUB,         ISD::VP_FMUL,
-        ISD::VP_FDIV,        ISD::VP_FNEG,         ISD::VP_FABS,
-        ISD::VP_FMA,         ISD::VP_REDUCE_FADD,  ISD::VP_REDUCE_SEQ_FADD,
-        ISD::VP_REDUCE_FMIN, ISD::VP_REDUCE_FMAX,  ISD::VP_SQRT,
-        ISD::VP_FMINNUM,     ISD::VP_FMAXNUM,      ISD::VP_FCEIL,
-        ISD::VP_FFLOOR,      ISD::VP_FROUND,       ISD::VP_FROUNDEVEN,
-        ISD::VP_FCOPYSIGN,   ISD::VP_FROUNDTOZERO, ISD::VP_FRINT,
-        ISD::VP_FNEARBYINT,  ISD::VP_SETCC,        ISD::VP_FMINIMUM,
-        ISD::VP_FMAXIMUM,    ISD::VP_REDUCE_FMINIMUM, ISD::VP_REDUCE_FMAXIMUM};
+    static const unsigned ZvfhminPromoteVPOps[] = {ISD::VP_FADD,
+                                                   ISD::VP_FSUB,
+                                                   ISD::VP_FMUL,
+                                                   ISD::VP_FDIV,
+                                                   ISD::VP_FNEG,
+                                                   ISD::VP_FABS,
+                                                   ISD::VP_FMA,
+                                                   ISD::VP_REDUCE_FADD,
+                                                   ISD::VP_REDUCE_SEQ_FADD,
+                                                   ISD::VP_REDUCE_FMIN,
+                                                   ISD::VP_REDUCE_FMAX,
+                                                   ISD::VP_SQRT,
+                                                   ISD::VP_FMINNUM,
+                                                   ISD::VP_FMAXNUM,
+                                                   ISD::VP_FCEIL,
+                                                   ISD::VP_FFLOOR,
+                                                   ISD::VP_FROUND,
+                                                   ISD::VP_FROUNDEVEN,
+                                                   ISD::VP_FCOPYSIGN,
+                                                   ISD::VP_FROUNDTOZERO,
+                                                   ISD::VP_FRINT,
+                                                   ISD::VP_FNEARBYINT,
+                                                   ISD::VP_SETCC,
+                                                   ISD::VP_FMINIMUM,
+                                                   ISD::VP_FMAXIMUM,
+                                                   ISD::VP_REDUCE_FMINIMUM,
+                                                   ISD::VP_REDUCE_FMAXIMUM};
 
     // Sets common operation actions on RVV floating-point vector types.
     const auto SetCommonVFPActions = [&](MVT VT) {
@@ -1490,13 +1555,26 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
     setTargetDAGCombine({ISD::ZERO_EXTEND, ISD::FP_TO_SINT, ISD::FP_TO_UINT,
                          ISD::FP_TO_SINT_SAT, ISD::FP_TO_UINT_SAT});
   if (Subtarget.hasVInstructions())
-    setTargetDAGCombine({ISD::FCOPYSIGN, ISD::MGATHER, ISD::MSCATTER,
-                         ISD::VP_GATHER, ISD::VP_SCATTER, ISD::SRA, ISD::SRL,
-                         ISD::SHL, ISD::STORE, ISD::SPLAT_VECTOR,
-                         ISD::BUILD_VECTOR, ISD::CONCAT_VECTORS,
-                         ISD::EXPERIMENTAL_VP_REVERSE, ISD::MUL,
-                         ISD::SDIV, ISD::UDIV, ISD::SREM, ISD::UREM,
-                         ISD::INSERT_VECTOR_ELT, ISD::ABS});
+    setTargetDAGCombine({ISD::FCOPYSIGN,
+                         ISD::MGATHER,
+                         ISD::MSCATTER,
+                         ISD::VP_GATHER,
+                         ISD::VP_SCATTER,
+                         ISD::SRA,
+                         ISD::SRL,
+                         ISD::SHL,
+                         ISD::STORE,
+                         ISD::SPLAT_VECTOR,
+                         ISD::BUILD_VECTOR,
+                         ISD::CONCAT_VECTORS,
+                         ISD::EXPERIMENTAL_VP_REVERSE,
+                         ISD::MUL,
+                         ISD::SDIV,
+                         ISD::UDIV,
+                         ISD::SREM,
+                         ISD::UREM,
+                         ISD::INSERT_VECTOR_ELT,
+                         ISD::ABS});
   if (Subtarget.hasVendorXTHeadMemPair())
     setTargetDAGCombine({ISD::LOAD, ISD::STORE});
   if (Subtarget.useRVVForFixedLengthVectors())
@@ -2076,7 +2154,6 @@ bool RISCVTargetLowering::canSplatOperand(unsigned Opcode, int Operand) const {
   }
 }
 
-
 bool RISCVTargetLowering::canSplatOperand(Instruction *I, int Operand) const {
   if (!I->getType()->isVectorTy() || !Subtarget.hasVInstructions())
     return false;
@@ -2336,8 +2413,8 @@ bool RISCVTargetLowering::isExtractSubvectorCheap(EVT ResVT, EVT SrcVT,
 }
 
 MVT RISCVTargetLowering::getRegisterTypeForCallingConv(LLVMContext &Context,
-                                                      CallingConv::ID CC,
-                                                      EVT VT) const {
+                                                       CallingConv::ID CC,
+                                                       EVT VT) const {
   // Use f32 to pass f16 if it is legal and Zfh/Zfhmin is not enabled.
   // We might still end up using a GPR but that will be decided based on ABI.
   if (VT == MVT::f16 && Subtarget.hasStdExtFOrZfinx() &&
@@ -2352,9 +2429,8 @@ MVT RISCVTargetLowering::getRegisterTypeForCallingConv(LLVMContext &Context,
   return PartVT;
 }
 
-unsigned RISCVTargetLowering::getNumRegistersForCallingConv(LLVMContext &Context,
-                                                           CallingConv::ID CC,
-                                                           EVT VT) const {
+unsigned RISCVTargetLowering::getNumRegistersForCallingConv(
+    LLVMContext &Context, CallingConv::ID CC, EVT VT) const {
   // Use f32 to pass f16 if it is legal and Zfh/Zfhmin is not enabled.
   // We might still end up using a GPR but that will be decided based on ABI.
   if (VT == MVT::f16 && Subtarget.hasStdExtFOrZfinx() &&
@@ -2411,7 +2487,8 @@ static void translateSetCCForBranch(const SDLoc &DL, SDValue &LHS, SDValue &RHS,
   if (auto *RHSC = dyn_cast<ConstantSDNode>(RHS)) {
     int64_t C = RHSC->getSExtValue();
     switch (CC) {
-    default: break;
+    default:
+      break;
     case ISD::SETGT:
       // Convert X > -1 to X >= 0.
       if (C == -1) {
@@ -2491,10 +2568,8 @@ unsigned RISCVTargetLowering::getRegClassIDForLMUL(RISCVII::VLMUL LMul) {
 
 unsigned RISCVTargetLowering::getSubregIndexByMVT(MVT VT, unsigned Index) {
   RISCVII::VLMUL LMUL = getLMUL(VT);
-  if (LMUL == RISCVII::VLMUL::LMUL_F8 ||
-      LMUL == RISCVII::VLMUL::LMUL_F4 ||
-      LMUL == RISCVII::VLMUL::LMUL_F2 ||
-      LMUL == RISCVII::VLMUL::LMUL_1) {
+  if (LMUL == RISCVII::VLMUL::LMUL_F8 || LMUL == RISCVII::VLMUL::LMUL_F4 ||
+      LMUL == RISCVII::VLMUL::LMUL_F2 || LMUL == RISCVII::VLMUL::LMUL_1) {
     static_assert(RISCV::sub_vrm1_7 == RISCV::sub_vrm1_0 + 7,
                   "Unexpected subreg numbering");
     return RISCV::sub_vrm1_0 + Index;
@@ -2584,7 +2659,6 @@ bool RISCVTargetLowering::isLegalElementTypeForRVV(EVT ScalarTy) const {
   }
 }
 
-
 unsigned RISCVTargetLowering::combineRepeatedFPDivisors() const {
   return NumRepeatedDivisors;
 }
@@ -2865,12 +2939,12 @@ InstructionCost RISCVTargetLowering::getLMULCost(MVT VT) const {
     else
       Cost = (LMul * DLenFactor);
   } else {
-    Cost = divideCeil(VT.getSizeInBits(), Subtarget.getRealMinVLen() / DLenFactor);
+    Cost =
+        divideCeil(VT.getSizeInBits(), Subtarget.getRealMinVLen() / DLenFactor);
   }
   return Cost;
 }
 
-
 /// Return the cost of a vrgather.vv instruction for the type VT.  vrgather.vv
 /// is generally quadratic in the number of vreg implied by LMUL.  Note that
 /// operand (index and possibly mask) are handled separately.
@@ -2981,7 +3055,8 @@ static SDValue lowerFP_TO_INT_SAT(SDValue Op, SelectionDAG &DAG,
   // Need to widen by more than 1 step, promote the FP type, then do a widening
   // convert.
   if (DstEltSize > (2 * SrcEltSize)) {
-    assert(SrcContainerVT.getVectorElementType() == MVT::f16 && "Unexpected VT!");
+    assert(SrcContainerVT.getVectorElementType() == MVT::f16 &&
+           "Unexpected VT!");
     MVT InterVT = SrcContainerVT.changeVectorElementType(MVT::f32);
     Src = DAG.getNode(RISCVISD::FP_EXTEND_VL, DL, InterVT, Src, Mask, VL);
   }
@@ -3083,10 +3158,9 @@ lowerVectorFTRUNC_FCEIL_FFLOOR_FROUND(SDValue Op, SelectionDAG &DAG,
 
   // If abs(Src) was larger than MaxVal or nan, keep it.
   MVT SetccVT = MVT::getVectorVT(MVT::i1, ContainerVT.getVectorElementCount());
-  Mask =
-      DAG.getNode(RISCVISD::SETCC_VL, DL, SetccVT,
-                  {Abs, MaxValSplat, DAG.getCondCode(ISD::SETOLT),
-                   Mask, Mask, VL});
+  Mask = DAG.getNode(
+      RISCVISD::SETCC_VL, DL, SetccVT,
+      {Abs, MaxValSplat, DAG.getCondCode(ISD::SETOLT), Mask, Mask, VL});
 
   // Truncate to integer and convert back to FP.
   MVT IntVT = ContainerVT.changeVectorElementTypeToInteger();
@@ -3112,8 +3186,8 @@ lowerVectorFTRUNC_FCEIL_FFLOOR_FROUND(SDValue Op, SelectionDAG &DAG,
     break;
   }
   case ISD::FTRUNC:
-    Truncated = DAG.getNode(RISCVISD::VFCVT_RTZ_X_F_VL, DL, IntVT, Src,
-                            Mask, VL);
+    Truncated =
+        DAG.getNode(RISCVISD::VFCVT_RTZ_X_F_VL, DL, IntVT, Src, Mask, VL);
     break;
   case ISD::FRINT:
   case ISD::VP_FRINT:
@@ -3500,7 +3574,6 @@ static SDValue matchSplatAsGather(SDValue SplatVal, MVT VT, const SDLoc &DL,
   return convertFromScalableVector(VT, Gather, DAG, Subtarget);
 }
 
-
 /// Try and optimize BUILD_VECTORs with "dominant values" - these are values
 /// which constitute a large proportion of the elements. In such cases we can
 /// splat a vector with the dominant element and make up the shortfall with
@@ -3509,8 +3582,9 @@ static SDValue matchSplatAsGather(SDValue SplatVal, MVT VT, const SDLoc &DL,
 /// upper-most element is the "dominant" one, allowing us to use a splat to
 /// "insert" the upper element, and an insert of the lower element at position
 /// 0, which improves codegen.
-static SDValue lowerBuildVectorViaDominantValues(SDValue Op, SelectionDAG &DAG,
-                                                 const RISCVSubtarget &Subtarget) {
+static SDValue
+lowerBuildVectorViaDominantValues(SDValue Op, SelectionDAG &DAG,
+                                  const RISCVSubtarget &Subtarget) {
   MVT VT = Op.getSimpleValueType();
   assert(VT.isFixedLengthVector() && "Unexpected vector!");
 
@@ -3578,8 +3652,8 @@ static SDValue lowerBuildVectorViaDominantValues(SDValue Op, SelectionDAG &DAG,
         !LastOp.isUndef() && ValueCounts[LastOp] == 1 &&
         LastOp != DominantValue) {
       Vec = convertToScalableVector(ContainerVT, Vec, DAG, Subtarget);
-      auto OpCode =
-        VT.isFloatingPoint() ? RISCVISD::VFSLIDE1DOWN_VL : RISCVISD::VSLIDE1DOWN_VL;
+      auto OpCode = VT.isFloatingPoint() ? RISCVISD::VFSLIDE1DOWN_VL
+                                         : RISCVISD::VSLIDE1DOWN_VL;
       if (!VT.isFloatingPoint())
         LastOp = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, LastOp);
       Vec = DAG.getNode(OpCode, DL, ContainerVT, DAG.getUNDEF(ContainerVT), Vec,
@@ -3658,9 +3732,8 @@ static SDValue lowerBuildVectorOfConstants(SDValue Op, SelectionDAG &DAG,
     // Now we can create our integer vector type. Note that it may be larger
     // than the resulting mask type: v4i1 would use v1i8 as its integer type.
     unsigned IntegerViaVecElts = divideCeil(NumElts, NumViaIntegerBits);
-    MVT IntegerViaVecVT =
-      MVT::getVectorVT(MVT::getIntegerVT(NumViaIntegerBits),
-                       IntegerViaVecElts);
+    MVT IntegerViaVecVT = MVT::getVectorVT(MVT::getIntegerVT(NumViaIntegerBits),
+                                           IntegerViaVecElts);
 
     uint64_t Bits = 0;
     unsigned BitPos = 0, IntegerEltIdx = 0;
@@ -3707,8 +3780,8 @@ static SDValue lowerBuildVectorOfConstants(SDValue Op, SelectionDAG &DAG,
   }
 
   if (SDValue Splat = cast<BuildVectorSDNode>(Op)->getSplatValue()) {
-    unsigned Opc = VT.isFloatingPoint() ? RISCVISD::VFMV_V_F_VL
-                                        : RISCVISD::VMV_V_X_VL;
+    unsigned Opc =
+        VT.isFloatingPoint() ? RISCVISD::VFMV_V_F_VL : RISCVISD::VMV_V_X_VL;
     if (!VT.isFloatingPoint())
       Splat = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, Splat);
     Splat =
@@ -3790,7 +3863,8 @@ static SDValue lowerBuildVectorOfConstants(SDValue Op, SelectionDAG &DAG,
     // means we only have a VTYPE toggle, not a VL toggle.  TODO: Should this
     // be moved into InsertVSETVLI?
     unsigned ViaVecLen =
-      (Subtarget.getRealMinVLen() >= VT.getSizeInBits() * NumElts) ? NumElts : 1;
+        (Subtarget.getRealMinVLen() >= VT.getSizeInBits() * NumElts) ? NumElts
+                                                                     : 1;
     MVT ViaVecVT = MVT::getVectorVT(ViaIntVT, ViaVecLen);
 
     uint64_t EltMask = maskTrailingOnes<uint64_t>(EltBitSize);
@@ -3813,13 +3887,12 @@ static SDValue lowerBuildVectorOfConstants(SDValue Op, SelectionDAG &DAG,
                               DAG.getConstant(SplatValue, DL, XLenVT),
                               DAG.getVectorIdxConstant(0, DL));
     if (ViaVecLen != 1)
-      Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL,
-                        MVT::getVectorVT(ViaIntVT, 1), Vec,
-                        DAG.getConstant(0, DL, XLenVT));
+      Vec =
+          DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, MVT::getVectorVT(ViaIntVT, 1),
+                      Vec, DAG.getConstant(0, DL, XLenVT));
     return DAG.getBitcast(VT, Vec);
   }
 
-
   // Attempt to detect "hidden" splats, which only reveal themselves as splats
   // when re-interpreted as a vector with a larger element type. For example,
   //   v4i16 = build_vector i16 0, i16 1, i16 0, i16 1
@@ -3844,8 +3917,9 @@ static SDValue lowerBuildVectorOfConstants(SDValue Op, SelectionDAG &DAG,
     // be moved into InsertVSETVLI?
     const unsigned RequiredVL = NumElts / SeqLen;
     const unsigned ViaVecLen =
-      (Subtarget.getRealMinVLen() >= ViaIntVT.getSizeInBits() * NumElts) ?
-      NumElts : RequiredVL;
+        (Subtarget.getRealMinVLen() >= ViaIntVT.getSizeInBits() * NumElts)
+            ? NumElts
+            : RequiredVL;
     MVT ViaVecVT = MVT::getVectorVT(ViaIntVT, ViaVecLen);
 
     unsigned EltIdx = 0;
@@ -3901,9 +3975,10 @@ static SDValue lowerBuildVectorOfConstants(SDValue Op, SelectionDAG &DAG,
     if (EltBitSize - SignBits < 8) {
       SDValue Source = DAG.getBuildVector(VT.changeVectorElementType(MVT::i8),
                                           DL, Op->ops());
-      Source = convertToScalableVector(ContainerVT.changeVectorElementType(MVT::i8),
-                                       Source, DAG, Subtarget);
-      SDValue Res = DAG.getNode(RISCVISD::VSEXT_VL, DL, ContainerVT, Source, Mask, VL);
+      Source = convertToScalableVector(
+          ContainerVT.changeVectorElementType(MVT::i8), Source, DAG, Subtarget);
+      SDValue Res =
+          DAG.getNode(RISCVISD::VSEXT_VL, DL, ContainerVT, Source, Mask, VL);
       return convertFromScalableVector(VT, Res, DAG, Subtarget);
     }
   }
@@ -4012,8 +4087,7 @@ static SDValue lowerBUILD_VECTOR(SDValue Op, SelectionDAG &DAG,
   assert(VT.isFixedLengthVector() && "Unexpected vector!");
 
   // If we don't have scalar f16, we need to bitcast to an i16 vector.
-  if (VT.getVectorElementType() == MVT::f16 &&
-      !Subtarget.hasStdExtZfhmin())
+  if (VT.getVectorElementType() == MVT::f16 && !Subtarget.hasStdExtZfhmin())
     return lowerBUILD_VECTORvXf16(Op, DAG);
 
   if (ISD::isBuildVectorOfConstantSDNodes(Op.getNode()) ||
@@ -4054,8 +4128,8 @@ static SDValue lowerBUILD_VECTOR(SDValue Op, SelectionDAG &DAG,
   if (SDValue Splat = cast<BuildVectorSDNode>(Op)->getSplatValue()) {
     if (auto Gather = matchSplatAsGather(Splat, VT, DL, DAG, Subtarget))
       return Gather;
-    unsigned Opc = VT.isFloatingPoint() ? RISCVISD::VFMV_V_F_VL
-                                        : RISCVISD::VMV_V_X_VL;
+    unsigned Opc =
+        VT.isFloatingPoint() ? RISCVISD::VFMV_V_F_VL : RISCVISD::VMV_V_X_VL;
     if (!VT.isFloatingPoint())
       Splat = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, Splat);
     Splat =
@@ -4156,7 +4230,8 @@ static SDValue lowerBUILD_VECTOR(SDValue Op, SelectionDAG &DAG,
   // TODO: unify with TTI getSlideCost.
   InstructionCost PerSlideCost = 1;
   switch (RISCVTargetLowering::getLMUL(ContainerVT)) {
-  default: break;
+  default:
+    break;
   case RISCVII::VLMUL::LMUL_2:
     PerSlideCost = 2;
     break;
@@ -4214,22 +4289,26 @@ static SDValue lowerBUILD_VECTOR(SDValue Op, SelectionDAG &DAG,
     }
 
     if (UndefCount) {
-      const SDValue Offset = DAG.getConstant(UndefCount, DL, Subtarget.getXLenVT());
-      Vec = getVSlidedown(DAG, Subtarget, DL, ContainerVT, DAG.getUNDEF(ContainerVT),
-                          Vec, Offset, Mask, VL, Policy);
+      const SDValue Offset =
+          DAG.getConstant(UndefCount, DL, Subtarget.getXLenVT());
+      Vec = getVSlidedown(DAG, Subtarget, DL, ContainerVT,
+                          DAG.getUNDEF(ContainerVT), Vec, Offset, Mask, VL,
+                          Policy);
       UndefCount = 0;
     }
-    auto OpCode =
-      VT.isFloatingPoint() ? RISCVISD::VFSLIDE1DOWN_VL : RISCVISD::VSLIDE1DOWN_VL;
+    auto OpCode = VT.isFloatingPoint() ? RISCVISD::VFSLIDE1DOWN_VL
+                                       : RISCVISD::VSLIDE1DOWN_VL;
     if (!VT.isFloatingPoint())
       V = DAG.getNode(ISD::ANY_EXTEND, DL, Subtarget.getXLenVT(), V);
     Vec = DAG.getNode(OpCode, DL, ContainerVT, DAG.getUNDEF(ContainerVT), Vec,
                       V, Mask, VL);
   }
   if (UndefCount) {
-    const SDValue Offset = DAG.getConstant(UndefCount, DL, Subtarget.getXLenVT());
-    Vec = getVSlidedown(DAG, Subtarget, DL, ContainerVT, DAG.getUNDEF(ContainerVT),
-                        Vec, Offset, Mask, VL, Policy);
+    const SDValue Offset =
+        DAG.getConstant(UndefCount, DL, Subtarget.getXLenVT());
+    Vec =
+        getVSlidedown(DAG, Subtarget, DL, ContainerVT,
+                      DAG.getUNDEF(ContainerVT), Vec, Offset, Mask, VL, Policy);
   }
   return convertFromScalableVector(VT, Vec, DAG, Subtarget);
 }
@@ -4370,27 +4449,26 @@ static SDValue lowerScalarInsert(SDValue Scalar, SDValue VL, MVT VT,
     }
   }
 
-
   if (VT.isFloatingPoint())
-    return DAG.getNode(RISCVISD::VFMV_S_F_VL, DL, VT,
-                       DAG.getUNDEF(VT), Scalar, VL);
+    return DAG.getNode(RISCVISD::VFMV_S_F_VL, DL, VT, DAG.getUNDEF(VT), Scalar,
+                       VL);
 
   // Avoid the tricky legalization cases by falling back to using the
   // splat code which already handles it gracefully.
   if (!Scalar.getValueType().bitsLE(XLenVT))
     return lowerScalarSplat(DAG.getUNDEF(VT), Scalar,
-                            DAG.getConstant(1, DL, XLenVT),
-                            VT, DL, DAG, Subtarget);
+                            DAG.getConstant(1, DL, XLenVT), VT, DL, DAG,
+                            Subtarget);
 
   // If the operand is a constant, sign extend to increase our chances
   // of being able to use a .vi instruction. ANY_EXTEND would become a
   // a zero extend and the simm5 check in isel would fail.
   // FIXME: Should we ignore the upper bits in isel instead?
   unsigned ExtOpc =
-    isa<ConstantSDNode>(Scalar) ? ISD::SIGN_EXTEND : ISD::ANY_EXTEND;
+      isa<ConstantSDNode>(Scalar) ? ISD::SIGN_EXTEND : ISD::ANY_EXTEND;
   Scalar = DAG.getNode(ExtOpc, DL, XLenVT, Scalar);
-  return DAG.getNode(RISCVISD::VMV_S_X_VL, DL, VT,
-                     DAG.getUNDEF(VT), Scalar, VL);
+  return DAG.getNode(RISCVISD::VMV_S_X_VL, DL, VT, DAG.getUNDEF(VT), Scalar,
+                     VL);
 }
 
 // Is this a shuffle extracts either the even or odd elements of a vector?
@@ -4711,7 +4789,8 @@ static SDValue lowerVECTOR_SHUFFLEAsVSlideup(const SDLoc &DL, MVT VT,
   auto TrueMask = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget).first;
   // We slide up by the index that the subvector is being inserted at, and set
   // VL to the index + the number of elements being inserted.
-  unsigned Policy = RISCVII::TAIL_UNDISTURBED_MASK_UNDISTURBED | RISCVII::MASK_AGNOSTIC;
+  unsigned Policy =
+      RISCVII::TAIL_UNDISTURBED_MASK_UNDISTURBED | RISCVII::MASK_AGNOSTIC;
   // If the we're adding a suffix to the in place vector, i.e. inserting right
   // up to the very end of it, then we don't actually care about the tail.
   if (NumSubElts + Index >= (int)NumElts)
@@ -4735,9 +4814,8 @@ static SDValue lowerVECTOR_SHUFFLEAsVSlideup(const SDLoc &DL, MVT VT,
 
 /// Match v(f)slide1up/down idioms.  These operations involve sliding
 /// N-1 elements to make room for an inserted scalar at one end.
-static SDValue lowerVECTOR_SHUFFLEAsVSlide1(const SDLoc &DL, MVT VT,
-                                            SDValue V1, SDValue V2,
-                                            ArrayRef<int> Mask,
+static SDValue lowerVECTOR_SHUFFLEAsVSlide1(const SDLoc &DL, MVT VT, SDValue V1,
+                                            SDValue V2, ArrayRef<int> Mask,
                                             const RISCVSubtarget &Subtarget,
                                             SelectionDAG &DAG) {
   bool OpsSwapped = false;
@@ -4768,21 +4846,23 @@ static SDValue lowerVECTOR_SHUFFLEAsVSlide1(const SDLoc &DL, MVT VT,
     return SDValue();
 
   const int InsertIdx = Mask[IsVSlidedown ? (NumElts - 1) : 0];
-  // Inserted lane must come from splat, undef scalar is legal but not profitable.
+  // Inserted lane must come from splat, undef scalar is legal but not
+  // profitable.
   if (InsertIdx < 0 || InsertIdx / NumElts != (unsigned)OpsSwapped)
     return SDValue();
 
   MVT ContainerVT = getContainerForFixedLengthVector(DAG, VT, Subtarget);
   auto [TrueMask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);
-  auto OpCode = IsVSlidedown ?
-    (VT.isFloatingPoint() ? RISCVISD::VFSLIDE1DOWN_VL : RISCVISD::VSLIDE1DOWN_VL) :
-    (VT.isFloatingPoint() ? RISCVISD::VFSLIDE1UP_VL : RISCVISD::VSLIDE1UP_VL);
+  auto OpCode = IsVSlidedown ? (VT.isFloatingPoint() ? RISCVISD::VFSLIDE1DOWN_VL
+                                                     : RISCVISD::VSLIDE1DOWN_VL)
+                             : (VT.isFloatingPoint() ? RISCVISD::VFSLIDE1UP_VL
+                                                     : RISCVISD::VSLIDE1UP_VL);
   if (!VT.isFloatingPoint())
     Splat = DAG.getNode(ISD::ANY_EXTEND, DL, Subtarget.getXLenVT(), Splat);
-  auto Vec = DAG.getNode(OpCode, DL, ContainerVT,
-                         DAG.getUNDEF(ContainerVT),
-                         convertToScalableVector(ContainerVT, V2, DAG, Subtarget),
-                         Splat, TrueMask, VL);
+  auto Vec =
+      DAG.getNode(OpCode, DL, ContainerVT, DAG.getUNDEF(ContainerVT),
+                  convertToScalableVector(ContainerVT, V2, DAG, Subtarget),
+                  Splat, TrueMask, VL);
   return convertFromScalableVector(VT, Vec, DAG, Subtarget);
 }
 
@@ -4941,10 +5021,9 @@ static SDValue lowerBitreverseShuffle(ShuffleVectorSDNode *SVN,
   return Res;
 }
 
-static bool isLegalBitRotate(ShuffleVectorSDNode *SVN,
-                             SelectionDAG &DAG,
-                             const RISCVSubtarget &Subtarget,
-                             MVT &RotateVT, unsigned &RotateAmt) {
+static bool isLegalBitRotate(ShuffleVectorSDNode *SVN, SelectionDAG &DAG,
+                             const RISCVSubtarget &Subtarget, MVT &RotateVT,
+                             unsigned &RotateAmt) {
   SDLoc DL(SVN);
 
   EVT VT = SVN->getValueType(0);
@@ -4955,7 +5034,7 @@ static bool isLegalBitRotate(ShuffleVectorSDNode *SVN,
                                           NumElts, NumSubElts, RotateAmt))
     return false;
   RotateVT = MVT::getVectorVT(MVT::getIntegerVT(EltSizeInBits * NumSubElts),
-                                  NumElts / NumSubElts);
+                              NumElts / NumSubElts);
 
   // We might have a RotateVT that isn't legal, e.g. v4i64 on zve32x.
   return Subtarget.getTargetLowering()->isTypeLegal(RotateVT);
@@ -5019,8 +5098,7 @@ static SDValue lowerShuffleViaVRegSplitting(ShuffleVectorSDNode *SVN,
   unsigned ElemsPerVReg = *VLen / ElemVT.getFixedSizeInBits();
   unsigned VRegsPerSrc = NumElts / ElemsPerVReg;
 
-  SmallVector<std::pair<int, SmallVector<int>>>
-    OutMasks(VRegsPerSrc, {-1, {}});
+  SmallVector<std::pair<int, SmallVector<int>>> OutMasks(VRegsPerSrc, {-1, {}});
 
   // Check if our mask can be done as a 1-to-1 mapping from source
   // to destination registers in the group without needing to
@@ -5056,7 +5134,7 @@ static SDValue lowerShuffleViaVRegSplitting(ShuffleVectorSDNode *SVN,
   // to avoid DAG combining it back to a large shuffle_vector again.
   V1 = convertToScalableVector(ContainerVT, V1, DAG, Subtarget);
   V2 = convertToScalableVector(ContainerVT, V2, DAG, Subtarget);
-  for (unsigned DstVecIdx = 0 ; DstVecIdx < OutMasks.size(); DstVecIdx++) {
+  for (unsigned DstVecIdx = 0; DstVecIdx < OutMasks.size(); DstVecIdx++) {
     auto &[SrcVecIdx, SrcSubMask] = OutMasks[DstVecIdx];
     if (SrcVecIdx == -1)
       continue;
@@ -5097,7 +5175,8 @@ static SDValue lowerVECTOR_SHUFFLE(SDValue Op, SelectionDAG &DAG,
     V1 = DAG.getNode(ISD::ZERO_EXTEND, DL, WidenVT, V1);
     V2 = V2.isUndef() ? DAG.getUNDEF(WidenVT)
                       : DAG.getNode(ISD::ZERO_EXTEND, DL, WidenVT, V2);
-    SDValue Shuffled = DAG.getVectorShuffle(WidenVT, DL, V1, V2, SVN->getMask());
+    SDValue Shuffled =
+        DAG.getVectorShuffle(WidenVT, DL, V1, V2, SVN->getMask());
     return DAG.getSetCC(DL, VT, Shuffled, DAG.getConstant(0, DL, WidenVT),
                         ISD::SETNE);
   }
@@ -5278,7 +5357,6 @@ static SDValue lowerVECTOR_SHUFFLE(SDValue Op, SelectionDAG &DAG,
     return getWideningInterleave(EvenV, OddV, DL, DAG, Subtarget);
   }
 
-
   // Handle any remaining single source shuffles
   assert(!V1.isUndef() && "Unexpected shuffle canonicalization");
   if (V2.isUndef()) {
@@ -5302,8 +5380,8 @@ static SDValue lowerVECTOR_SHUFFLE(SDValue Op, SelectionDAG &DAG,
     unsigned GatherVVOpc = RISCVISD::VRGATHER_VV_VL;
     MVT IndexVT = VT.changeTypeToInteger();
     // Since we can't introduce illegal index types at this stage, use i16 and
-    // vrgatherei16 if the corresponding index type for plain vrgather is greater
-    // than XLenVT.
+    // vrgatherei16 if the corresponding index type for plain vrgather is
+    // greater than XLenVT.
     if (IndexVT.getScalarType().bitsGT(XLenVT)) {
       GatherVVOpc = RISCVISD::VRGATHEREI16_VV_VL;
       IndexVT = IndexVT.changeVectorElementType(MVT::i16);
@@ -5319,19 +5397,19 @@ static SDValue lowerVECTOR_SHUFFLE(SDValue Op, SelectionDAG &DAG,
     }
 
     MVT IndexContainerVT =
-      ContainerVT.changeVectorElementType(IndexVT.getScalarType());
+        ContainerVT.changeVectorElementType(IndexVT.getScalarType());
 
     V1 = convertToScalableVector(ContainerVT, V1, DAG, Subtarget);
     SmallVector<SDValue> GatherIndicesLHS;
     for (int MaskIndex : Mask) {
       bool IsLHSIndex = MaskIndex < (int)NumElts && MaskIndex >= 0;
       GatherIndicesLHS.push_back(IsLHSIndex
-                                 ? DAG.getConstant(MaskIndex, DL, XLenVT)
-                                 : DAG.getUNDEF(XLenVT));
+                                     ? DAG.getConstant(MaskIndex, DL, XLenVT)
+                                     : DAG.getUNDEF(XLenVT));
     }
     SDValue LHSIndices = DAG.getBuildVector(IndexVT, DL, GatherIndicesLHS);
-    LHSIndices = convertToScalableVector(IndexContainerVT, LHSIndices, DAG,
-                                         Subtarget);
+    LHSIndices =
+        convertToScalableVector(IndexContainerVT, LHSIndices, DAG, Subtarget);
     SDValue Gather = DAG.getNode(GatherVVOpc, DL, ContainerVT, V1, LHSIndices,
                                  DAG.getUNDEF(ContainerVT), TrueMask, VL);
     return convertFromScalableVector(VT, Gather, DAG, Subtarget);
@@ -5345,14 +5423,15 @@ static SDValue lowerVECTOR_SHUFFLE(SDValue Op, SelectionDAG &DAG,
   // Construct the appropriate indices into each vector.
   for (int MaskIndex : Mask) {
     bool IsLHSOrUndefIndex = MaskIndex < (int)NumElts;
-    ShuffleMaskLHS.push_back(IsLHSOrUndefIndex && MaskIndex >= 0
-                             ? MaskIndex : -1);
+    ShuffleMaskLHS.push_back(IsLHSOrUndefIndex && MaskIndex >= 0 ? MaskIndex
+                                                                 : -1);
     ShuffleMaskRHS.push_back(IsLHSOrUndefIndex ? -1 : (MaskIndex - NumElts));
   }
 
   // Try to pick a profitable operand order.
   bool SwapOps = DAG.isSplatValue(V2) && !DAG.isSplatValue(V1);
-  SwapOps = SwapOps ^ ShuffleVectorInst::isIdentityMask(ShuffleMaskRHS, NumElts);
+  SwapOps =
+      SwapOps ^ ShuffleVectorInst::isIdentityMask(ShuffleMaskRHS, NumElts);
 
   // Recursively invoke lowering for each operand if we had two
   // independent single source shuffles, and then combine the result via a
@@ -5733,8 +5812,8 @@ static SDValue lowerSADDO_SSUBO(SDValue Op, SelectionDAG &DAG) {
   SDValue Res = DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, WideOp);
   SDValue SExt = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, MVT::i64, WideOp,
                              DAG.getValueType(MVT::i32));
-  SDValue Ovf = DAG.getSetCC(DL, Op.getValue(1).getValueType(), WideOp, SExt,
-                             ISD::SETNE);
+  SDValue Ovf =
+      DAG.getSetCC(DL, Op.getValue(1).getValueType(), WideOp, SExt, ISD::SETNE);
   return DAG.getMergeValues({Res, Ovf}, DL);
 }
 
@@ -5749,8 +5828,8 @@ static SDValue lowerSMULO(SDValue Op, SelectionDAG &DAG) {
   SDValue Res = DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Mul);
   SDValue SExt = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, MVT::i64, Mul,
                              DAG.getValueType(MVT::i32));
-  SDValue Ovf = DAG.getSetCC(DL, Op.getValue(1).getValueType(), Mul, SExt,
-                             ISD::SETNE);
+  SDValue Ovf =
+      DAG.getSetCC(DL, Op.getValue(1).getValueType(), Mul, SExt, ISD::SETNE);
   return DAG.getMergeValues({Res, Ovf}, DL);
 }
 
@@ -6304,8 +6383,7 @@ SDValue RISCVTargetLowering::LowerOperation(SDValue Op,
       SDValue FPConv = DAG.getNode(RISCVISD::FMV_H_X, DL, MVT::f16, NewOp0);
       return FPConv;
     }
-    if (VT == MVT::bf16 && Op0VT == MVT::i16 &&
-        Subtarget.hasStdExtZfbfmin()) {
+    if (VT == MVT::bf16 && Op0VT == MVT::i16 && Subtarget.hasStdExtZfbfmin()) {
       SDValue NewOp0 = DAG.getNode(ISD::ANY_EXTEND, DL, XLenVT, Op0);
       SDValue FPConv = DAG.getNode(RISCVISD::FMV_H_X, DL, MVT::bf16, NewOp0);
       return FPConv;
@@ -6683,7 +6761,8 @@ SDValue RISCVTargetLowering::LowerOperation(SDValue Op,
 
     MVT ContainerVT = getContainerForFixedLengthVector(VT);
     MVT SrcContainerVT = getContainerForFixedLengthVector(SrcVT);
-    assert(ContainerVT.getVectorElementCount() == SrcContainerVT.getVectorElementCount() &&
+    assert(ContainerVT.getVectorElementCount() ==
+               SrcContainerVT.getVectorElementCount() &&
            "Expected same element count");
 
     auto [Mask, VL] = getDefaultVLOps(VT, ContainerVT, DL, DAG, Subtarget);
@@ -6830,6 +6909,7 @@ SDValue RISCVTargetLowering::LowerOperation(SDValue Op,
   case ISD::STEP_VECTOR:
     return lowerSTEP_VECTOR(Op, DAG);
   case ISD::VECTOR_REVERSE:
+    // return lowerVECTOR_REVERSE_MTK(Op, DAG);
     return lowerVECTOR_REVERSE(Op, DAG);
   case ISD::VECTOR_SPLICE:
     return lowerVECTOR_SPLICE(Op, DAG);
@@ -7251,8 +7331,8 @@ SDValue RISCVTargetLowering::LowerOperation(SDValue Op,
     return lowerFMAXIMUM_FMINIMUM(Op, DAG, Subtarget);
   case ISD::EXPERIMENTAL_VP_SPLICE:
     return lowerVPSpliceExperimental(Op, DAG);
-  case ISD::EXPERIMENTAL_VP_REVERSE:
-    return lowerVPReverseExperimental(Op, DAG);
+  // case ISD::EXPERIMENTAL_VP_REVERSE:
+  //   return lowerVPReverseExperimental(Op, DAG);
   case ISD::EXPERIMENTAL_VP_SPLAT:
     return lowerVPSplatExperimental(Op, DAG);
   case ISD::CLEAR_CACHE: {
@@ -7301,8 +7381,8 @@ static SDValue getTargetNode(JumpTableSDNode *N, const SDLoc &DL, EVT Ty,
 }
 
 template <class NodeTy>
-SDValue RISCVTargetLowering::getAddr(NodeTy *N, SelectionDAG &DAG,
-                                     bool IsLocal, bool IsExternWeak) const {
+SDValue RISCVTargetLowering::getAddr(NodeTy *N, SelectionDAG &DAG, bool IsLocal,
+                                     bool IsExternWeak) const {
   SDLoc DL(N);
   EVT Ty = getPointerTy(DAG.getDataLayout());
 
@@ -7570,15 +7650,15 @@ static SDValue combineSelectToBinOp(SDNode *N, SelectionDAG &DAG,
     }
     // (select c, y, -1) -> (c-1) | y
     if (isAllOnesConstant(FalseV)) {
-      SDValue Neg = DAG.getNode(ISD::ADD, DL, VT, CondV,
-                                DAG.getAllOnesConstant(DL, VT));
+      SDValue Neg =
+          DAG.getNode(ISD::ADD, DL, VT, CondV, DAG.getAllOnesConstant(DL, VT));
       return DAG.getNode(ISD::OR, DL, VT, Neg, DAG.getFreeze(TrueV));
     }
 
     // (select c, 0, y) -> (c-1) & y
     if (isNullConstant(TrueV)) {
-      SDValue Neg = DAG.getNode(ISD::ADD, DL, VT, CondV,
-                                DAG.getAllOnesConstant(DL, VT));
+      SDValue Neg =
+          DAG.getNode(ISD::ADD, DL, VT, CondV, DAG.getAllOnesConstant(DL, VT));
       return DAG.getNode(ISD::AND, DL, VT, Neg, DAG.getFreeze(FalseV));
     }
     // (select c, y, 0) -> -c & y
@@ -8473,8 +8553,8 @@ SDValue RISCVTargetLowering::lowerINSERT_VECTOR_ELT(SDValue Op,
   if (auto *IdxC = dyn_cast<ConstantSDNode>(Idx)) {
     const unsigned OrigIdx = IdxC->getZExtValue();
     // Do we know an upper bound on LMUL?
-    if (auto ShrunkVT = getSmallestVTForIndex(ContainerVT, OrigIdx,
-                                              DL, DAG, Subtarget)) {
+    if (auto ShrunkVT =
+            getSmallestVTForIndex(ContainerVT, OrigIdx, DL, DAG, Subtarget)) {
       ContainerVT = *ShrunkVT;
       AlignedIdx = DAG.getVectorIdxConstant(0, DL);
     }
@@ -8483,8 +8563,7 @@ SDValue RISCVTargetLowering::lowerINSERT_VECTOR_ELT(SDValue Op,
     // the insert in m1 as we can determine the register corresponding to
     // the index in the register group.
     const MVT M1VT = getLMUL1VT(ContainerVT);
-    if (auto VLEN = Subtarget.getRealVLen();
-        VLEN && ContainerVT.bitsGT(M1VT)) {
+    if (auto VLEN = Subtarget.getRealVLen(); VLEN && ContainerVT.bitsGT(M1VT)) {
       EVT ElemVT = VecVT.getVectorElementType();
       unsigned ElemsPerVReg = *VLEN / ElemVT.getFixedSizeInBits();
       unsigned RemIdx = OrigIdx % ElemsPerVReg;
@@ -8497,8 +8576,8 @@ SDValue RISCVTargetLowering::lowerINSERT_VECTOR_ELT(SDValue Op,
     }
 
     if (AlignedIdx)
-      Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, ContainerVT, Vec,
-                        AlignedIdx);
+      Vec =
+          DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, ContainerVT, Vec, AlignedIdx);
   }
 
   MVT XLenVT = Subtarget.getXLenVT();
@@ -8553,20 +8632,19 @@ SDValue RISCVTargetLowering::lowerINSERT_VECTOR_ELT(SDValue Op,
     if (isNullConstant(Idx)) {
       // First slide in the lo value, then the hi in above it. We use slide1down
       // to avoid the register group overlap constraint of vslide1up.
-      ValInVec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32ContainerVT,
-                             Vec, Vec, ValLo, I32Mask, InsertI64VL);
+      ValInVec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32ContainerVT, Vec,
+                             Vec, ValLo, I32Mask, InsertI64VL);
       // If the source vector is undef don't pass along the tail elements from
       // the previous slide1down.
       SDValue Tail = Vec.isUndef() ? Vec : ValInVec;
-      ValInVec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32ContainerVT,
-                             Tail, ValInVec, ValHi, I32Mask, InsertI64VL);
+      ValInVec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32ContainerVT, Tail,
+                             ValInVec, ValHi, I32Mask, InsertI64VL);
       // Bitcast back to the right container type.
       ValInVec = DAG.getBitcast(ContainerVT, ValInVec);
 
       if (AlignedIdx)
-        ValInVec =
-            DAG.getNode(ISD::INSERT_SUBVECTOR, DL, OrigContainerVT, OrigVec,
-                        ValInVec, AlignedIdx);
+        ValInVec = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, OrigContainerVT,
+                               OrigVec, ValInVec, AlignedIdx);
       if (!VecVT.isFixedLengthVector())
         return ValInVec;
       return convertFromScalableVector(VecVT, ValInVec, DAG, Subtarget);
@@ -8574,10 +8652,10 @@ SDValue RISCVTargetLowering::lowerINSERT_VECTOR_ELT(SDValue Op,
 
     // First slide in the lo value, then the hi in above it. We use slide1down
     // to avoid the register group overlap constraint of vslide1up.
-    ValInVec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32ContainerVT,
-                           DAG.getUNDEF(I32ContainerVT),
-                           DAG.getUNDEF(I32ContainerVT), ValLo,
-                           I32Mask, InsertI64VL);
+    ValInVec =
+        DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32ContainerVT,
+                    DAG.getUNDEF(I32ContainerVT), DAG.getUNDEF(I32ContainerVT),
+                    ValLo, I32Mask, InsertI64VL);
     ValInVec = DAG.getNode(RISCVISD::VSLIDE1DOWN_VL, DL, I32ContainerVT,
                            DAG.getUNDEF(I32ContainerVT), ValInVec, ValHi,
                            I32Mask, InsertI64VL);
@@ -8701,7 +8779,7 @@ SDValue RISCVTargetLowering::lowerEXTRACT_VECTOR_ELT(SDValue Op,
     unsigned RemIdx = OrigIdx % ElemsPerVReg;
     unsigned SubRegIdx = OrigIdx / ElemsPerVReg;
     unsigned ExtractIdx =
-      SubRegIdx * M1VT.getVectorElementCount().getKnownMinValue();
+        SubRegIdx * M1VT.getVectorElementCount().getKnownMinValue();
     Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, M1VT, Vec,
                       DAG.getVectorIdxConstant(ExtractIdx, DL));
     Idx = DAG.getVectorIdxConstant(RemIdx, DL);
@@ -8859,8 +8937,8 @@ static SDValue lowerVectorIntrinsicScalars(SDValue Op, SelectionDAG &DAG,
         SDValue LMUL = DAG.getConstant(Lmul, DL, XLenVT);
         unsigned Sew = RISCVVType::encodeSEW(I32VT.getScalarSizeInBits());
         SDValue SEW = DAG.getConstant(Sew, DL, XLenVT);
-        SDValue SETVLMAX = DAG.getTargetConstant(
-            Intrinsic::riscv_vsetvlimax, DL, MVT::i32);
+        SDValue SETVLMAX =
+            DAG.getTargetConstant(Intrinsic::riscv_vsetvlimax, DL, MVT::i32);
         I32VL = DAG.getNode(ISD::INTRINSIC_WO_CHAIN, DL, XLenVT, SETVLMAX, SEW,
                             LMUL);
       } else {
@@ -9069,8 +9147,8 @@ static void processVCIXOperands(SDValue &OrigOp,
 // LMUL * VLEN should be greater than or equal to EGS * SEW
 static inline bool isValidEGW(int EGS, EVT VT,
                               const RISCVSubtarget &Subtarget) {
-  return (Subtarget.getRealMinVLen() *
-             VT.getSizeInBits().getKnownMinValue()) / RISCV::RVVBitsPerBlock >=
+  return (Subtarget.getRealMinVLen() * VT.getSizeInBits().getKnownMinValue()) /
+             RISCV::RVVBitsPerBlock >=
          EGS * VT.getScalarSizeInBits();
 }
 
@@ -9097,14 +9175,30 @@ SDValue RISCVTargetLowering::LowerINTRINSIC_WO_CHAIN(SDValue Op,
   case Intrinsic::riscv_sm3p1: {
     unsigned Opc;
     switch (IntNo) {
-    case Intrinsic::riscv_orc_b:      Opc = RISCVISD::ORC_B;      break;
-    case Intrinsic::riscv_brev8:      Opc = RISCVISD::BREV8;      break;
-    case Intrinsic::riscv_sha256sig0: Opc = RISCVISD::SHA256SIG0; break;
-    case Intrinsic::riscv_sha256sig1: Opc = RISCVISD::SHA256SIG1; break;
-    case Intrinsic::riscv_sha256sum0: Opc = RISCVISD::SHA256SUM0; break;
-    case Intrinsic::riscv_sha256sum1: Opc = RISCVISD::SHA256SUM1; break;
-    case Intrinsic::riscv_sm3p0:      Opc = RISCVISD::SM3P0;      break;
-    case Intrinsic::riscv_sm3p1:      Opc = RISCVISD::SM3P1;      break;
+    case Intrinsic::riscv_orc_b:
+      Opc = RISCVISD::ORC_B;
+      break;
+    case Intrinsic::riscv_brev8:
+      Opc = RISCVISD::BREV8;
+      break;
+    case Intrinsic::riscv_sha256sig0:
+      Opc = RISCVISD::SHA256SIG0;
+      break;
+    case Intrinsic::riscv_sha256sig1:
+      Opc = RISCVISD::SHA256SIG1;
+      break;
+    case Intrinsic::riscv_sha256sum0:
+      Opc = RISCVISD::SHA256SUM0;
+      break;
+    case Intrinsic::riscv_sha256sum1:
+      Opc = RISCVISD::SHA256SUM1;
+      break;
+    case Intrinsic::riscv_sm3p0:
+      Opc = RISCVISD::SM3P0;
+      break;
+    case Intrinsic::riscv_sm3p1:
+      Opc = RISCVISD::SM3P1;
+      break;
     }
 
     if (RV64LegalI32 && Subtarget.is64Bit() && Op.getValueType() == MVT::i32) {
@@ -9247,7 +9341,8 @@ SDValue RISCVTargetLowering::LowerINTRINSIC_WO_CHAIN(SDValue Op,
     SDValue Vec = Op.getOperand(1);
     SDValue VL = getVLOperand(Op);
 
-    SDValue SplattedVal = splatSplitI64WithVL(DL, VT, SDValue(), Scalar, VL, DAG);
+    SDValue SplattedVal =
+        splatSplitI64WithVL(DL, VT, SDValue(), Scalar, VL, DAG);
     if (Op.getOperand(1).isUndef())
       return SplattedVal;
     SDValue SplattedIdx =
@@ -9423,7 +9518,8 @@ SDValue RISCVTargetLowering::LowerINTRINSIC_W_CHAIN(SDValue Op,
       MVT MaskVT = getMaskTypeFor(ContainerVT);
       if (VT.isFixedLengthVector()) {
         Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);
-        PassThru = convertToScalableVector(ContainerVT, PassThru, DAG, Subtarget);
+        PassThru =
+            convertToScalableVector(ContainerVT, PassThru, DAG, Subtarget);
       }
     }
 
@@ -9498,8 +9594,8 @@ SDValue RISCVTargetLowering::LowerINTRINSIC_W_CHAIN(SDValue Op,
     MVT VT = Op->getSimpleValueType(0);
     MVT ContainerVT = getContainerForFixedLengthVector(VT);
 
-    SDValue VL = getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG,
-                         Subtarget);
+    SDValue VL =
+        getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG, Subtarget);
     SDValue IntID = DAG.getTargetConstant(VlsegInts[NF - 2], DL, XLenVT);
     auto *Load = cast<MemIntrinsicSDNode>(Op);
     SmallVector<EVT, 9> ContainerVTs(NF, ContainerVT);
@@ -9619,8 +9715,8 @@ SDValue RISCVTargetLowering::LowerINTRINSIC_VOID(SDValue Op,
     MVT VT = Op->getOperand(2).getSimpleValueType();
     MVT ContainerVT = getContainerForFixedLengthVector(VT);
 
-    SDValue VL = getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG,
-                         Subtarget);
+    SDValue VL =
+        getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG, Subtarget);
     SDValue IntID = DAG.getTargetConstant(VssegInts[NF - 2], DL, XLenVT);
     SDValue Ptr = Op->getOperand(NF + 2);
 
@@ -9703,7 +9799,6 @@ static unsigned getRVVReductionOp(unsigned ISDOpcode) {
   case ISD::VP_REDUCE_FMINIMUM:
     return RISCVISD::VECREDUCE_FMIN_VL;
   }
-
 }
 
 SDValue RISCVTargetLowering::lowerVectorMaskVecReduction(SDValue Op,
@@ -9813,8 +9908,8 @@ static SDValue lowerReductionSeq(unsigned RVVOpcode, MVT ResVT,
   // prove it is non-zero.  For the AVL=0 case, we need the scalar to
   // be the result of the reduction operation.
   auto InnerVL = NonZeroAVL ? VL : DAG.getConstant(1, DL, XLenVT);
-  SDValue InitialValue = lowerScalarInsert(StartValue, InnerVL, InnerVT, DL,
-                                           DAG, Subtarget);
+  SDValue InitialValue =
+      lowerScalarInsert(StartValue, InnerVL, InnerVT, DL, DAG, Subtarget);
   if (M1VT != InnerVT)
     InitialValue =
         DAG.getNode(ISD::INSERT_SUBVECTOR, DL, M1VT, DAG.getUNDEF(M1VT),
@@ -10507,10 +10602,10 @@ SDValue RISCVTargetLowering::lowerVECTOR_DEINTERLEAVE(SDValue Op,
       DAG.getNode(ISD::ADD, DL, IdxVT, EvenIdx, DAG.getConstant(1, DL, IdxVT));
 
   // Gather the even and odd elements into two separate vectors
-  SDValue EvenWide = DAG.getNode(RISCVISD::VRGATHER_VV_VL, DL, ConcatVT,
-                                 Concat, EvenIdx, Passthru, Mask, VL);
-  SDValue OddWide = DAG.getNode(RISCVISD::VRGATHER_VV_VL, DL, ConcatVT,
-                                Concat, OddIdx, Passthru, Mask, VL);
+  SDValue EvenWide = DAG.getNode(RISCVISD::VRGATHER_VV_VL, DL, ConcatVT, Concat,
+                                 EvenIdx, Passthru, Mask, VL);
+  SDValue OddWide = DAG.getNode(RISCVISD::VRGATHER_VV_VL, DL, ConcatVT, Concat,
+                                OddIdx, Passthru, Mask, VL);
 
   // Extract the result half of the gather for even and odd
   SDValue Even = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VecVT, EvenWide,
@@ -10537,7 +10632,8 @@ SDValue RISCVTargetLowering::lowerVECTOR_INTERLEAVE(SDValue Op,
   SDValue VL = DAG.getRegister(RISCV::X0, XLenVT);
 
   // If the VT is LMUL=8, we need to split and reassemble.
-  if (VecVT.getSizeInBits().getKnownMinValue() == (8 * RISCV::RVVBitsPerBlock)) {
+  if (VecVT.getSizeInBits().getKnownMinValue() ==
+      (8 * RISCV::RVVBitsPerBlock)) {
     auto [Op0Lo, Op0Hi] = DAG.SplitVectorOperand(Op.getNode(), 0);
     auto [Op1Lo, Op1Hi] = DAG.SplitVectorOperand(Op.getNode(), 1);
     EVT SplitVT = Op0Lo.getValueType();
@@ -10547,10 +10643,10 @@ SDValue RISCVTargetLowering::lowerVECTOR_INTERLEAVE(SDValue Op,
     SDValue ResHi = DAG.getNode(ISD::VECTOR_INTERLEAVE, DL,
                                 DAG.getVTList(SplitVT, SplitVT), Op0Hi, Op1Hi);
 
-    SDValue Lo = DAG.getNode(ISD::CONCAT_VECTORS, DL, VecVT,
-                             ResLo.getValue(0), ResLo.getValue(1));
-    SDValue Hi = DAG.getNode(ISD::CONCAT_VECTORS, DL, VecVT,
-                             ResHi.getValue(0), ResHi.getValue(1));
+    SDValue Lo = DAG.getNode(ISD::CONCAT_VECTORS, DL, VecVT, ResLo.getValue(0),
+                             ResLo.getValue(1));
+    SDValue Hi = DAG.getNode(ISD::CONCAT_VECTORS, DL, VecVT, ResHi.getValue(0),
+                             ResHi.getValue(1));
     return DAG.getMergeValues({Lo, Hi}, DL);
   }
 
@@ -10563,9 +10659,9 @@ SDValue RISCVTargetLowering::lowerVECTOR_INTERLEAVE(SDValue Op,
                                         DAG, Subtarget);
   } else {
     // Otherwise, fallback to using vrgathere16.vv
-    MVT ConcatVT =
-      MVT::getVectorVT(VecVT.getVectorElementType(),
-                       VecVT.getVectorElementCount().multiplyCoefficientBy(2));
+    MVT ConcatVT = MVT::getVectorVT(
+        VecVT.getVectorElementType(),
+        VecVT.getVectorElementCount().multiplyCoefficientBy(2));
     SDValue Concat = DAG.getNode(ISD::CONCAT_VECTORS, DL, ConcatVT,
                                  Op.getOperand(0), Op.getOperand(1));
 
@@ -10575,7 +10671,8 @@ SDValue RISCVTargetLowering::lowerVECTOR_INTERLEAVE(SDValue Op,
     SDValue StepVec = DAG.getStepVector(DL, IdxVT);
 
     // 1 1 1 1 1 1 1 1 ...
-    SDValue Ones = DAG.getSplatVector(IdxVT, DL, DAG.getConstant(1, DL, XLenVT));
+    SDValue Ones =
+        DAG.getSplatVector(IdxVT, DL, DAG.getConstant(1, DL, XLenVT));
 
     // 1 0 1 0 1 0 1 0 ...
     SDValue OddMask = DAG.getNode(ISD::AND, DL, IdxVT, StepVec, Ones);
@@ -10596,8 +10693,9 @@ SDValue RISCVTargetLowering::lowerVECTOR_INTERLEAVE(SDValue Op,
     // Then perform the interleave
     //   v[0]   v[n]   v[1] v[n+1]   v[2] v[n+2]   v[3] v[n+3] ...
     SDValue TrueMask = getAllOnesMask(IdxVT, VL, DL, DAG);
-    Interleaved = DAG.getNode(RISCVISD::VRGATHEREI16_VV_VL, DL, ConcatVT,
-                              Concat, Idx, DAG.getUNDEF(ConcatVT), TrueMask, VL);
+    Interleaved =
+        DAG.getNode(RISCVISD::VRGATHEREI16_VV_VL, DL, ConcatVT, Concat, Idx,
+                    DAG.getUNDEF(ConcatVT), TrueMask, VL);
   }
 
   // Extract the two halves from the interleaved result
@@ -10644,21 +10742,32 @@ SDValue RISCVTargetLowering::lowerSTEP_VECTOR(SDValue Op,
 // TODO: This code assumes VLMAX <= 65536 for LMUL=8 SEW=16.
 SDValue RISCVTargetLowering::lowerVECTOR_REVERSE(SDValue Op,
                                                  SelectionDAG &DAG) const {
-  SDLoc DL(Op);
-  MVT VecVT = Op.getSimpleValueType();
+  SDLoc DL(Op);                        // DL means Dag Location
+  MVT VecVT = Op.getSimpleValueType(); // VecVT means Vector Type
+
+  // Case 1: Deal with i1 vector
   if (VecVT.getVectorElementType() == MVT::i1) {
     MVT WidenVT = MVT::getVectorVT(MVT::i8, VecVT.getVectorElementCount());
+    // ZERO_EXTEND make <vscale x ? x i1> to <vscale x ? x i8>
     SDValue Op1 = DAG.getNode(ISD::ZERO_EXTEND, DL, WidenVT, Op.getOperand(0));
+    // Reverse the vector
     SDValue Op2 = DAG.getNode(ISD::VECTOR_REVERSE, DL, WidenVT, Op1);
+    // TRUNCATE make <vscale x ? x i8> to <vscale x ? x i1>
     return DAG.getNode(ISD::TRUNCATE, DL, VecVT, Op2);
   }
-  unsigned EltSize = VecVT.getScalarSizeInBits();
-  unsigned MinSize = VecVT.getSizeInBits().getKnownMinValue();
-  unsigned VectorBitsMax = Subtarget.getRealMaxVLen();
+  // Case 2: Deal with normal vector types
+  unsigned EltSize = VecVT.getScalarSizeInBits(); //  EltSize means Element Size
+  unsigned MinSize =
+      VecVT.getSizeInBits()
+          .getKnownMinValue(); // MinSize means minimum vector bit
+  unsigned VectorBitsMax = Subtarget.getRealMaxVLen(); // Maximum VLEN
+  // Maximum vector element number (consider LMUL and SEW)
   unsigned MaxVLMAX =
-    RISCVTargetLowering::computeVLMAX(VectorBitsMax, EltSize, MinSize);
+      RISCVTargetLowering::computeVLMAX(VectorBitsMax, EltSize, MinSize);
 
   unsigned GatherOpc = RISCVISD::VRGATHER_VV_VL;
+  // Convert vector element type to integer type for index calculation and
+  // manipulation
   MVT IntVT = VecVT.changeVectorElementTypeToInteger();
 
   // If this is SEW=8 and VLMAX is potentially more than 256, we need
@@ -10690,13 +10799,14 @@ SDValue RISCVTargetLowering::lowerVECTOR_REVERSE(SDValue Op,
     GatherOpc = RISCVISD::VRGATHEREI16_VV_VL;
   }
 
-  MVT XLenVT = Subtarget.getXLenVT();
-  auto [Mask, VL] = getDefaultScalableVLOps(VecVT, DL, DAG, Subtarget);
+  MVT XLenVT =
+      Subtarget.getXLenVT(); // -> In our project, we use RV64, so XLenVT is i64
+  auto [Mask, VL] = getDefaultScalableVLOps(VecVT, DL, DAG, Subtarget); // VL is x0
 
   // Calculate VLMAX-1 for the desired SEW.
-  SDValue VLMinus1 = DAG.getNode(ISD::SUB, DL, XLenVT,
-                                 computeVLMax(VecVT, DL, DAG),
-                                 DAG.getConstant(1, DL, XLenVT));
+  SDValue VLMinus1 =
+      DAG.getNode(ISD::SUB, DL, XLenVT, computeVLMax(VecVT, DL, DAG),
+                  DAG.getConstant(1, DL, XLenVT));
 
   // Splat VLMAX-1 taking care to handle SEW==64 on RV32.
   bool IsRV32E64 =
@@ -10712,10 +10822,34 @@ SDValue RISCVTargetLowering::lowerVECTOR_REVERSE(SDValue Op,
   SDValue Indices = DAG.getNode(RISCVISD::SUB_VL, DL, IntVT, SplatVL, VID,
                                 DAG.getUNDEF(IntVT), Mask, VL);
 
-  return DAG.getNode(GatherOpc, DL, VecVT, Op.getOperand(0), Indices,
+
+
+  bool useVREV = GatherOpc == RISCVISD::VRGATHER_VV_VL;
+
+  bool IsWidenedI1 = false;
+  // Only perform the check if the current vector type is i8
+  if (VecVT.getVectorElementType() == MVT::i8) {
+    // Get the input operand to the ISD::VECTOR_REVERSE node
+    SDValue InputOperand = Op.getOperand(0);
+    if (InputOperand.getOpcode() == ISD::ZERO_EXTEND) {
+      MVT SourceVT = InputOperand.getOperand(0).getSimpleValueType();
+      if (SourceVT.isVector() && SourceVT.getVectorElementType() == MVT::i1) {
+        IsWidenedI1 = true; // Mark this as the widened i1 case
+      }
+    }
+  }
+
+
+  if (useVREV && !IsWidenedI1) {
+    return DAG.getNode(RISCVISD::VREVERSEMTK_V_VL, DL, VecVT,
+                      Op.getOperand(0));
+  } else {
+    return DAG.getNode(GatherOpc, DL, VecVT, Op.getOperand(0), Indices,
                      DAG.getUNDEF(VecVT), Mask, VL);
+  }
 }
 
+
 SDValue RISCVTargetLowering::lowerVECTOR_SPLICE(SDValue Op,
                                                 SelectionDAG &DAG) const {
   SDLoc DL(Op);
@@ -10780,7 +10914,8 @@ RISCVTargetLowering::lowerFixedLengthVectorLoadToRVV(SDValue Op,
     return DAG.getMergeValues({Result, NewLoad.getValue(1)}, DL);
   }
 
-  SDValue VL = getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG, Subtarget);
+  SDValue VL =
+      getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG, Subtarget);
 
   bool IsMaskOp = VT.getVectorElementType() == MVT::i1;
   SDValue IntID = DAG.getTargetConstant(
@@ -10827,7 +10962,6 @@ RISCVTargetLowering::lowerFixedLengthVectorStoreToRVV(SDValue Op,
   SDValue NewValue =
       convertToScalableVector(ContainerVT, StoreVal, DAG, Subtarget);
 
-
   // If we know the exact VLEN and our fixed length vector completely fills
   // the container, use a whole register store instead.
   const auto [MinVLMAX, MaxVLMAX] =
@@ -10840,8 +10974,8 @@ RISCVTargetLowering::lowerFixedLengthVectorStoreToRVV(SDValue Op,
                         MMO->getFlags(), MMO->getAAInfo());
   }
 
-  SDValue VL = getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG,
-                       Subtarget);
+  SDValue VL =
+      getVLOp(VT.getVectorNumElements(), ContainerVT, DL, DAG, Subtarget);
 
   bool IsMaskOp = VT.getVectorElementType() == MVT::i1;
   SDValue IntID = DAG.getTargetConstant(
@@ -11640,126 +11774,126 @@ SDValue RISCVTargetLowering::lowerVPSplatExperimental(SDValue Op,
   return convertFromScalableVector(VT, Result, DAG, Subtarget);
 }
 
-SDValue
-RISCVTargetLowering::lowerVPReverseExperimental(SDValue Op,
-                                                SelectionDAG &DAG) const {
-  SDLoc DL(Op);
-  MVT VT = Op.getSimpleValueType();
-  MVT XLenVT = Subtarget.getXLenVT();
-
-  SDValue Op1 = Op.getOperand(0);
-  SDValue Mask = Op.getOperand(1);
-  SDValue EVL = Op.getOperand(2);
-
-  MVT ContainerVT = VT;
-  if (VT.isFixedLengthVector()) {
-    ContainerVT = getContainerForFixedLengthVector(VT);
-    Op1 = convertToScalableVector(ContainerVT, Op1, DAG, Subtarget);
-    MVT MaskVT = getMaskTypeFor(ContainerVT);
-    Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);
-  }
-
-  MVT GatherVT = ContainerVT;
-  MVT IndicesVT = ContainerVT.changeVectorElementTypeToInteger();
-  // Check if we are working with mask vectors
-  bool IsMaskVector = ContainerVT.getVectorElementType() == MVT::i1;
-  if (IsMaskVector) {
-    GatherVT = IndicesVT = ContainerVT.changeVectorElementType(MVT::i8);
-
-    // Expand input operand
-    SDValue SplatOne = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IndicesVT,
-                                   DAG.getUNDEF(IndicesVT),
-                                   DAG.getConstant(1, DL, XLenVT), EVL);
-    SDValue SplatZero = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IndicesVT,
-                                    DAG.getUNDEF(IndicesVT),
-                                    DAG.getConstant(0, DL, XLenVT), EVL);
-    Op1 = DAG.getNode(RISCVISD::VMERGE_VL, DL, IndicesVT, Op1, SplatOne,
-                      SplatZero, DAG.getUNDEF(IndicesVT), EVL);
-  }
-
-  unsigned EltSize = GatherVT.getScalarSizeInBits();
-  unsigned MinSize = GatherVT.getSizeInBits().getKnownMinValue();
-  unsigned VectorBitsMax = Subtarget.getRealMaxVLen();
-  unsigned MaxVLMAX =
-      RISCVTargetLowering::computeVLMAX(VectorBitsMax, EltSize, MinSize);
-
-  unsigned GatherOpc = RISCVISD::VRGATHER_VV_VL;
-  // If this is SEW=8 and VLMAX is unknown or more than 256, we need
-  // to use vrgatherei16.vv.
-  // TODO: It's also possible to use vrgatherei16.vv for other types to
-  // decrease register width for the index calculation.
-  // NOTE: This code assumes VLMAX <= 65536 for LMUL=8 SEW=16.
-  if (MaxVLMAX > 256 && EltSize == 8) {
-    // If this is LMUL=8, we have to split before using vrgatherei16.vv.
-    // Split the vector in half and reverse each half using a full register
-    // reverse.
-    // Swap the halves and concatenate them.
-    // Slide the concatenated result by (VLMax - VL).
-    if (MinSize == (8 * RISCV::RVVBitsPerBlock)) {
-      auto [LoVT, HiVT] = DAG.GetSplitDestVTs(GatherVT);
-      auto [Lo, Hi] = DAG.SplitVector(Op1, DL);
-
-      SDValue LoRev = DAG.getNode(ISD::VECTOR_REVERSE, DL, LoVT, Lo);
-      SDValue HiRev = DAG.getNode(ISD::VECTOR_REVERSE, DL, HiVT, Hi);
-
-      // Reassemble the low and high pieces reversed.
-      // NOTE: this Result is unmasked (because we do not need masks for
-      // shuffles). If in the future this has to change, we can use a SELECT_VL
-      // between Result and UNDEF using the mask originally passed to VP_REVERSE
-      SDValue Result =
-          DAG.getNode(ISD::CONCAT_VECTORS, DL, GatherVT, HiRev, LoRev);
-
-      // Slide off any elements from past EVL that were reversed into the low
-      // elements.
-      unsigned MinElts = GatherVT.getVectorMinNumElements();
-      SDValue VLMax =
-          DAG.getVScale(DL, XLenVT, APInt(XLenVT.getSizeInBits(), MinElts));
-      SDValue Diff = DAG.getNode(ISD::SUB, DL, XLenVT, VLMax, EVL);
-
-      Result = getVSlidedown(DAG, Subtarget, DL, GatherVT,
-                             DAG.getUNDEF(GatherVT), Result, Diff, Mask, EVL);
-
-      if (IsMaskVector) {
-        // Truncate Result back to a mask vector
-        Result =
-            DAG.getNode(RISCVISD::SETCC_VL, DL, ContainerVT,
-                        {Result, DAG.getConstant(0, DL, GatherVT),
-                         DAG.getCondCode(ISD::SETNE),
-                         DAG.getUNDEF(getMaskTypeFor(ContainerVT)), Mask, EVL});
-      }
-
-      if (!VT.isFixedLengthVector())
-        return Result;
-      return convertFromScalableVector(VT, Result, DAG, Subtarget);
-    }
-
-    // Just promote the int type to i16 which will double the LMUL.
-    IndicesVT = MVT::getVectorVT(MVT::i16, IndicesVT.getVectorElementCount());
-    GatherOpc = RISCVISD::VRGATHEREI16_VV_VL;
-  }
-
-  SDValue VID = DAG.getNode(RISCVISD::VID_VL, DL, IndicesVT, Mask, EVL);
-  SDValue VecLen =
-      DAG.getNode(ISD::SUB, DL, XLenVT, EVL, DAG.getConstant(1, DL, XLenVT));
-  SDValue VecLenSplat = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IndicesVT,
-                                    DAG.getUNDEF(IndicesVT), VecLen, EVL);
-  SDValue VRSUB = DAG.getNode(RISCVISD::SUB_VL, DL, IndicesVT, VecLenSplat, VID,
-                              DAG.getUNDEF(IndicesVT), Mask, EVL);
-  SDValue Result = DAG.getNode(GatherOpc, DL, GatherVT, Op1, VRSUB,
-                               DAG.getUNDEF(GatherVT), Mask, EVL);
-
-  if (IsMaskVector) {
-    // Truncate Result back to a mask vector
-    Result = DAG.getNode(
-        RISCVISD::SETCC_VL, DL, ContainerVT,
-        {Result, DAG.getConstant(0, DL, GatherVT), DAG.getCondCode(ISD::SETNE),
-         DAG.getUNDEF(getMaskTypeFor(ContainerVT)), Mask, EVL});
-  }
-
-  if (!VT.isFixedLengthVector())
-    return Result;
-  return convertFromScalableVector(VT, Result, DAG, Subtarget);
-}
+// SDValue
+// RISCVTargetLowering::lowerVPReverseExperimental(SDValue Op,
+//                                                 SelectionDAG &DAG) const {
+//   SDLoc DL(Op);
+//   MVT VT = Op.getSimpleValueType();
+//   MVT XLenVT = Subtarget.getXLenVT();
+
+//   SDValue Op1 = Op.getOperand(0);
+//   SDValue Mask = Op.getOperand(1);
+//   SDValue EVL = Op.getOperand(2);
+
+//   MVT ContainerVT = VT;
+//   if (VT.isFixedLengthVector()) {
+//     ContainerVT = getContainerForFixedLengthVector(VT);
+//     Op1 = convertToScalableVector(ContainerVT, Op1, DAG, Subtarget);
+//     MVT MaskVT = getMaskTypeFor(ContainerVT);
+//     Mask = convertToScalableVector(MaskVT, Mask, DAG, Subtarget);
+//   }
+
+//   MVT GatherVT = ContainerVT;
+//   MVT IndicesVT = ContainerVT.changeVectorElementTypeToInteger();
+//   // Check if we are working with mask vectors
+//   bool IsMaskVector = ContainerVT.getVectorElementType() == MVT::i1;
+//   if (IsMaskVector) {
+//     GatherVT = IndicesVT = ContainerVT.changeVectorElementType(MVT::i8);
+
+//     // Expand input operand
+//     SDValue SplatOne = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IndicesVT,
+//                                    DAG.getUNDEF(IndicesVT),
+//                                    DAG.getConstant(1, DL, XLenVT), EVL);
+//     SDValue SplatZero = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IndicesVT,
+//                                     DAG.getUNDEF(IndicesVT),
+//                                     DAG.getConstant(0, DL, XLenVT), EVL);
+//     Op1 = DAG.getNode(RISCVISD::VMERGE_VL, DL, IndicesVT, Op1, SplatOne,
+//                       SplatZero, DAG.getUNDEF(IndicesVT), EVL);
+//   }
+
+//   unsigned EltSize = GatherVT.getScalarSizeInBits();
+//   unsigned MinSize = GatherVT.getSizeInBits().getKnownMinValue();
+//   unsigned VectorBitsMax = Subtarget.getRealMaxVLen();
+//   unsigned MaxVLMAX =
+//       RISCVTargetLowering::computeVLMAX(VectorBitsMax, EltSize, MinSize);
+
+//   unsigned GatherOpc = RISCVISD::VRGATHER_VV_VL;
+//   // If this is SEW=8 and VLMAX is unknown or more than 256, we need
+//   // to use vrgatherei16.vv.
+//   // TODO: It's also possible to use vrgatherei16.vv for other types to
+//   // decrease register width for the index calculation.
+//   // NOTE: This code assumes VLMAX <= 65536 for LMUL=8 SEW=16.
+//   if (MaxVLMAX > 256 && EltSize == 8) {
+//     // If this is LMUL=8, we have to split before using vrgatherei16.vv.
+//     // Split the vector in half and reverse each half using a full register
+//     // reverse.
+//     // Swap the halves and concatenate them.
+//     // Slide the concatenated result by (VLMax - VL).
+//     if (MinSize == (8 * RISCV::RVVBitsPerBlock)) {
+//       auto [LoVT, HiVT] = DAG.GetSplitDestVTs(GatherVT);
+//       auto [Lo, Hi] = DAG.SplitVector(Op1, DL);
+
+//       SDValue LoRev = DAG.getNode(ISD::VECTOR_REVERSE, DL, LoVT, Lo);
+//       SDValue HiRev = DAG.getNode(ISD::VECTOR_REVERSE, DL, HiVT, Hi);
+
+//       // Reassemble the low and high pieces reversed.
+//       // NOTE: this Result is unmasked (because we do not need masks for
+//       // shuffles). If in the future this has to change, we can use a SELECT_VL
+//       // between Result and UNDEF using the mask originally passed to VP_REVERSE
+//       SDValue Result =
+//           DAG.getNode(ISD::CONCAT_VECTORS, DL, GatherVT, HiRev, LoRev);
+
+//       // Slide off any elements from past EVL that were reversed into the low
+//       // elements.
+//       unsigned MinElts = GatherVT.getVectorMinNumElements();
+//       SDValue VLMax =
+//           DAG.getVScale(DL, XLenVT, APInt(XLenVT.getSizeInBits(), MinElts));
+//       SDValue Diff = DAG.getNode(ISD::SUB, DL, XLenVT, VLMax, EVL);
+
+//       Result = getVSlidedown(DAG, Subtarget, DL, GatherVT,
+//                              DAG.getUNDEF(GatherVT), Result, Diff, Mask, EVL);
+
+//       if (IsMaskVector) {
+//         // Truncate Result back to a mask vector
+//         Result =
+//             DAG.getNode(RISCVISD::SETCC_VL, DL, ContainerVT,
+//                         {Result, DAG.getConstant(0, DL, GatherVT),
+//                          DAG.getCondCode(ISD::SETNE),
+//                          DAG.getUNDEF(getMaskTypeFor(ContainerVT)), Mask, EVL});
+//       }
+
+//       if (!VT.isFixedLengthVector())
+//         return Result;
+//       return convertFromScalableVector(VT, Result, DAG, Subtarget);
+//     }
+
+//     // Just promote the int type to i16 which will double the LMUL.
+//     IndicesVT = MVT::getVectorVT(MVT::i16, IndicesVT.getVectorElementCount());
+//     GatherOpc = RISCVISD::VRGATHEREI16_VV_VL;
+//   }
+
+//   SDValue VID = DAG.getNode(RISCVISD::VID_VL, DL, IndicesVT, Mask, EVL);
+//   SDValue VecLen =
+//       DAG.getNode(ISD::SUB, DL, XLenVT, EVL, DAG.getConstant(1, DL, XLenVT));
+//   SDValue VecLenSplat = DAG.getNode(RISCVISD::VMV_V_X_VL, DL, IndicesVT,
+//                                     DAG.getUNDEF(IndicesVT), VecLen, EVL);
+//   SDValue VRSUB = DAG.getNode(RISCVISD::SUB_VL, DL, IndicesVT, VecLenSplat, VID,
+//                               DAG.getUNDEF(IndicesVT), Mask, EVL);
+//   SDValue Result = DAG.getNode(GatherOpc, DL, GatherVT, Op1, VRSUB,
+//                                DAG.getUNDEF(GatherVT), Mask, EVL);
+
+//   if (IsMaskVector) {
+//     // Truncate Result back to a mask vector
+//     Result = DAG.getNode(
+//         RISCVISD::SETCC_VL, DL, ContainerVT,
+//         {Result, DAG.getConstant(0, DL, GatherVT), DAG.getCondCode(ISD::SETNE),
+//          DAG.getUNDEF(getMaskTypeFor(ContainerVT)), Mask, EVL});
+//   }
+
+//   if (!VT.isFixedLengthVector())
+//     return Result;
+//   return convertFromScalableVector(VT, Result, DAG, Subtarget);
+// }
 
 SDValue RISCVTargetLowering::lowerLogicVPOp(SDValue Op,
                                             SelectionDAG &DAG) const {
@@ -12453,8 +12587,8 @@ void RISCVTargetLowering::ReplaceNodeResults(SDNode *N,
     // based on the opcode.
     unsigned ExtOpc = ISD::ANY_EXTEND;
     if (VT != MVT::i32)
-      ExtOpc = N->getOpcode() == ISD::SDIV ? ISD::SIGN_EXTEND
-                                           : ISD::ZERO_EXTEND;
+      ExtOpc =
+          N->getOpcode() == ISD::SDIV ? ISD::SIGN_EXTEND : ISD::ZERO_EXTEND;
 
     Results.push_back(customLegalizeToWOp(N, DAG, ExtOpc));
     break;
@@ -12568,8 +12702,8 @@ void RISCVTargetLowering::ReplaceNodeResults(SDNode *N,
       // Emit a special ABSW node that will be expanded to NEGW+MAX at isel.
       // This allows us to remember that the result is sign extended. Expanding
       // to NEGW+MAX here requires a Freeze which breaks ComputeNumSignBits.
-      SDValue Src = DAG.getNode(ISD::SIGN_EXTEND, DL, MVT::i64,
-                                N->getOperand(0));
+      SDValue Src =
+          DAG.getNode(ISD::SIGN_EXTEND, DL, MVT::i64, N->getOperand(0));
       SDValue Abs = DAG.getNode(RISCVISD::ABSW, DL, MVT::i64, Src);
       Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Abs));
       return;
@@ -12740,14 +12874,30 @@ void RISCVTargetLowering::ReplaceNodeResults(SDNode *N,
         return;
       unsigned Opc;
       switch (IntNo) {
-      case Intrinsic::riscv_orc_b:      Opc = RISCVISD::ORC_B;      break;
-      case Intrinsic::riscv_brev8:      Opc = RISCVISD::BREV8;      break;
-      case Intrinsic::riscv_sha256sig0: Opc = RISCVISD::SHA256SIG0; break;
-      case Intrinsic::riscv_sha256sig1: Opc = RISCVISD::SHA256SIG1; break;
-      case Intrinsic::riscv_sha256sum0: Opc = RISCVISD::SHA256SUM0; break;
-      case Intrinsic::riscv_sha256sum1: Opc = RISCVISD::SHA256SUM1; break;
-      case Intrinsic::riscv_sm3p0:      Opc = RISCVISD::SM3P0;      break;
-      case Intrinsic::riscv_sm3p1:      Opc = RISCVISD::SM3P1;      break;
+      case Intrinsic::riscv_orc_b:
+        Opc = RISCVISD::ORC_B;
+        break;
+      case Intrinsic::riscv_brev8:
+        Opc = RISCVISD::BREV8;
+        break;
+      case Intrinsic::riscv_sha256sig0:
+        Opc = RISCVISD::SHA256SIG0;
+        break;
+      case Intrinsic::riscv_sha256sig1:
+        Opc = RISCVISD::SHA256SIG1;
+        break;
+      case Intrinsic::riscv_sha256sum0:
+        Opc = RISCVISD::SHA256SUM0;
+        break;
+      case Intrinsic::riscv_sha256sum1:
+        Opc = RISCVISD::SHA256SUM1;
+        break;
+      case Intrinsic::riscv_sm3p0:
+        Opc = RISCVISD::SM3P0;
+        break;
+      case Intrinsic::riscv_sm3p1:
+        Opc = RISCVISD::SM3P1;
+        break;
       }
 
       SDValue NewOp =
@@ -13000,7 +13150,7 @@ combineBinOpOfExtractToReduceTree(SDNode *N, SelectionDAG &DAG,
   if (LHS.getOpcode() == ISD::EXTRACT_VECTOR_ELT &&
       LHS.getOperand(0) == SrcVec && isa<ConstantSDNode>(LHS.getOperand(1))) {
     uint64_t LHSIdx =
-      cast<ConstantSDNode>(LHS.getOperand(1))->getLimitedValue();
+        cast<ConstantSDNode>(LHS.getOperand(1))->getLimitedValue();
     if (0 == std::min(LHSIdx, RHSIdx) && 1 == std::max(LHSIdx, RHSIdx)) {
       EVT ReduceVT = EVT::getVectorVT(*DAG.getContext(), VT, 2);
       SDValue Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, ReduceVT, SrcVec,
@@ -13035,7 +13185,6 @@ combineBinOpOfExtractToReduceTree(SDNode *N, SelectionDAG &DAG,
   return SDValue();
 }
 
-
 // Try to fold (<bop> x, (reduction.<bop> vec, start))
 static SDValue combineBinOpToReduce(SDNode *N, SelectionDAG &DAG,
                                     const RISCVSubtarget &Subtarget) {
@@ -13095,8 +13244,7 @@ static SDValue combineBinOpToReduce(SDNode *N, SelectionDAG &DAG,
   SDValue ScalarV = Reduce.getOperand(2);
   EVT ScalarVT = ScalarV.getValueType();
   if (ScalarV.getOpcode() == ISD::INSERT_SUBVECTOR &&
-      ScalarV.getOperand(0)->isUndef() &&
-      isNullConstant(ScalarV.getOperand(2)))
+      ScalarV.getOperand(0)->isUndef() && isNullConstant(ScalarV.getOperand(2)))
     ScalarV = ScalarV.getOperand(1);
 
   // Make sure that ScalarV is a splat with VL=1.
@@ -13669,9 +13817,10 @@ static SDValue performTRUNCATECombine(SDNode *N, SelectionDAG &DAG,
   // shift amounts larger than 31 would produce poison. If we wait until
   // type legalization, we'll create RISCVISD::SRLW and we can't recover it
   // to use a BEXT instruction.
-  if (!RV64LegalI32 && Subtarget.is64Bit() && Subtarget.hasStdExtZbs() && VT == MVT::i1 &&
-      N0.getValueType() == MVT::i32 && N0.getOpcode() == ISD::SRL &&
-      !isa<ConstantSDNode>(N0.getOperand(1)) && N0.hasOneUse()) {
+  if (!RV64LegalI32 && Subtarget.is64Bit() && Subtarget.hasStdExtZbs() &&
+      VT == MVT::i1 && N0.getValueType() == MVT::i32 &&
+      N0.getOpcode() == ISD::SRL && !isa<ConstantSDNode>(N0.getOperand(1)) &&
+      N0.hasOneUse()) {
     SDLoc DL(N0);
     SDValue Op0 = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, N0.getOperand(0));
     SDValue Op1 = DAG.getNode(ISD::ZERO_EXTEND, DL, MVT::i64, N0.getOperand(1));
@@ -13730,8 +13879,8 @@ static SDValue combineOrOfCZERO(SDNode *N, SDValue N0, SDValue N1,
   assert(N->getOpcode() == ISD::OR && "Unexpected opcode");
 
   if (N0.getOpcode() != RISCVISD::CZERO_EQZ ||
-      N1.getOpcode() != RISCVISD::CZERO_NEZ ||
-      !N0.hasOneUse() || !N1.hasOneUse())
+      N1.getOpcode() != RISCVISD::CZERO_NEZ || !N0.hasOneUse() ||
+      !N1.hasOneUse())
     return SDValue();
 
   // Should have the same condition.
@@ -13744,17 +13893,17 @@ static SDValue combineOrOfCZERO(SDNode *N, SDValue N0, SDValue N1,
 
   if (TrueV.getOpcode() != ISD::XOR || FalseV.getOpcode() != ISD::XOR ||
       TrueV.getOperand(1) != FalseV.getOperand(1) ||
-      !isOneConstant(TrueV.getOperand(1)) ||
-      !TrueV.hasOneUse() || !FalseV.hasOneUse())
+      !isOneConstant(TrueV.getOperand(1)) || !TrueV.hasOneUse() ||
+      !FalseV.hasOneUse())
     return SDValue();
 
   EVT VT = N->getValueType(0);
   SDLoc DL(N);
 
-  SDValue NewN0 = DAG.getNode(RISCVISD::CZERO_EQZ, DL, VT, TrueV.getOperand(0),
-                              Cond);
-  SDValue NewN1 = DAG.getNode(RISCVISD::CZERO_NEZ, DL, VT, FalseV.getOperand(0),
-                              Cond);
+  SDValue NewN0 =
+      DAG.getNode(RISCVISD::CZERO_EQZ, DL, VT, TrueV.getOperand(0), Cond);
+  SDValue NewN1 =
+      DAG.getNode(RISCVISD::CZERO_NEZ, DL, VT, FalseV.getOperand(0), Cond);
   SDValue NewOr = DAG.getNode(ISD::OR, DL, VT, NewN0, NewN1);
   return DAG.getNode(ISD::XOR, DL, VT, NewOr, TrueV.getOperand(1));
 }
@@ -13809,8 +13958,8 @@ static SDValue performXORCombine(SDNode *N, SelectionDAG &DAG,
   // fold (xor (sllw 1, x), -1) -> (rolw ~1, x)
   // NOTE: Assumes ROL being legal means ROLW is legal.
   const TargetLowering &TLI = DAG.getTargetLoweringInfo();
-  if (N0.getOpcode() == RISCVISD::SLLW &&
-      isAllOnesConstant(N1) && isOneConstant(N0.getOperand(0)) &&
+  if (N0.getOpcode() == RISCVISD::SLLW && isAllOnesConstant(N1) &&
+      isOneConstant(N0.getOperand(0)) &&
       TLI.isOperationLegal(ISD::ROTL, MVT::i64)) {
     SDLoc DL(N);
     return DAG.getNode(RISCVISD::ROLW, DL, MVT::i64,
@@ -13883,10 +14032,11 @@ static SDValue expandMul(SDNode *N, SelectionDAG &DAG,
     return SDValue();
   uint64_t MulAmt = CNode->getZExtValue();
 
-  // WARNING: The code below is knowingly incorrect with regards to undef semantics.
-  // We're adding additional uses of X here, and in principle, we should be freezing
-  // X before doing so.  However, adding freeze here causes real regressions, and no
-  // other target properly freezes X in these cases either.
+  // WARNING: The code below is knowingly incorrect with regards to undef
+  // semantics. We're adding additional uses of X here, and in principle, we
+  // should be freezing X before doing so.  However, adding freeze here causes
+  // real regressions, and no other target properly freezes X in these cases
+  // either.
   SDValue X = N->getOperand(0);
 
   if (HasShlAdd) {
@@ -14099,7 +14249,8 @@ static SDValue performMULCombine(SDNode *N, SelectionDAG &DAG,
 
 /// According to the property that indexed load/store instructions zero-extend
 /// their indices, try to narrow the type of index operand.
-static bool narrowIndex(SDValue &N, ISD::MemIndexType IndexType, SelectionDAG &DAG) {
+static bool narrowIndex(SDValue &N, ISD::MemIndexType IndexType,
+                        SelectionDAG &DAG) {
   if (isIndexTypeSigned(IndexType))
     return false;
 
@@ -14122,8 +14273,8 @@ static bool narrowIndex(SDValue &N, ISD::MemIndexType IndexType, SelectionDAG &D
     LLVMContext &C = *DAG.getContext();
     EVT ResultVT = EVT::getIntegerVT(C, ActiveBits).getRoundIntegerType(C);
     if (ResultVT.bitsLT(VT.getVectorElementType())) {
-      N = DAG.getNode(ISD::TRUNCATE, DL,
-                      VT.changeVectorElementType(ResultVT), N);
+      N = DAG.getNode(ISD::TRUNCATE, DL, VT.changeVectorElementType(ResultVT),
+                      N);
       return true;
     }
   }
@@ -14210,8 +14361,8 @@ static SDValue performSETCCCombine(SDNode *N, SelectionDAG &DAG,
 
   SDValue SExtOp = DAG.getNode(ISD::SIGN_EXTEND_INREG, N, OpVT,
                                N0.getOperand(0), DAG.getValueType(MVT::i32));
-  return DAG.getSetCC(dl, VT, SExtOp, DAG.getConstant(C1.trunc(32).sext(64),
-                                                      dl, OpVT), Cond);
+  return DAG.getSetCC(dl, VT, SExtOp,
+                      DAG.getConstant(C1.trunc(32).sext(64), dl, OpVT), Cond);
 }
 
 static SDValue
@@ -15382,8 +15533,8 @@ static SDValue performFP_TO_INTCombine(SDNode *N,
 //   (fp_to_int_sat (fround X))     -> (select X == nan, 0, (fcvt X, rmm))
 //   (fp_to_int_sat (frint X))      -> (select X == nan, 0, (fcvt X, dyn))
 static SDValue performFP_TO_INT_SATCombine(SDNode *N,
-                                       TargetLowering::DAGCombinerInfo &DCI,
-                                       const RISCVSubtarget &Subtarget) {
+                                           TargetLowering::DAGCombinerInfo &DCI,
+                                           const RISCVSubtarget &Subtarget) {
   SelectionDAG &DAG = DCI.DAG;
   const TargetLowering &TLI = DAG.getTargetLoweringInfo();
   MVT XLenVT = Subtarget.getXLenVT();
@@ -15428,8 +15579,8 @@ static SDValue performFP_TO_INT_SATCombine(SDNode *N,
   Src = Src.getOperand(0);
 
   SDLoc DL(N);
-  SDValue FpToInt = DAG.getNode(Opc, DL, XLenVT, Src,
-                                DAG.getTargetConstant(FRM, DL, XLenVT));
+  SDValue FpToInt =
+      DAG.getNode(Opc, DL, XLenVT, Src, DAG.getTargetConstant(FRM, DL, XLenVT));
 
   // fcvt.wu.* sign extends bit 31 on RV64. FP_TO_UINT_SAT expects to zero
   // extend.
@@ -15616,17 +15767,17 @@ static SDValue performSRACombine(SDNode *N, SelectionDAG &DAG,
   // Combine (sra (sext_inreg (shl X, C1), i32), C2) ->
   // (sra (shl X, C1+32), C2+32) so it gets selected as SLLI+SRAI instead of
   // SLLIW+SRAIW. SLLI+SRAI have compressed forms.
-  if (ShAmt < 32 &&
-      N0.getOpcode() == ISD::SIGN_EXTEND_INREG && N0.hasOneUse() &&
-      cast<VTSDNode>(N0.getOperand(1))->getVT() == MVT::i32 &&
-      N0.getOperand(0).getOpcode() == ISD::SHL && N0.getOperand(0).hasOneUse() &&
+  if (ShAmt < 32 && N0.getOpcode() == ISD::SIGN_EXTEND_INREG &&
+      N0.hasOneUse() && cast<VTSDNode>(N0.getOperand(1))->getVT() == MVT::i32 &&
+      N0.getOperand(0).getOpcode() == ISD::SHL &&
+      N0.getOperand(0).hasOneUse() &&
       isa<ConstantSDNode>(N0.getOperand(0).getOperand(1))) {
     uint64_t LShAmt = N0.getOperand(0).getConstantOperandVal(1);
     if (LShAmt < 32) {
       SDLoc ShlDL(N0.getOperand(0));
-      SDValue Shl = DAG.getNode(ISD::SHL, ShlDL, MVT::i64,
-                                N0.getOperand(0).getOperand(0),
-                                DAG.getConstant(LShAmt + 32, ShlDL, MVT::i64));
+      SDValue Shl =
+          DAG.getNode(ISD::SHL, ShlDL, MVT::i64, N0.getOperand(0).getOperand(0),
+                      DAG.getConstant(LShAmt + 32, ShlDL, MVT::i64));
       SDLoc DL(N);
       return DAG.getNode(ISD::SRA, DL, MVT::i64, Shl,
                          DAG.getConstant(ShAmt + 32, DL, MVT::i64));
@@ -15701,9 +15852,8 @@ static SDValue performSRACombine(SDNode *N, SelectionDAG &DAG,
   if (ShAmt == 32)
     return SExt;
 
-  return DAG.getNode(
-      ISD::SHL, DL, MVT::i64, SExt,
-      DAG.getConstant(32 - ShAmt, DL, MVT::i64));
+  return DAG.getNode(ISD::SHL, DL, MVT::i64, SExt,
+                     DAG.getConstant(32 - ShAmt, DL, MVT::i64));
 }
 
 // Invert (and/or (set cc X, Y), (xor Z, 1)) to (or/and (set !cc X, Y)), Z) if
@@ -15758,8 +15908,7 @@ static SDValue tryDemorganOfBooleanCondition(SDValue Cond, SelectionDAG &DAG) {
                          DAG.getConstant(1, SDLoc(Setcc), VT), CCVal);
   } else if (CCVal == ISD::SETLT && isOneConstant(Setcc.getOperand(1))) {
     // (setlt X, 1) by converting to (setlt 0, X).
-    Setcc = DAG.getSetCC(SDLoc(Setcc), VT,
-                         DAG.getConstant(0, SDLoc(Setcc), VT),
+    Setcc = DAG.getSetCC(SDLoc(Setcc), VT, DAG.getConstant(0, SDLoc(Setcc), VT),
                          Setcc.getOperand(0), CCVal);
   } else
     return SDValue();
@@ -16015,9 +16164,10 @@ static SDValue performSELECTCombine(SDNode *N, SelectionDAG &DAG,
 
   SDValue TrueVal = N->getOperand(1);
   SDValue FalseVal = N->getOperand(2);
-  if (SDValue V = tryFoldSelectIntoOp(N, DAG, TrueVal, FalseVal, /*Swapped*/false))
+  if (SDValue V =
+          tryFoldSelectIntoOp(N, DAG, TrueVal, FalseVal, /*Swapped*/ false))
     return V;
-  return tryFoldSelectIntoOp(N, DAG, FalseVal, TrueVal, /*Swapped*/true);
+  return tryFoldSelectIntoOp(N, DAG, FalseVal, TrueVal, /*Swapped*/ true);
 }
 
 /// If we have a build_vector where each lane is binop X, C, where C
@@ -16115,12 +16265,13 @@ static SDValue performINSERT_VECTOR_ELTCombine(SDNode *N, SelectionDAG &DAG,
       return SDValue();
     // FIXME: Return failure if the RHS type doesn't match the LHS. Shifts may
     // have different LHS and RHS types.
-    if (InVec.getOperand(0).getValueType() != InVec.getOperand(1).getValueType())
+    if (InVec.getOperand(0).getValueType() !=
+        InVec.getOperand(1).getValueType())
       return SDValue();
-    SDValue LHS = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, VT,
-                              InVecLHS, InValLHS, EltNo);
-    SDValue RHS = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, VT,
-                              InVecRHS, InValRHS, EltNo);
+    SDValue LHS =
+        DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, VT, InVecLHS, InValLHS, EltNo);
+    SDValue RHS =
+        DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, VT, InVecRHS, InValRHS, EltNo);
     return DAG.getNode(InVecOpcode, DL, VT, LHS, RHS);
   }
 
@@ -16142,8 +16293,8 @@ static SDValue performINSERT_VECTOR_ELTCombine(SDNode *N, SelectionDAG &DAG,
 
   unsigned ConcatOpIdx = Elt / ConcatNumElts;
   SDValue ConcatOp = InVec.getOperand(ConcatOpIdx);
-  ConcatOp = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, ConcatVT,
-                         ConcatOp, InVal, NewIdx);
+  ConcatOp = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, ConcatVT, ConcatOp, InVal,
+                         NewIdx);
 
   SmallVector<SDValue> ConcatOps;
   ConcatOps.append(InVec->op_begin(), InVec->op_end());
@@ -16250,9 +16401,8 @@ static SDValue performCONCAT_VECTORSCombine(SDNode *N, SelectionDAG &DAG,
   if (MustNegateStride)
     Stride = DAG.getNegative(Stride, DL, Stride.getValueType());
 
-  SDValue AllOneMask =
-    DAG.getSplat(WideVecVT.changeVectorElementType(MVT::i1), DL,
-                 DAG.getConstant(1, DL, MVT::i1));
+  SDValue AllOneMask = DAG.getSplat(WideVecVT.changeVectorElementType(MVT::i1),
+                                    DL, DAG.getConstant(1, DL, MVT::i1));
 
   uint64_t MemSize;
   if (auto *ConstStride = dyn_cast<ConstantSDNode>(Stride);
@@ -16352,15 +16502,16 @@ static SDValue combineToVWMACC(SDNode *N, SelectionDAG &DAG,
   return DAG.getNode(Opc, DL, VT, Ops);
 }
 
-static bool legalizeScatterGatherIndexType(SDLoc DL, SDValue &Index,
-                                           ISD::MemIndexType &IndexType,
-                                           RISCVTargetLowering::DAGCombinerInfo &DCI) {
+static bool
+legalizeScatterGatherIndexType(SDLoc DL, SDValue &Index,
+                               ISD::MemIndexType &IndexType,
+                               RISCVTargetLowering::DAGCombinerInfo &DCI) {
   if (!DCI.isBeforeLegalize())
     return false;
 
   SelectionDAG &DAG = DCI.DAG;
   const MVT XLenVT =
-    DAG.getMachineFunction().getSubtarget<RISCVSubtarget>().getXLenVT();
+      DAG.getMachineFunction().getSubtarget<RISCVSubtarget>().getXLenVT();
 
   const EVT IndexVT = Index.getValueType();
 
@@ -16435,7 +16586,7 @@ static bool matchIndexAsWiderOp(EVT VT, SDValue Index, SDValue Mask,
 
   const unsigned ElementSize = VT.getScalarStoreSize();
   const unsigned WiderElementSize = ElementSize * 2;
-  if (WiderElementSize > ST.getELen()/8)
+  if (WiderElementSize > ST.getELen() / 8)
     return false;
 
   if (!ST.enableUnalignedVectorMem() && BaseAlign < WiderElementSize)
@@ -16454,7 +16605,7 @@ static bool matchIndexAsWiderOp(EVT VT, SDValue Index, SDValue Mask,
         return false;
       continue;
     }
-    uint64_t Last = Index->getConstantOperandVal(i-1);
+    uint64_t Last = Index->getConstantOperandVal(i - 1);
     if (C != Last + ElementSize)
       return false;
   }
@@ -17003,8 +17154,8 @@ SDValue RISCVTargetLowering::PerformDAGCombine(SDNode *N,
     // FIXME: Generalize to other binary ops with identical operand?
     if (TrueV.getOpcode() == ISD::XOR && FalseV.getOpcode() == ISD::XOR &&
         TrueV.getOperand(1) == FalseV.getOperand(1) &&
-        isOneConstant(TrueV.getOperand(1)) &&
-        TrueV.hasOneUse() && FalseV.hasOneUse()) {
+        isOneConstant(TrueV.getOperand(1)) && TrueV.hasOneUse() &&
+        FalseV.hasOneUse()) {
       SDValue NewSel = DAG.getNode(RISCVISD::SELECT_CC, DL, VT, LHS, RHS, CC,
                                    TrueV.getOperand(0), FalseV.getOperand(0));
       return DAG.getNode(ISD::XOR, DL, VT, NewSel, TrueV.getOperand(1));
@@ -17116,13 +17267,12 @@ SDValue RISCVTargetLowering::PerformDAGCombine(SDNode *N,
     SmallVector<int> ShuffleMask;
     if (MGN->getExtensionType() == ISD::NON_EXTLOAD &&
         matchIndexAsShuffle(VT, Index, MGN->getMask(), ShuffleMask)) {
-      SDValue Load = DAG.getMaskedLoad(VT, DL, MGN->getChain(),
-                                       MGN->getBasePtr(), DAG.getUNDEF(XLenVT),
-                                       MGN->getMask(), DAG.getUNDEF(VT),
-                                       MGN->getMemoryVT(), MGN->getMemOperand(),
-                                       ISD::UNINDEXED, ISD::NON_EXTLOAD);
+      SDValue Load = DAG.getMaskedLoad(
+          VT, DL, MGN->getChain(), MGN->getBasePtr(), DAG.getUNDEF(XLenVT),
+          MGN->getMask(), DAG.getUNDEF(VT), MGN->getMemoryVT(),
+          MGN->getMemOperand(), ISD::UNINDEXED, ISD::NON_EXTLOAD);
       SDValue Shuffle =
-        DAG.getVectorShuffle(VT, DL, Load, DAG.getUNDEF(VT), ShuffleMask);
+          DAG.getVectorShuffle(VT, DL, Load, DAG.getUNDEF(VT), ShuffleMask);
       return DAG.getMergeValues({Shuffle, Load.getValue(1)}, DL);
     }
 
@@ -17132,8 +17282,8 @@ SDValue RISCVTargetLowering::PerformDAGCombine(SDNode *N,
       SmallVector<SDValue> NewIndices;
       for (unsigned i = 0; i < Index->getNumOperands(); i += 2)
         NewIndices.push_back(Index.getOperand(i));
-      EVT IndexVT = Index.getValueType()
-        .getHalfNumVectorElementsVT(*DAG.getContext());
+      EVT IndexVT =
+          Index.getValueType().getHalfNumVectorElementsVT(*DAG.getContext());
       Index = DAG.getBuildVector(IndexVT, DL, NewIndices);
 
       unsigned ElementSize = VT.getScalarStoreSize();
@@ -17147,17 +17297,16 @@ SDValue RISCVTargetLowering::PerformDAGCombine(SDNode *N,
                                     EltCnt.divideCoefficientBy(2));
       SDValue Mask = DAG.getSplat(MaskVT, DL, DAG.getConstant(1, DL, MVT::i1));
 
-      SDValue Gather =
-        DAG.getMaskedGather(DAG.getVTList(WideVT, MVT::Other), WideVT, DL,
-                            {MGN->getChain(), Passthru, Mask, MGN->getBasePtr(),
-                             Index, ScaleOp},
-                            MGN->getMemOperand(), IndexType, ISD::NON_EXTLOAD);
+      SDValue Gather = DAG.getMaskedGather(
+          DAG.getVTList(WideVT, MVT::Other), WideVT, DL,
+          {MGN->getChain(), Passthru, Mask, MGN->getBasePtr(), Index, ScaleOp},
+          MGN->getMemOperand(), IndexType, ISD::NON_EXTLOAD);
       SDValue Result = DAG.getBitcast(VT, Gather.getValue(0));
       return DAG.getMergeValues({Result, Gather.getValue(1)}, DL);
     }
     break;
   }
-  case ISD::MSCATTER:{
+  case ISD::MSCATTER: {
     const auto *MSN = cast<MaskedScatterSDNode>(N);
     SDValue Index = MSN->getIndex();
     SDValue ScaleOp = MSN->getScale();
@@ -17775,7 +17924,8 @@ bool RISCVTargetLowering::targetShrinkDemandedConstant(
   APInt NewMask = ShrunkMask;
   if (MinSignedBits <= 12)
     NewMask.setBitsFrom(11);
-  else if (!C->isOpaque() && MinSignedBits <= 32 && !ShrunkMask.isSignedIntN(32))
+  else if (!C->isOpaque() && MinSignedBits <= 32 &&
+           !ShrunkMask.isSignedIntN(32))
     NewMask.setBitsFrom(31);
   else
     return false;
@@ -17804,23 +17954,20 @@ static uint64_t computeGREVOrGORC(uint64_t x, unsigned ShAmt, bool IsGORC) {
   return x;
 }
 
-void RISCVTargetLowering::computeKnownBitsForTargetNode(const SDValue Op,
-                                                        KnownBits &Known,
-                                                        const APInt &DemandedElts,
-                                                        const SelectionDAG &DAG,
-                                                        unsigned Depth) const {
+void RISCVTargetLowering::computeKnownBitsForTargetNode(
+    const SDValue Op, KnownBits &Known, const APInt &DemandedElts,
+    const SelectionDAG &DAG, unsigned Depth) const {
   unsigned BitWidth = Known.getBitWidth();
   unsigned Opc = Op.getOpcode();
-  assert((Opc >= ISD::BUILTIN_OP_END ||
-          Opc == ISD::INTRINSIC_WO_CHAIN ||
-          Opc == ISD::INTRINSIC_W_CHAIN ||
-          Opc == ISD::INTRINSIC_VOID) &&
+  assert((Opc >= ISD::BUILTIN_OP_END || Opc == ISD::INTRINSIC_WO_CHAIN ||
+          Opc == ISD::INTRINSIC_W_CHAIN || Opc == ISD::INTRINSIC_VOID) &&
          "Should use MaskedValueIsZero if you don't know whether Op"
          " is a target node!");
 
   Known.resetAll();
   switch (Opc) {
-  default: break;
+  default:
+    break;
   case RISCVISD::SELECT_CC: {
     Known = DAG.computeKnownBits(Op.getOperand(4), Depth + 1);
     // If we don't know any bits, early out.
@@ -17889,8 +18036,7 @@ void RISCVTargetLowering::computeKnownBitsForTargetNode(const SDValue Op,
     Known = DAG.computeKnownBits(Op.getOperand(0), Depth + 1);
     bool IsGORC = Op.getOpcode() == RISCVISD::ORC_B;
     // To compute zeros, we need to invert the value and invert it back after.
-    Known.Zero =
-        ~computeGREVOrGORC(~Known.Zero.getZExtValue(), 7, IsGORC);
+    Known.Zero = ~computeGREVOrGORC(~Known.Zero.getZExtValue(), 7, IsGORC);
     Known.One = computeGREVOrGORC(Known.One.getZExtValue(), 7, IsGORC);
     break;
   }
@@ -17901,7 +18047,7 @@ void RISCVTargetLowering::computeKnownBitsForTargetNode(const SDValue Op,
     const unsigned MaxVLenB = Subtarget.getRealMaxVLen() / 8;
     assert(MinVLenB > 0 && "READ_VLENB without vector extension enabled?");
     Known.Zero.setLowBits(Log2_32(MinVLenB));
-    Known.Zero.setBitsFrom(Log2_32(MaxVLenB)+1);
+    Known.Zero.setBitsFrom(Log2_32(MaxVLenB) + 1);
     if (MaxVLenB == MinVLenB)
       Known.One.setBit(Log2_32(MinVLenB));
     break;
@@ -17954,7 +18100,8 @@ unsigned RISCVTargetLowering::ComputeNumSignBitsForTargetNode(
   case RISCVISD::SELECT_CC: {
     unsigned Tmp =
         DAG.ComputeNumSignBits(Op.getOperand(3), DemandedElts, Depth + 1);
-    if (Tmp == 1) return 1;  // Early out.
+    if (Tmp == 1)
+      return 1; // Early out.
     unsigned Tmp2 =
         DAG.ComputeNumSignBits(Op.getOperand(4), DemandedElts, Depth + 1);
     return std::min(Tmp, Tmp2);
@@ -17969,7 +18116,8 @@ unsigned RISCVTargetLowering::ComputeNumSignBitsForTargetNode(
     // if the input has at least 33 sign bits.
     unsigned Tmp =
         DAG.ComputeNumSignBits(Op.getOperand(0), DemandedElts, Depth + 1);
-    if (Tmp < 33) return 1;
+    if (Tmp < 33)
+      return 1;
     return 33;
   }
   case RISCVISD::SLLW:
@@ -18083,8 +18231,8 @@ RISCVTargetLowering::getTargetConstantFromLoad(LoadSDNode *Ld) const {
   auto *CNodeLo = GetSupportedConstantPool(Ptr.getOperand(1));
   auto *CNodeHi = GetSupportedConstantPool(Ptr.getOperand(0).getOperand(0));
 
-  if (!CNodeLo || CNodeLo->getTargetFlags() != RISCVII::MO_LO ||
-      !CNodeHi || CNodeHi->getTargetFlags() != RISCVII::MO_HI)
+  if (!CNodeLo || CNodeLo->getTargetFlags() != RISCVII::MO_LO || !CNodeHi ||
+      CNodeHi->getTargetFlags() != RISCVII::MO_HI)
     return nullptr;
 
   if (CNodeLo->getConstVal() != CNodeHi->getConstVal())
@@ -18193,9 +18341,9 @@ static MachineBasicBlock *emitSplitF64Pseudo(MachineInstr &MI,
   return BB;
 }
 
-static MachineBasicBlock *emitBuildPairF64Pseudo(MachineInstr &MI,
-                                                 MachineBasicBlock *BB,
-                                                 const RISCVSubtarget &Subtarget) {
+static MachineBasicBlock *
+emitBuildPairF64Pseudo(MachineInstr &MI, MachineBasicBlock *BB,
+                       const RISCVSubtarget &Subtarget) {
   assert(MI.getOpcode() == RISCV::BuildPairF64Pseudo &&
          "Unexpected instruction");
 
@@ -18883,18 +19031,15 @@ void RISCVTargetLowering::AdjustInstrPostInstrSelection(MachineInstr &MI,
 // register-size fields in the same situations they would be for fixed
 // arguments.
 
-static const MCPhysReg ArgFPR16s[] = {
-  RISCV::F10_H, RISCV::F11_H, RISCV::F12_H, RISCV::F13_H,
-  RISCV::F14_H, RISCV::F15_H, RISCV::F16_H, RISCV::F17_H
-};
-static const MCPhysReg ArgFPR32s[] = {
-  RISCV::F10_F, RISCV::F11_F, RISCV::F12_F, RISCV::F13_F,
-  RISCV::F14_F, RISCV::F15_F, RISCV::F16_F, RISCV::F17_F
-};
-static const MCPhysReg ArgFPR64s[] = {
-  RISCV::F10_D, RISCV::F11_D, RISCV::F12_D, RISCV::F13_D,
-  RISCV::F14_D, RISCV::F15_D, RISCV::F16_D, RISCV::F17_D
-};
+static const MCPhysReg ArgFPR16s[] = {RISCV::F10_H, RISCV::F11_H, RISCV::F12_H,
+                                      RISCV::F13_H, RISCV::F14_H, RISCV::F15_H,
+                                      RISCV::F16_H, RISCV::F17_H};
+static const MCPhysReg ArgFPR32s[] = {RISCV::F10_F, RISCV::F11_F, RISCV::F12_F,
+                                      RISCV::F13_F, RISCV::F14_F, RISCV::F15_F,
+                                      RISCV::F16_F, RISCV::F17_F};
+static const MCPhysReg ArgFPR64s[] = {RISCV::F10_D, RISCV::F11_D, RISCV::F12_D,
+                                      RISCV::F13_D, RISCV::F14_D, RISCV::F15_D,
+                                      RISCV::F16_D, RISCV::F17_D};
 // This is an interim calling convention and it may be changed in the future.
 static const MCPhysReg ArgVRs[] = {
     RISCV::V8,  RISCV::V9,  RISCV::V10, RISCV::V11, RISCV::V12, RISCV::V13,
@@ -19271,8 +19416,8 @@ void RISCVTargetLowering::analyzeInputArgs(
     if (Fn(MF.getDataLayout(), ABI, i, ArgVT, ArgVT, CCValAssign::Full,
            ArgFlags, CCInfo, /*IsFixed=*/true, IsRet, ArgTy, *this,
            Dispatcher)) {
-      LLVM_DEBUG(dbgs() << "InputArg #" << i << " has unhandled type "
-                        << ArgVT << '\n');
+      LLVM_DEBUG(dbgs() << "InputArg #" << i << " has unhandled type " << ArgVT
+                        << '\n');
       llvm_unreachable(nullptr);
     }
   }
@@ -19301,8 +19446,8 @@ void RISCVTargetLowering::analyzeOutputArgs(
     if (Fn(MF.getDataLayout(), ABI, i, ArgVT, ArgVT, CCValAssign::Full,
            ArgFlags, CCInfo, Outs[i].IsFixed, IsRet, OrigTy, *this,
            Dispatcher)) {
-      LLVM_DEBUG(dbgs() << "OutputArg #" << i << " has unhandled type "
-                        << ArgVT << "\n");
+      LLVM_DEBUG(dbgs() << "OutputArg #" << i << " has unhandled type " << ArgVT
+                        << "\n");
       llvm_unreachable(nullptr);
     }
   }
@@ -19593,8 +19738,8 @@ bool RISCV::CC_RISCV_FastCC(const DataLayout &DL, RISCVABI::ABI ABI,
 }
 
 bool RISCV::CC_RISCV_GHC(unsigned ValNo, MVT ValVT, MVT LocVT,
-                         CCValAssign::LocInfo LocInfo,
-                         ISD::ArgFlagsTy ArgFlags, CCState &State) {
+                         CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
+                         CCState &State) {
   if (ArgFlags.isNest()) {
     report_fatal_error(
         "Attribute 'nest' is not supported in GHC calling convention");
@@ -19619,7 +19764,7 @@ bool RISCV::CC_RISCV_GHC(unsigned ValNo, MVT ValVT, MVT LocVT,
   if (LocVT == MVT::f32 && Subtarget.hasStdExtF()) {
     // Pass in STG registers: F1, ..., F6
     //                        fs0 ... fs5
-    static const MCPhysReg FPR32List[] = {RISCV::F8_F, RISCV::F9_F,
+    static const MCPhysReg FPR32List[] = {RISCV::F8_F,  RISCV::F9_F,
                                           RISCV::F18_F, RISCV::F19_F,
                                           RISCV::F20_F, RISCV::F21_F};
     if (unsigned Reg = State.AllocateReg(FPR32List)) {
@@ -19682,14 +19827,14 @@ SDValue RISCVTargetLowering::LowerFormalArguments(
   if (Func.hasFnAttribute("interrupt")) {
     if (!Func.arg_empty())
       report_fatal_error(
-        "Functions with the interrupt attribute cannot have arguments!");
+          "Functions with the interrupt attribute cannot have arguments!");
 
     StringRef Kind =
-      MF.getFunction().getFnAttribute("interrupt").getValueAsString();
+        MF.getFunction().getFnAttribute("interrupt").getValueAsString();
 
     if (!(Kind == "user" || Kind == "supervisor" || Kind == "machine"))
       report_fatal_error(
-        "Function interrupt attribute argument not supported!");
+          "Function interrupt attribute argument not supported!");
   }
 
   EVT PtrVT = getPointerTy(DAG.getDataLayout());
@@ -20105,7 +20250,8 @@ SDValue RISCVTargetLowering::LowerCall(CallLoweringInfo &CLI,
     const GlobalValue *GV = S->getGlobal();
     Callee = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0, RISCVII::MO_CALL);
   } else if (ExternalSymbolSDNode *S = dyn_cast<ExternalSymbolSDNode>(Callee)) {
-    Callee = DAG.getTargetExternalSymbol(S->getSymbol(), PtrVT, RISCVII::MO_CALL);
+    Callee =
+        DAG.getTargetExternalSymbol(S->getSymbol(), PtrVT, RISCVII::MO_CALL);
   }
 
   // The first call operand is the chain and the second is the target address.
@@ -20299,7 +20445,7 @@ RISCVTargetLowering::LowerReturn(SDValue Chain, CallingConv::ID CallConv,
 
     MachineFunction &MF = DAG.getMachineFunction();
     StringRef Kind =
-      MF.getFunction().getFnAttribute("interrupt").getValueAsString();
+        MF.getFunction().getFnAttribute("interrupt").getValueAsString();
 
     if (Kind == "supervisor")
       RetOpc = RISCVISD::SRET_GLUE;
@@ -20623,6 +20769,7 @@ const char *RISCVTargetLowering::getTargetNodeName(unsigned Opcode) const {
   NODE_NAME_CASE(SF_VC_V_XVW_SE)
   NODE_NAME_CASE(SF_VC_V_IVW_SE)
   NODE_NAME_CASE(SF_VC_V_VVW_SE)
+  NODE_NAME_CASE(VREVERSEMTK_V_VL)
   NODE_NAME_CASE(SF_VC_V_FVW_SE)
   }
   // clang-format on
@@ -21334,7 +21481,8 @@ bool RISCVTargetLowering::shouldExtendTypeInLibCall(EVT Type) const {
   return true;
 }
 
-bool RISCVTargetLowering::shouldSignExtendTypeInLibCall(EVT Type, bool IsSigned) const {
+bool RISCVTargetLowering::shouldSignExtendTypeInLibCall(EVT Type,
+                                                        bool IsSigned) const {
   if (Subtarget.is64Bit() && Type == MVT::i32)
     return true;
 
@@ -21429,9 +21577,8 @@ bool RISCVTargetLowering::allowsMisalignedMemoryAccesses(
   return Subtarget.enableUnalignedVectorMem();
 }
 
-
-EVT RISCVTargetLowering::getOptimalMemOpType(const MemOp &Op,
-                                             const AttributeList &FuncAttributes) const {
+EVT RISCVTargetLowering::getOptimalMemOpType(
+    const MemOp &Op, const AttributeList &FuncAttributes) const {
   if (!Subtarget.hasVInstructions())
     return MVT::Other;
 
@@ -21446,7 +21593,7 @@ EVT RISCVTargetLowering::getOptimalMemOpType(const MemOp &Op,
   // combining will typically form larger LMUL operations from the LMUL1
   // operations emitted here, and that's okay because combining isn't
   // introducing new memory operations; it's just merging existing ones.
-  const unsigned MinVLenInBytes = Subtarget.getRealMinVLen()/8;
+  const unsigned MinVLenInBytes = Subtarget.getRealMinVLen() / 8;
   if (Op.size() < MinVLenInBytes)
     // TODO: Figure out short memops.  For the moment, do the default thing
     // which ends up using scalar sequences.
@@ -21469,7 +21616,8 @@ EVT RISCVTargetLowering::getOptimalMemOpType(const MemOp &Op,
       RequiredAlign = std::min(RequiredAlign, Op.getSrcAlign());
     PreferredVT = MVT::getIntegerVT(RequiredAlign.value() * 8);
   }
-  return MVT::getVectorVT(PreferredVT, MinVLenInBytes/PreferredVT.getStoreSize());
+  return MVT::getVectorVT(PreferredVT,
+                          MinVLenInBytes / PreferredVT.getStoreSize());
 }
 
 bool RISCVTargetLowering::splitValueIntoRegisterParts(
@@ -21657,7 +21805,8 @@ bool RISCVTargetLowering::isLegalStridedLoadStore(EVT DataType,
     return false;
 
   // Only support fixed vectors if we know the minimum vector size.
-  if (DataType.isFixedLengthVector() && !Subtarget.useRVVForFixedLengthVectors())
+  if (DataType.isFixedLengthVector() &&
+      !Subtarget.useRVVForFixedLengthVectors())
     return false;
 
   EVT ScalarType = DataType.getScalarType();
diff --git a/llvm/lib/Target/RISCV/RISCVISelLowering.h b/llvm/lib/Target/RISCV/RISCVISelLowering.h
index 0b0ad9229..a2ea3b61d 100644
--- a/llvm/lib/Target/RISCV/RISCVISelLowering.h
+++ b/llvm/lib/Target/RISCV/RISCVISelLowering.h
@@ -372,7 +372,7 @@ enum NodeType : unsigned {
   VRGATHER_VX_VL,
   VRGATHER_VV_VL,
   VRGATHEREI16_VV_VL,
-
+  
   // Vector sign/zero extend with additional mask & VL operands.
   VSEXT_VL,
   VZEXT_VL,
@@ -411,6 +411,7 @@ enum NodeType : unsigned {
   /// Software guarded BRIND node. Operand 0 is the chain operand and
   /// operand 1 is the target address.
   SW_GUARDED_BRIND,
+  VREVERSEMTK_V_VL,
 
   // FP to 32 bit int conversions for RV64. These are used to keep track of the
   // result being sign extended to 64 bit. These saturate out of range inputs.
@@ -954,6 +955,7 @@ private:
   SDValue lowerVECTOR_INTERLEAVE(SDValue Op, SelectionDAG &DAG) const;
   SDValue lowerSTEP_VECTOR(SDValue Op, SelectionDAG &DAG) const;
   SDValue lowerVECTOR_REVERSE(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerVECTOR_REVERSE_MTK(SDValue Op, SelectionDAG &DAG) const;
   SDValue lowerVECTOR_SPLICE(SDValue Op, SelectionDAG &DAG) const;
   SDValue lowerABS(SDValue Op, SelectionDAG &DAG) const;
   SDValue lowerMaskedLoad(SDValue Op, SelectionDAG &DAG) const;
diff --git a/llvm/lib/Target/RISCV/RISCVInstrInfo.td b/llvm/lib/Target/RISCV/RISCVInstrInfo.td
index 04054d2c3..f4e55ec68 100644
--- a/llvm/lib/Target/RISCV/RISCVInstrInfo.td
+++ b/llvm/lib/Target/RISCV/RISCVInstrInfo.td
@@ -2087,3 +2087,7 @@ include "RISCVInstrInfoXwch.td"
 //===----------------------------------------------------------------------===//
 
 include "RISCVInstrGISel.td"
+
+
+// MTK try try
+include "RISCVInstrInfoMTKVReverse.td"
\ No newline at end of file
diff --git a/llvm/lib/Target/RISCV/RISCVInstrInfoMTKVReverse.td b/llvm/lib/Target/RISCV/RISCVInstrInfoMTKVReverse.td
new file mode 100644
index 000000000..617ec99d1
--- /dev/null
+++ b/llvm/lib/Target/RISCV/RISCVInstrInfoMTKVReverse.td
@@ -0,0 +1,138 @@
+//===----------------------------------------------------------------------===// 
+// try try VREV instructions
+//===----------------------------------------------------------------------===//
+// Define the base class for all reverse instructions
+class RVInstV_MTK<bits<6> funct6, bits<5> vs1, RISCVVFormat opv, dag outs,
+              dag ins, string opcodestr, string argstr>
+    : RVInst<outs, ins, opcodestr, argstr, [], InstFormatR> {
+  bits<5> vs2;
+  bits<5> vd;
+  bit vm = 0b1; 
+
+  let Inst{31-26} = funct6;
+  let Inst{25} = vm; // value 1 means don't use mask
+  let Inst{24-20} = vs2;
+  let Inst{19-15} = vs1;
+  let Inst{14-12} = opv.Value; // 0b010 -> Defined in RISCVInstrFormatsV.td
+  let Inst{11-7} = vd;
+  let Inst{6-0} = OPC_OP_V_MTK.Value; // 0b0101011 -> Defined in RISCVInstrFormats.td
+
+  let Uses = [VTYPE, VL];
+  let RVVConstraint = VMConstraint;
+}
+
+// Inherit from the base class and add VRegClass parameter for register class flexibility
+
+let hasSideEffects = 0, mayLoad = 0, mayStore = 0 in {
+// op vd, vs2, 
+class VALUVs2_MTK<bits<6> funct6, bits<5> vs1, RISCVVFormat opv, string opcodestr>
+    : RVInstV_MTK<funct6, vs1, opv, (outs VR:$vd),
+               (ins VR:$vs2),
+               opcodestr, "$vd, $vs2">;
+}
+
+
+def Vrev     : RISCVVConstraint<!or(VS2Constraint.Value,
+                                        VMConstraint.Value)>;
+
+// Usually it will be defined in RISCVInstrInfo.td
+// SDTVecReverse is defined in TargetSelectionDAG.td
+def riscv_vreversemtk_v_vl : SDNode<"RISCVISD::VREVERSEMTK_V_VL", SDTVecReverse>;
+
+
+// VREV_V_e8M1, VREV_V_e8M2, VREV_V_e8M4, VREV_V_e8M8 -> vrev8m1, vrev8m2, vrev8m4, vrev8m8
+// Constraints = "@earlyclobber $vd"
+// let Constraints = "@earlyclobber $vd", RVVConstraint = Vrev in {
+  def VREV_V : VALUVs2_MTK<0b010010, 0b01001, OPMVV, "vrev.v">;
+// }
+
+
+//===----------------------------------------------------------------------===//
+// try try Pseudo instructions
+//===----------------------------------------------------------------------===//
+
+
+// VPseudoUnaryNoMask and VPseudoUnaryMask are defined in RISCVInstrInfoVPseudos.td
+multiclass VPseudoUnaryV_V_REV<LMULInfo m> {
+  let VLMul = m.value in {
+    foreach eew = [8, 16, 32, 64] in {
+      def "_V_E"#eew#"_"#m.MX : VPseudoUnaryNoMask<m.vrclass, m.vrclass> {
+        let Predicates = [HasVInstructions];
+      }
+      def "_V_E"#eew#"_"#m.MX#"_MASK" : VPseudoUnaryMask<m.vrclass, m.vrclass>, RISCVMaskedPseudo<MaskIdx=2> {
+        let Predicates = [HasVInstructions];
+      }
+    } 
+  } 
+}
+
+// Define a class to hold the Suffix string and LMULInfo object pair
+class SuffixLmulPair<string suffix, LMULInfo lmul> {
+  string Suffix = suffix;
+  LMULInfo Lmul = lmul;
+}
+
+// Define the list using instances of the SuffixLmulPair class
+defvar MF_PAIRS = [
+  SuffixLmulPair<"_V_E8_MF2", V_MF2>,
+  SuffixLmulPair<"_V_E8_MF4", V_MF4>,
+  SuffixLmulPair<"_V_E8_MF8", V_MF8>,
+  SuffixLmulPair<"_V_E16_MF4", V_MF4>,
+  SuffixLmulPair<"_V_E16_MF2", V_MF2>,
+  SuffixLmulPair<"_V_E32_MF2", V_MF2>
+];
+
+multiclass VPseudoUnaryV_V_REV_MF {
+  let Predicates = [HasVInstructions] in {
+    // Iterate over the list of SuffixLmulPair instances
+    foreach pair = MF_PAIRS in {
+      // Access the fields by their defined names
+      defvar suffix_name = pair.Suffix;
+      defvar lmul_info = pair.Lmul;
+
+      // Define the unmasked pseudo instruction
+      def suffix_name : VPseudoUnaryNoMask<VR, VR>, SchedUnary<"WriteVREV8V", "ReadVREV8V", lmul_info.MX, forceMergeOpRead=true>;
+
+      // Define the masked pseudo instruction
+      def suffix_name # "_MASK" : VPseudoUnaryMask<VR, VR>, RISCVMaskedPseudo<MaskIdx=2>, SchedUnary<"WriteVREV8V", "ReadVREV8V", lmul_info.MX, forceMergeOpRead=true>;
+    }
+  }
+}
+
+
+
+defvar MxList_REV = [V_M1, V_M2, V_M4, V_M8];
+
+multiclass VPseudoVREV {
+  foreach m = MxList_REV in {
+    defm "" : VPseudoUnaryV_V_REV<m>, 
+              SchedUnary<"WriteVREV8V", "ReadVREV8V", m.MX, forceMergeOpRead=true>;
+  }
+  defm "" : VPseudoUnaryV_V_REV_MF;
+}
+
+defm PseudoVREV : VPseudoVREV;
+
+//===----------------------------------------------------------------------===//
+// try try Pseudo instructions Pattern match
+//===----------------------------------------------------------------------===//
+// VPatUnarySDNode_V
+// VPatUnaryVL_V
+multiclass VPatUnarySDNode_V_REV<SDPatternOperator op, string instruction_name,
+                             Predicate predicate = HasVInstructions> {
+  foreach vti = [VI8MF8, VI8MF4, VI8MF2, VI8M1, VI8M2, VI8M4, VI8M8, VI16MF4, VI16MF2, VI16M1, VI16M2, VI16M4, VI16M8, VI32MF2, VI32M1, VI32M2, VI32M4, VI32M8, VI64M1, VI64M2, VI64M4, VI64M8,
+  VF16MF4, VF16MF2, VF32MF2, VF16M1, VF32M1, VF64M1, VF16M2, VF16M4, VF16M8, VF32M2, VF32M4, VF32M8, VF64M2, VF64M4, VF64M8] in {
+    let Predicates = !listconcat([predicate],
+                                 GetVTypePredicates<vti>.Predicates) in {
+      def : Pat<(vti.Vector (op (vti.Vector vti.RegClass:$vs2))),
+                (!cast<Instruction>(instruction_name#"_V_E"#vti.SEW#"_"#vti.LMul.MX)
+                  (vti.Vector (IMPLICIT_DEF)),
+                   vti.RegClass:$vs2,
+                   vti.AVL, vti.Log2SEW, TA_MA)>; 
+    }
+  }
+}
+defm : VPatUnarySDNode_V_REV<riscv_vreversemtk_v_vl, "PseudoVREV", HasVInstructions>;
+
+
+// Pseudo Instruction PseudoVREV_V_e32M2 -> VREV_V_e32M2
\ No newline at end of file
diff --git a/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp b/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp
index 5a92d6bab..fd6d02045 100644
--- a/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp
+++ b/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp
@@ -17,9 +17,15 @@
 #include "llvm/IR/PatternMatch.h"
 #include <cmath>
 #include <optional>
+#include "llvm/ADT/SmallString.h"
+#include <string>
+
 using namespace llvm;
 using namespace llvm::PatternMatch;
-
+extern SmallString<256> ttilog;
+extern SmallString<256> ttislp;
+#define STR(a) std::to_string(a.getValue().value())
+#define STRI(a) std::to_string(a)
 #define DEBUG_TYPE "riscvtti"
 
 static cl::opt<unsigned> RVVRegisterWidthLMUL(
@@ -306,11 +312,14 @@ std::optional<unsigned> RISCVTTIImpl::getMaxVScale() const {
 }
 
 std::optional<unsigned> RISCVTTIImpl::getVScaleForTuning() const {
+  // Check if the target has vector instructions
   if (ST->hasVInstructions())
+    // get the minimum vector length
     if (unsigned MinVLen = ST->getRealMinVLen();
         MinVLen >= RISCV::RVVBitsPerBlock)
+      // return the vscale value
       return MinVLen / RISCV::RVVBitsPerBlock;
-  return BaseT::getVScaleForTuning();
+  return BaseT::getVScaleForTuning(); // return std::nullopt
 }
 
 TypeSize
@@ -366,6 +375,8 @@ InstructionCost RISCVTTIImpl::getShuffleCost(TTI::ShuffleKind Kind,
   // First, handle cases where having a fixed length vector enables us to
   // give a more accurate cost than falling back to generic scalable codegen.
   // TODO: Each of these cases hints at a modeling gap around scalable vectors.
+  // llvm::outs() << "@@MVT: " << LT.second << "\n";
+
   if (isa<FixedVectorType>(Tp)) {
     switch (Kind) {
     default:
@@ -601,7 +612,17 @@ InstructionCost RISCVTTIImpl::getShuffleCost(TTI::ShuffleKind Kind,
         getRISCVInstructionCost(Opcodes, LT.second, CostKind);
     // Mask operation additionally required extend and truncate
     InstructionCost ExtendCost = Tp->getElementType()->isIntegerTy(1) ? 3 : 0;
-    return LT.first * (LenCost + GatherCost + ExtendCost);
+    InstructionCost ShuffleCost = LT.first * (LenCost + GatherCost + ExtendCost);
+    // ttilog += "-> ShuffleCost(" + STR(ShuffleCost) + ") = LT.first("+STR(LT.first) + ") * (" + STR(LenCost) + " + " + STR(GatherCost) + " + " + STR(ExtendCost) + ")";
+    ttilog += "-> ShuffleCost(1)";
+    ttilog += "\t-> Mask = [";
+    for (size_t i = 0; i < Mask.size();i++){
+      ttilog += STRI(Mask[i]) + ", ";
+    }
+    ttilog += "]";
+    ttilog += "\t-> ASM: [VID_V, VRSUB_VX, VRGATHER_VV]";
+    // return ShuffleCost;
+    return (LT.second.isFixedLengthVector()) ? ShuffleCost : 1;
   }
   }
   return BaseT::getShuffleCost(Kind, Tp, Mask, CostKind, Index, SubTp);
diff --git a/llvm/lib/Target/RISCV/tryVrgather.td b/llvm/lib/Target/RISCV/tryVrgather.td
new file mode 100644
index 000000000..3bbdfd85b
--- /dev/null
+++ b/llvm/lib/Target/RISCV/tryVrgather.td
@@ -0,0 +1,242 @@
+// In this file, I am gonna try to trace all the code related to vrgather
+
+//===----------------------------------------------------------------------===//
+// RISCVInstrFormats.td
+//===----------------------------------------------------------------------===//
+// vrgather: The destination vector register group cannot overlap with the
+// source vector register groups.
+def Vrgather     : RISCVVConstraint<!or(VS2Constraint.Value,
+                                        VS1Constraint.Value,
+                                        VMConstraint.Value)>;
+
+
+//===----------------------------------------------------------------------===//
+// RISCVInstrInfoV.td
+//===----------------------------------------------------------------------===//
+multiclass VGTR_IV_V_X_I<string opcodestr, bits<6> funct6> {
+  def V  : VALUVV<funct6, OPIVV, opcodestr # ".vv">,
+           SchedBinaryMC<"WriteVRGatherVV", "ReadVRGatherVV_data",
+                         "ReadVRGatherVV_index">;
+  def X  : VALUVX<funct6, OPIVX, opcodestr # ".vx">,
+           SchedBinaryMC<"WriteVRGatherVX", "ReadVRGatherVX_data",
+                         "ReadVRGatherVX_index">;
+  def I  : VALUVI<funct6, opcodestr # ".vi", uimm5>,
+           SchedUnaryMC<"WriteVRGatherVI", "ReadVRGatherVI_data">;
+}
+
+
+let Predicates = [HasVInstructions] in {
+// Vector Register Gather Instruction
+let Constraints = "@earlyclobber $vd", RVVConstraint = Vrgather in {
+    defm VRGATHER_V : VGTR_IV_V_X_I<"vrgather", 0b001100>;
+    def VRGATHEREI16_VV : VALUVV<0b001110, OPIVV, "vrgatherei16.vv">,
+                        SchedBinaryMC<"WriteVRGatherEI16VV",
+                                        "ReadVRGatherEI16VV_data",
+                                        "ReadVRGatherEI16VV_index">;
+} // Constraints = "@earlyclobber $vd", RVVConstraint = Vrgather
+} // Predicates = [HasVInstructions]
+
+//===----------------------------------------------------------------------===//
+// RISCVInstrInfoVPseudos.td
+//===----------------------------------------------------------------------===//
+multiclass VPseudoVGTR_EI16_VV {
+  defvar constraint = "@earlyclobber $rd";
+  foreach m = MxList in {
+    defvar mx = m.MX;
+    foreach sew = EEWList in {
+      defvar dataEMULOctuple = m.octuple;
+      // emul = lmul * 16 / sew
+      defvar idxEMULOctuple = !srl(!mul(dataEMULOctuple, 16), !logtwo(sew));
+      if !and(!ge(idxEMULOctuple, 1), !le(idxEMULOctuple, 64)) then {
+        defvar emulMX = octuple_to_str<idxEMULOctuple>.ret;
+        defvar emul = !cast<LMULInfo>("V_" # emulMX);
+        defvar sews = SchedSEWSet<mx>.val;
+        foreach e = sews in {
+          defm _VV
+              : VPseudoBinaryEmul<m.vrclass, m.vrclass, emul.vrclass, m, emul,
+                                  constraint, e>,
+                SchedBinary<"WriteVRGatherEI16VV", "ReadVRGatherEI16VV_data",
+                            "ReadVRGatherEI16VV_index", mx, e, forceMergeOpRead=true>;
+        }
+      }
+    }
+  }
+}
+
+
+multiclass VPseudoVGTR_VV_VX_VI {
+  defvar constraint = "@earlyclobber $rd";
+  // MxList is the possible LMUL values -> [V_MF8, V_MF4, V_MF2, V_M1, V_M2, V_M4, V_M8];
+  foreach m = MxList in {
+    defvar mx = m.MX; // mx is LMUL value (e.g. V_MF8 = 1/8, V_MF4 = 1/4, etc.)
+    defm "" : VPseudoBinaryV_VX<m, constraint>,
+              SchedBinary<"WriteVRGatherVX", "ReadVRGatherVX_data",
+                          "ReadVRGatherVX_index", mx, forceMergeOpRead=true>;
+    defm "" : VPseudoBinaryV_VI<uimm5, m, constraint>,
+              SchedUnary<"WriteVRGatherVI", "ReadVRGatherVI_data", mx,
+                         forceMergeOpRead=true>;
+
+    defvar sews = SchedSEWSet<mx>.val;
+    foreach e = sews in {
+      defm "" : VPseudoBinaryV_VV<m, constraint, e>,
+                SchedBinary<"WriteVRGatherVV", "ReadVRGatherVV_data",
+                              "ReadVRGatherVV_index", mx, e, forceMergeOpRead=true>;
+    }
+  }
+}
+
+let Predicates = [HasVInstructions] in {
+defm PseudoVRGATHER     : VPseudoVGTR_VV_VX_VI;
+defm PseudoVRGATHEREI16 : VPseudoVGTR_EI16_VV;
+} // Predicates = [HasVInstructions]
+
+
+// Intrinsic Patterns
+defm : VPatBinaryV_VV_VX_VI_INT<"int_riscv_vrgather", "PseudoVRGATHER",
+                                AllIntegerVectors, uimm5>;
+                                
+defm : VPatBinaryV_VV_VX_VI_INT<"int_riscv_vrgather", "PseudoVRGATHER",
+                                AllFloatVectors, uimm5>;
+
+defm : VPatBinaryV_VV_INT_EEW<"int_riscv_vrgatherei16_vv", "PseudoVRGATHEREI16",
+                              eew=16, vtilist=AllIntegerVectors>;
+
+
+defm : VPatBinaryV_VV_INT_EEW<"int_riscv_vrgatherei16_vv", "PseudoVRGATHEREI16",
+                              eew=16, vtilist=AllFloatVectors>;
+
+//===----------------------------------------------------------------------===//
+// RISCVInstrInfoVVLPatterns.td
+//===----------------------------------------------------------------------===//
+def riscv_vrgather_vx_vl : SDNode<"RISCVISD::VRGATHER_VX_VL",
+                                  SDTypeProfile<1, 5, [SDTCisVec<0>,
+                                                       SDTCisSameAs<0, 1>,
+                                                       SDTCisVT<2, XLenVT>,
+                                                       SDTCisSameAs<0, 3>,
+                                                       SDTCVecEltisVT<4, i1>,
+                                                       SDTCisSameNumEltsAs<0, 4>,
+                                                       SDTCisVT<5, XLenVT>]>>;
+def riscv_vrgather_vv_vl : SDNode<"RISCVISD::VRGATHER_VV_VL",
+                                  SDTypeProfile<1, 5, [SDTCisVec<0>,
+                                                       SDTCisSameAs<0, 1>,
+                                                       SDTCisInt<2>,
+                                                       SDTCisSameNumEltsAs<0, 2>,
+                                                       SDTCisSameSizeAs<0, 2>,
+                                                       SDTCisSameAs<0, 3>,
+                                                       SDTCVecEltisVT<4, i1>,
+                                                       SDTCisSameNumEltsAs<0, 4>,
+                                                       SDTCisVT<5, XLenVT>]>>;
+def riscv_vrgatherei16_vv_vl : SDNode<"RISCVISD::VRGATHEREI16_VV_VL",
+                                      SDTypeProfile<1, 5, [SDTCisVec<0>,
+                                                           SDTCisSameAs<0, 1>,
+                                                           SDTCisInt<2>,
+                                                           SDTCVecEltisVT<2, i16>,
+                                                           SDTCisSameNumEltsAs<0, 2>,
+                                                           SDTCisSameAs<0, 3>,
+                                                           SDTCVecEltisVT<4, i1>,
+                                                           SDTCisSameNumEltsAs<0, 4>,
+                                                           SDTCisVT<5, XLenVT>]>>;
+
+
+// 16.4. Vector Register Gather Instruction
+// Integer Vector Gather Instruction
+foreach vti = AllIntegerVectors in {
+  let Predicates = GetVTypePredicates<vti>.Predicates in {
+    def : Pat<(vti.Vector (riscv_vrgather_vv_vl vti.RegClass:$rs2,
+                                                vti.RegClass:$rs1,
+                                                vti.RegClass:$merge,
+                                                (vti.Mask V0),
+                                                VLOpFrag)),
+              (!cast<Instruction>("PseudoVRGATHER_VV_"# vti.LMul.MX#"_E"# vti.SEW#"_MASK")
+                   vti.RegClass:$merge, vti.RegClass:$rs2, vti.RegClass:$rs1,
+                   (vti.Mask V0), GPR:$vl, vti.Log2SEW, TAIL_AGNOSTIC)>;
+    def : Pat<(vti.Vector (riscv_vrgather_vx_vl vti.RegClass:$rs2, GPR:$rs1,
+                                                vti.RegClass:$merge,
+                                                (vti.Mask V0),
+                                                VLOpFrag)),
+              (!cast<Instruction>("PseudoVRGATHER_VX_"# vti.LMul.MX#"_MASK")
+                   vti.RegClass:$merge, vti.RegClass:$rs2, GPR:$rs1,
+                   (vti.Mask V0), GPR:$vl, vti.Log2SEW, TAIL_AGNOSTIC)>;
+    def : Pat<(vti.Vector (riscv_vrgather_vx_vl vti.RegClass:$rs2,
+                                                uimm5:$imm,
+                                                vti.RegClass:$merge,
+                                                (vti.Mask V0),
+                                                VLOpFrag)),
+              (!cast<Instruction>("PseudoVRGATHER_VI_"# vti.LMul.MX#"_MASK")
+                   vti.RegClass:$merge, vti.RegClass:$rs2, uimm5:$imm,
+                   (vti.Mask V0), GPR:$vl, vti.Log2SEW, TAIL_AGNOSTIC)>;
+  }
+
+  // emul = lmul * 16 / sew
+  defvar vlmul = vti.LMul;
+  defvar octuple_lmul = vlmul.octuple;
+  defvar octuple_emul = !srl(!mul(octuple_lmul, 16), vti.Log2SEW);
+  if !and(!ge(octuple_emul, 1), !le(octuple_emul, 64)) then {
+    defvar emul_str = octuple_to_str<octuple_emul>.ret;
+    defvar ivti = !cast<VTypeInfo>("VI16" # emul_str);
+    defvar inst = "PseudoVRGATHEREI16_VV_" # vti.LMul.MX # "_E" # vti.SEW # "_" # emul_str;
+    let Predicates = GetVTypePredicates<vti>.Predicates in
+    def : Pat<(vti.Vector
+               (riscv_vrgatherei16_vv_vl vti.RegClass:$rs2,
+                                         (ivti.Vector ivti.RegClass:$rs1),
+                                         vti.RegClass:$merge,
+                                         (vti.Mask V0),
+                                         VLOpFrag)),
+              (!cast<Instruction>(inst#"_MASK")
+                   vti.RegClass:$merge, vti.RegClass:$rs2, ivti.RegClass:$rs1,
+                   (vti.Mask V0), GPR:$vl, vti.Log2SEW, TAIL_AGNOSTIC)>;
+  }
+}
+
+
+// Floating Point Vector Gather Instruction
+foreach vti = AllFloatVectors in {
+  defvar ivti = GetIntVTypeInfo<vti>.Vti;
+  let Predicates = GetVTypePredicates<ivti>.Predicates in {
+    def : Pat<(vti.Vector
+               (riscv_vrgather_vv_vl vti.RegClass:$rs2,
+                                     (ivti.Vector vti.RegClass:$rs1),
+                                     vti.RegClass:$merge,
+                                     (vti.Mask V0),
+                                     VLOpFrag)),
+              (!cast<Instruction>("PseudoVRGATHER_VV_"# vti.LMul.MX#"_E"# vti.SEW#"_MASK")
+                   vti.RegClass:$merge, vti.RegClass:$rs2, vti.RegClass:$rs1,
+                   (vti.Mask V0), GPR:$vl, vti.Log2SEW, TAIL_AGNOSTIC)>;
+    def : Pat<(vti.Vector (riscv_vrgather_vx_vl vti.RegClass:$rs2, GPR:$rs1,
+                                                vti.RegClass:$merge,
+                                                (vti.Mask V0),
+                                                VLOpFrag)),
+              (!cast<Instruction>("PseudoVRGATHER_VX_"# vti.LMul.MX#"_MASK")
+                   vti.RegClass:$merge, vti.RegClass:$rs2, GPR:$rs1,
+                   (vti.Mask V0), GPR:$vl, vti.Log2SEW, TAIL_AGNOSTIC)>;
+    def : Pat<(vti.Vector
+               (riscv_vrgather_vx_vl vti.RegClass:$rs2,
+                                     uimm5:$imm,
+                                     vti.RegClass:$merge,
+                                     (vti.Mask V0),
+                                     VLOpFrag)),
+              (!cast<Instruction>("PseudoVRGATHER_VI_"# vti.LMul.MX#"_MASK")
+                   vti.RegClass:$merge, vti.RegClass:$rs2, uimm5:$imm,
+                   (vti.Mask V0), GPR:$vl, vti.Log2SEW, TAIL_AGNOSTIC)>;
+  }
+
+  defvar vlmul = vti.LMul;
+  defvar octuple_lmul = vlmul.octuple;
+  defvar octuple_emul = !srl(!mul(octuple_lmul, 16), vti.Log2SEW);
+  if !and(!ge(octuple_emul, 1), !le(octuple_emul, 64)) then {
+    defvar emul_str = octuple_to_str<octuple_emul>.ret;
+    defvar ivti = !cast<VTypeInfo>("VI16" # emul_str);
+    defvar inst = "PseudoVRGATHEREI16_VV_" # vti.LMul.MX # "_E" # vti.SEW # "_" # emul_str;
+    let Predicates = !listconcat(GetVTypePredicates<vti>.Predicates,
+                                 GetVTypePredicates<ivti>.Predicates) in
+    def : Pat<(vti.Vector
+               (riscv_vrgatherei16_vv_vl vti.RegClass:$rs2,
+                                         (ivti.Vector ivti.RegClass:$rs1),
+                                         vti.RegClass:$merge,
+                                         (vti.Mask V0),
+                                         VLOpFrag)),
+              (!cast<Instruction>(inst#"_MASK")
+                   vti.RegClass:$merge, vti.RegClass:$rs2, ivti.RegClass:$rs1,
+                   (vti.Mask V0), GPR:$vl, vti.Log2SEW, TAIL_AGNOSTIC)>;
+  }
+}
diff --git a/llvm/lib/Target/RISCV/trytry.td b/llvm/lib/Target/RISCV/trytry.td
new file mode 100644
index 000000000..4b1be56c4
--- /dev/null
+++ b/llvm/lib/Target/RISCV/trytry.td
@@ -0,0 +1,342 @@
+
+// RISCVInstrInfoVPseudos.td
+// The actual table.
+def RISCVVPseudosTable : GenericTable {
+  let FilterClass = "RISCVVPseudo";
+  let FilterClassField = "NeedBeInPseudoTable";
+  let CppTypeName = "PseudoInfo";
+  let Fields = [ "Pseudo", "BaseInstr" ];
+  let PrimaryKey = [ "Pseudo" ];
+  let PrimaryKeyName = "getPseudoInfo";
+  let PrimaryKeyEarlyOut = true;
+}
+
+def RISCVVInversePseudosTable : GenericTable {
+  let FilterClass = "RISCVVPseudo";
+  let CppTypeName = "PseudoInfo";
+  let Fields = [ "Pseudo", "BaseInstr", "VLMul", "SEW"];
+  let PrimaryKey = [ "BaseInstr", "VLMul", "SEW"];
+  let PrimaryKeyName = "getBaseInfo";
+  let PrimaryKeyEarlyOut = true;
+}
+
+
+//===----------------------------------------------------------------------===//
+// VREV8 instruction format and definition in zvkb extension
+//===----------------------------------------------------------------------===//
+// RISCVInstrFormatsV.td
+class RVInstV<bits<6> funct6, bits<5> vs1, RISCVVFormat opv, dag outs,
+              dag ins, string opcodestr, string argstr>
+    : RVInst<outs, ins, opcodestr, argstr, [], InstFormatR> {
+  bits<5> vs2;
+  bits<5> vd;
+  bit vm;
+
+  let Inst{31-26} = funct6;
+  let Inst{25} = vm;
+  let Inst{24-20} = vs2;
+  let Inst{19-15} = vs1;
+  let Inst{14-12} = opv.Value; // 010
+  let Inst{11-7} = vd;
+  let Inst{6-0} = OPC_OP_V.Value;  // 1010111
+
+  let Uses = [VTYPE, VL];
+  let RVVConstraint = VMConstraint;
+}
+
+// RISCVInstrInfoV.td
+let hasSideEffects = 0, mayLoad = 1, mayStore = 0 in {
+  class VALUVs2<bits<6> funct6, bits<5> vs1, RISCVVFormat opv, string opcodestr>
+    : RVInstV<funct6, vs1, opv, (outs VR:$vd),
+                (ins VR:$vs2), VMaskOp:$vm,
+                opcodestr, "$vd, $vs2$vm">;
+}
+
+// RISCVInstrInfoZvk.td
+let Predicates = [HasStdExtZvkb] in {
+  def VREV8V : VALUVs2<0b010010, 0b01001, OPMVV, "vrev8.v">; 
+}
+
+
+//===----------------------------------------------------------------------===//
+// PseudoVREV8 instruction format and definition in zvkb extension
+//===----------------------------------------------------------------------===//
+
+//--------------------------------
+// Class related to VPseudoUnaryV_V
+//--------------------------------
+
+// RISCVInstrInfoVPseudos.td
+class PseudoToVInst<string PseudoInst> {
+  defvar AffixSubsts = [["Pseudo", ""],
+                        ["_E64", ""],
+                        ["_E32", ""],
+                        ["_E16", ""],
+                        ["_E8", ""],
+                        ["FPR64", "F"],
+                        ["FPR32", "F"],
+                        ["FPR16", "F"],
+                        ["_TIED", ""],
+                        ["_MASK", ""],
+                        ["_B64", ""],
+                        ["_B32", ""],
+                        ["_B16", ""],
+                        ["_B8", ""],
+                        ["_B4", ""],
+                        ["_B2", ""],
+                        ["_B1", ""],
+                        ["_MF8", ""],
+                        ["_MF4", ""],
+                        ["_MF2", ""],
+                        ["_M1", ""],
+                        ["_M2", ""],
+                        ["_M4", ""],
+                        ["_M8", ""],
+                        ["_SE", ""],
+                        ["_RM", ""]
+                       ];
+  string VInst = !foldl(PseudoInst, AffixSubsts, Acc, AffixSubst,
+                        !subst(AffixSubst[0], AffixSubst[1], Acc));
+}
+
+// RISCVInstrFormats.td
+class Pseudo<dag outs, dag ins, list<dag> pattern, string opcodestr = "", string argstr = "">
+    : RVInst<outs, ins, opcodestr, argstr, pattern, InstFormatPseudo> {
+  let isPseudo = 1;
+  let isCodeGenOnly = 1;
+}
+
+// This class holds the record of the RISCVVPseudoTable below.
+// This represents the information we need in codegen for each pseudo.
+// The definition should be consistent with `struct PseudoInfo` in
+// RISCVInstrInfo.h.
+
+// RISCVInstrInfoVPseudos.td
+class RISCVVPseudo {
+  Pseudo Pseudo = !cast<Pseudo>(NAME); // Used as a key.
+  Instruction BaseInstr = !cast<Instruction>(PseudoToVInst<NAME>.VInst);
+  // SEW = 0 is used to denote that the Pseudo is not SEW specific (or unknown).
+  bits<8> SEW = 0;
+  bit NeedBeInPseudoTable = 1;
+}
+
+// RISCVInstrInfoVPseudos.td
+class VPseudoUnaryNoMask<DAGOperand RetClass,
+                         DAGOperand OpClass,
+                         string Constraint = "",
+                         int TargetConstraintType = 1> :
+      Pseudo<(outs RetClass:$rd),
+             (ins RetClass:$merge, OpClass:$rs2,
+                  AVL:$vl, ixlenimm:$sew, ixlenimm:$policy), []>,
+      RISCVVPseudo {
+  let mayLoad = 0;
+  let mayStore = 0;
+  let hasSideEffects = 0;
+  let Constraints = !interleave([Constraint, "$rd = $merge"], ",");
+  let TargetOverlapConstraintType = TargetConstraintType;
+  let HasVLOp = 1;
+  let HasSEWOp = 1;
+  let HasVecPolicyOp = 1;
+}
+
+// RISCVInstrInfoVPseudos.td
+class VPseudoUnaryMask<VReg RetClass,
+                       VReg OpClass,
+                       string Constraint = "",
+                       int TargetConstraintType = 1> :
+      Pseudo<(outs GetVRegNoV0<RetClass>.R:$rd),
+             (ins GetVRegNoV0<RetClass>.R:$merge, OpClass:$rs2,
+                  VMaskOp:$vm, AVL:$vl, ixlenimm:$sew, ixlenimm:$policy), []>,
+      RISCVVPseudo {
+  let mayLoad = 0;
+  let mayStore = 0;
+  let hasSideEffects = 0;
+  let Constraints = !interleave([Constraint, "$rd = $merge"], ",");
+  let TargetOverlapConstraintType = TargetConstraintType;
+  let HasVLOp = 1;
+  let HasSEWOp = 1;
+  let HasVecPolicyOp = 1;
+  let UsesMaskPolicy = 1;
+}
+
+
+// Describes the relation of a masked pseudo to the unmasked variants.
+//    Note that all masked variants (in this table) have exactly one
+//    unmasked variant.  For all but compares, both the masked and
+//    unmasked variant have a passthru and policy operand.  For compares,
+//    neither has a policy op, and only the masked version has a passthru.
+
+// RISCVInstrInfoVPseudos.td
+class RISCVMaskedPseudo<bits<4> MaskIdx, bit ActiveAffectsRes=false> {
+  Pseudo MaskedPseudo = !cast<Pseudo>(NAME);
+  Pseudo UnmaskedPseudo = !cast<Pseudo>(!subst("_MASK", "", NAME));
+  bits<4> MaskOpIdx = MaskIdx;
+  bit ActiveElementsAffectResult = ActiveAffectsRes;
+}
+
+// RISCVInstrInfoVPseudos.td
+// This class describes information associated to the LMUL.
+class LMULInfo<int lmul, int oct, VReg regclass, VReg wregclass,
+               VReg f2regclass, VReg f4regclass, VReg f8regclass, string mx> {
+  bits<3> value = lmul; // This is encoded as the vlmul field of vtype.
+  VReg vrclass = regclass;
+  VReg wvrclass = wregclass;
+  VReg f8vrclass = f8regclass;
+  VReg f4vrclass = f4regclass;
+  VReg f2vrclass = f2regclass;
+  string MX = mx;
+  int octuple = oct;
+}
+
+// RISCVInstrInfoZvk.td
+multiclass VPseudoUnaryV_V<LMULInfo m> {
+  let VLMul = m.value in {
+    defvar suffix = "_V_" # m.MX;
+    def suffix : VPseudoUnaryNoMask<m.vrclass, m.vrclass>;
+    def suffix # "_MASK" : VPseudoUnaryMask<m.vrclass, m.vrclass>,
+                                            RISCVMaskedPseudo<MaskIdx=2>;
+  }
+}
+
+//--------------------------------
+// Class related to SchedUnary
+//--------------------------------
+
+// Common class of scheduling definitions.
+// `ReadVMergeOp` will be prepended to reads if instruction is masked.
+// `ReadVMask` will be appended to reads if instruction is masked.
+// Operands:
+//   `writes`       SchedWrites that are listed for each explicit def operand
+//                  in order.
+//   `reads`        SchedReads that are listed for each explicit use operand.
+//   `forceMasked`  Forced to be masked (e.g. Add-with-Carry Instructions).
+//   `forceMergeOpRead` Force to have read for merge operand.
+
+// RISCVInstrInfoV.td
+class SchedCommon<list<SchedWrite> writes, list<SchedRead> reads,
+                  string mx = "WorstCase", int sew = 0, bit forceMasked = 0,
+                  bit forceMergeOpRead = 0> : Sched<[]> {
+  defvar isMasked = !ne(!find(NAME, "_MASK"), -1);
+  defvar isMaskedOrForceMasked = !or(forceMasked, isMasked);
+  defvar mergeRead = !if(!or(!eq(mx, "WorstCase"), !eq(sew, 0)),
+                            !cast<SchedRead>("ReadVMergeOp_" # mx),
+                            !cast<SchedRead>("ReadVMergeOp_" # mx # "_E" #sew));
+  defvar needsMergeRead = !or(isMaskedOrForceMasked, forceMergeOpRead);
+  defvar readsWithMask =
+      !if(isMaskedOrForceMasked, !listconcat(reads, [ReadVMask]), reads);
+  defvar allReads =
+      !if(needsMergeRead, !listconcat([mergeRead], readsWithMask), reads);
+let SchedRW = !listconcat(writes, allReads);
+}
+
+// Common class of scheduling definitions for n-ary instructions.
+// The scheudling resources are relevant to LMUL and may be relevant to SEW.
+
+// RISCVInstrInfoV.td
+class SchedNary<string write, list<string> reads, string mx, int sew = 0,
+                bit forceMasked = 0, bit forceMergeOpRead = 0>
+    : SchedCommon<[!cast<SchedWrite>(
+                      !if(sew,
+                          write # "_" # mx # "_E" # sew,
+                          write # "_" # mx))],
+                  !foreach(read, reads,
+                           !cast<SchedRead>(!if(sew, read #"_" #mx #"_E" #sew,
+                                                 read #"_" #mx))),
+                  mx, sew, forceMasked, forceMergeOpRead>;
+
+class SchedUnary<string write, string read0, string mx, int sew = 0,
+                 bit forceMasked = 0, bit forceMergeOpRead = 0>:
+  SchedNary<write, [read0], mx, sew, forceMasked, forceMergeOpRead>;
+
+
+// RISCVInstrInfoZvk.td
+multiclass VPseudoVREV8 {
+  foreach m = MxList in {
+    defvar mx = m.MX;
+    defm "" : VPseudoUnaryV_V<m>,
+              SchedUnary<"WriteVREV8V", "ReadVREV8V", mx, forceMergeOpRead=true>;
+  }
+}
+
+let Predicates = [HasStdExtZvkb] in {
+  defm PseudoVREV8 : VPseudoVREV8;
+}
+
+
+//===----------------------------------------------------------------------===//
+// PseudoVREV8 Pattern Matching
+//===----------------------------------------------------------------------===//
+
+class VTypeInfo<ValueType Vec, ValueType Mas, int Sew, LMULInfo M,
+                ValueType Scal = XLenVT, RegisterClass ScalarReg = GPR> {
+  ValueType Vector = Vec;
+  ValueType Mask = Mas;
+  int SEW = Sew;
+  int Log2SEW = !logtwo(Sew);
+  VReg RegClass = M.vrclass;
+  LMULInfo LMul = M;
+  ValueType Scalar = Scal;
+  RegisterClass ScalarRegClass = ScalarReg;
+  // The pattern fragment which produces the AVL operand, representing the
+  // "natural" vector length for this type. For scalable vectors this is VLMax.
+  OutPatFrag AVL = VLMax;
+
+  string ScalarSuffix = !cond(!eq(Scal, XLenVT) : "X",
+                              !eq(Scal, f16) : "FPR16",
+                              !eq(Scal, bf16) : "FPR16",
+                              !eq(Scal, f32) : "FPR32",
+                              !eq(Scal, f64) : "FPR64");
+}
+
+defset list<VTypeInfo> AllVectors = {
+  defset list<VTypeInfo> AllIntegerVectors = {
+    defset list<VTypeInfo> NoGroupIntegerVectors = {
+      defset list<VTypeInfo> FractionalGroupIntegerVectors = {
+        def VI8MF8:  VTypeInfo<vint8mf8_t,  vbool64_t, 8,  V_MF8>;
+        def VI8MF4:  VTypeInfo<vint8mf4_t,  vbool32_t, 8,  V_MF4>;
+        def VI8MF2:  VTypeInfo<vint8mf2_t,  vbool16_t, 8,  V_MF2>;
+        def VI16MF4: VTypeInfo<vint16mf4_t, vbool64_t, 16, V_MF4>;
+        def VI16MF2: VTypeInfo<vint16mf2_t, vbool32_t, 16, V_MF2>;
+        def VI32MF2: VTypeInfo<vint32mf2_t, vbool64_t, 32, V_MF2>;
+      }
+      def VI8M1:  VTypeInfo<vint8m1_t,  vbool8_t,   8, V_M1>;
+      def VI16M1: VTypeInfo<vint16m1_t, vbool16_t, 16, V_M1>;
+      def VI32M1: VTypeInfo<vint32m1_t, vbool32_t, 32, V_M1>;
+      def VI64M1: VTypeInfo<vint64m1_t, vbool64_t, 64, V_M1>;
+    }
+    defset list<GroupVTypeInfo> GroupIntegerVectors = {
+      def VI8M2: GroupVTypeInfo<vint8m2_t, vint8m1_t, vbool4_t, 8, V_M2>;
+      def VI8M4: GroupVTypeInfo<vint8m4_t, vint8m1_t, vbool2_t, 8, V_M4>;
+      def VI8M8: GroupVTypeInfo<vint8m8_t, vint8m1_t, vbool1_t, 8, V_M8>;
+
+      def VI16M2: GroupVTypeInfo<vint16m2_t, vint16m1_t, vbool8_t, 16, V_M2>;
+      def VI16M4: GroupVTypeInfo<vint16m4_t, vint16m1_t, vbool4_t, 16, V_M4>;
+      def VI16M8: GroupVTypeInfo<vint16m8_t, vint16m1_t, vbool2_t, 16, V_M8>;
+
+      def VI32M2: GroupVTypeInfo<vint32m2_t, vint32m1_t, vbool16_t, 32, V_M2>;
+      def VI32M4: GroupVTypeInfo<vint32m4_t, vint32m1_t, vbool8_t,  32, V_M4>;
+      def VI32M8: GroupVTypeInfo<vint32m8_t, vint32m1_t, vbool4_t,  32, V_M8>;
+
+      def VI64M2: GroupVTypeInfo<vint64m2_t, vint64m1_t, vbool32_t, 64, V_M2>;
+      def VI64M4: GroupVTypeInfo<vint64m4_t, vint64m1_t, vbool16_t, 64, V_M4>;
+      def VI64M8: GroupVTypeInfo<vint64m8_t, vint64m1_t, vbool8_t,  64, V_M8>;
+    }
+  }
+}
+
+multiclass VPatUnarySDNode_V<SDPatternOperator op, string instruction_name,
+                             Predicate predicate = HasStdExtZvbb> {
+  foreach vti = AllIntegerVectors in {
+    let Predicates = !listconcat([predicate],
+                                 GetVTypePredicates<vti>.Predicates) in {
+      def : Pat<(vti.Vector (op (vti.Vector vti.RegClass:$rs1))),
+                (!cast<Instruction>(instruction_name#"_V_"#vti.LMul.MX)
+                   (vti.Vector (IMPLICIT_DEF)),
+                   vti.RegClass:$rs1,
+                   vti.AVL, vti.Log2SEW, TA_MA)>;
+    }
+  }
+}
+
+defm : VPatUnarySDNode_V<bswap, "PseudoVREV8", HasStdExtZvkb>;
+
diff --git a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
index 68363abdb..a181696de 100644
--- a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
+++ b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
@@ -52,7 +52,6 @@
 //  Vectorizing Compilers.
 //
 //===----------------------------------------------------------------------===//
-
 #include "llvm/Transforms/Vectorize/LoopVectorize.h"
 #include "LoopVectorizationPlanner.h"
 #include "VPRecipeBuilder.h"
@@ -152,9 +151,11 @@
 #include <limits>
 #include <map>
 #include <memory>
-#include <string>
 #include <tuple>
 #include <utility>
+#include "llvm/ADT/SmallString.h"
+#include <string>
+
 
 using namespace llvm;
 
@@ -164,6 +165,11 @@ using namespace llvm;
 #ifndef NDEBUG
 const char VerboseDebug[] = DEBUG_TYPE "-verbose";
 #endif
+std::string output_stream;
+SmallString<256> ttilog;
+#define STR(a) std::to_string(a.getValue().value())
+#define STRI(a) std::to_string(a)
+
 
 /// @{
 /// Metadata attribute names
@@ -1180,6 +1186,23 @@ public:
     CM_IntrinsicCall
   };
 
+  std::string GetDecisionName(const InstWidening &Decision){
+      switch (Decision)
+      {
+          case CM_Unknown: return "Unknown";
+          case CM_Widen: return "Widen";
+          case CM_Widen_Reverse: return "Widen_Reverse";
+          case CM_Interleave: return "Interleave";
+          case CM_GatherScatter: return "GatherScatter";
+          case CM_Scalarize: return "Scalarize";
+          case CM_VectorCall: return "VectorCall";
+          case CM_IntrinsicCall: return "IntrinsicCall";
+          default: return "Unknown";
+      }
+      
+  }
+
+
   /// Save vectorization decision \p W and \p Cost taken by the cost model for
   /// instruction \p I and vector width \p VF.
   void setWideningDecision(Instruction *I, ElementCount VF, InstWidening W,
@@ -1440,8 +1463,9 @@ public:
 
   /// Returns the TailFoldingStyle that is best for the current loop.
   TailFoldingStyle getTailFoldingStyle(bool IVUpdateMayOverflow = true) const {
-    if (!ChosenTailFoldingStyle)
+    if (!ChosenTailFoldingStyle){
       return TailFoldingStyle::None;
+    }
     return IVUpdateMayOverflow ? ChosenTailFoldingStyle->first
                                : ChosenTailFoldingStyle->second;
   }
@@ -4296,35 +4320,60 @@ bool LoopVectorizationPlanner::isMoreProfitable(
     const VectorizationFactor &A, const VectorizationFactor &B) const {
   InstructionCost CostA = A.Cost;
   InstructionCost CostB = B.Cost;
-
+  // Predicated Scalar Evolution
   unsigned MaxTripCount = PSE.getSE()->getSmallConstantMaxTripCount(OrigLoop);
 
   // Improve estimate for the vector width if it is scalable.
   unsigned EstimatedWidthA = A.Width.getKnownMinValue();
   unsigned EstimatedWidthB = B.Width.getKnownMinValue();
   if (std::optional<unsigned> VScale = getVScaleForTuning(OrigLoop, TTI)) {
-    if (A.Width.isScalable())
+    if (A.Width.isScalable()) {
+      llvm::outs() << "A is scalable.\t";
       EstimatedWidthA *= *VScale;
-    if (B.Width.isScalable())
+    } else {
+      llvm::outs() << "A is not scalable.\t";
+    }
+    if (B.Width.isScalable()) {
+      llvm::outs() << "B is scalable.\t";
       EstimatedWidthB *= *VScale;
+    } else {
+      llvm::outs() << "B is not scalable.\t";
+    }
+    llvm::outs() << "\n";
   }
 
   // Assume vscale may be larger than 1 (or the value being tuned for),
   // so that scalable vectorization is slightly favorable over fixed-width
   // vectorization.
-  bool PreferScalable = !TTI.preferFixedOverScalableIfEqualCost() &&
-                        A.Width.isScalable() && !B.Width.isScalable();
 
+  // Original code -> If false, then we prefer fixed-width vectorization over scalable vectorization
+  bool PreferScalable = !TTI.preferFixedOverScalableIfEqualCost() && A.Width.isScalable() && !B.Width.isScalable();
+  
+  // bool PreferScalable = true;
+  // if (A.Width.isScalable() && !B.Width.isScalable())
+  // {
+  //   PreferScalable = !TTI.preferFixedOverScalableIfEqualCost();
+  //   // PreferScalable = false;
+  // }
+  
   auto CmpFn = [PreferScalable](const InstructionCost &LHS,
                                 const InstructionCost &RHS) {
+    // The RHS is the fixed-width vectorization factor
     return PreferScalable ? LHS <= RHS : LHS < RHS;
   };
 
   // To avoid the need for FP division:
-  //      (CostA / EstimatedWidthA) < (CostB / EstimatedWidthB)
+  // 
+  // (CostA / EstimatedWidthA) < (CostB / EstimatedWidthB)
   // <=>  (CostA * EstimatedWidthB) < (CostB * EstimatedWidthA)
-  if (!MaxTripCount)
+  if (!MaxTripCount){
+    llvm::outs() << "A VF: " << A.Width << ", EstimatedWidthA: " << EstimatedWidthA << ", CostA: " << CostA << "\n";
+    llvm::outs() << "B VF: " << B.Width << ", EstimatedWidthB: " << EstimatedWidthB << ", CostB: " << CostB << "\n";
+    llvm::outs() << "CostA * EstimatedWidthB: " << CostA * EstimatedWidthB << ", CostB * EstimatedWidthA: " << CostB * EstimatedWidthA << "\n";
+    // The loop in this case maybe is a infinite loop, or a loop with trip count is not known at compile time
     return CmpFn(CostA * EstimatedWidthB, CostB * EstimatedWidthA);
+  }
+
 
   auto GetCostForTC = [MaxTripCount, this](unsigned VF,
                                            InstructionCost VectorCost,
@@ -4333,17 +4382,18 @@ bool LoopVectorizationPlanner::isMoreProfitable(
     // will be rounded up to an integer number of iterations under
     // FoldTailByMasking. The total cost in that case will be
     // VecCost*ceil(TripCount/VF). When not folding the tail, the total
-    // cost will be VecCost*floor(TC/VF) + ScalarCost*(TC%VF). There will be
-    // some extra overheads, but for the purpose of comparing the costs of
+    // cost will be VecCost*floor(TC/VF) -> Vector loop cost + ScalarCost*(TC%VF) -> Tail loop cost. 
+    // There will be some extra overheads, but for the purpose of comparing the costs of
     // different VFs we can use this to compare the total loop-body cost
     // expected after vectorization.
     if (CM.foldTailByMasking())
       return VectorCost * divideCeil(MaxTripCount, VF);
     return VectorCost * (MaxTripCount / VF) + ScalarCost * (MaxTripCount % VF);
   };
-
+  // RTCost is run-time cost
   auto RTCostA = GetCostForTC(EstimatedWidthA, CostA, A.ScalarCost);
   auto RTCostB = GetCostForTC(EstimatedWidthB, CostB, B.ScalarCost);
+  llvm::outs() << "RTCostA: " << RTCostA << ", RTCostB: " << RTCostB << "\n";
   return CmpFn(RTCostA, RTCostB);
 }
 
@@ -4513,6 +4563,7 @@ static bool willGenerateVectors(VPlan &Plan, ElementCount VF,
 VectorizationFactor LoopVectorizationPlanner::selectVectorizationFactor() {
   InstructionCost ExpectedCost = CM.expectedCost(ElementCount::getFixed(1));
   LLVM_DEBUG(dbgs() << "LV: Scalar loop costs: " << ExpectedCost << ".\n");
+  llvm::outs() << "LV: Scalar loop costs: " << ExpectedCost << ".";
   assert(ExpectedCost.isValid() && "Unexpected invalid cost for scalar loop");
   assert(any_of(VPlans,
                 [](std::unique_ptr<VPlan> &P) {
@@ -4523,7 +4574,7 @@ VectorizationFactor LoopVectorizationPlanner::selectVectorizationFactor() {
   const VectorizationFactor ScalarCost(ElementCount::getFixed(1), ExpectedCost,
                                        ExpectedCost);
   VectorizationFactor ChosenFactor = ScalarCost;
-
+  // FK means force kind of vectorization
   bool ForceVectorization = Hints.getForce() == LoopVectorizeHints::FK_Enabled;
   if (ForceVectorization &&
       (VPlans.size() > 1 || !VPlans[0]->hasScalarVFOnly())) {
@@ -4532,7 +4583,7 @@ VectorizationFactor LoopVectorizationPlanner::selectVectorizationFactor() {
     // evaluation.
     ChosenFactor.Cost = InstructionCost::getMax();
   }
-
+  // SmallVector is a LLVM version of std::vector
   SmallVector<InstructionVFPair> InvalidCosts;
   for (auto &P : VPlans) {
     for (ElementCount VF : P->vectorFactors()) {
@@ -4552,6 +4603,8 @@ VectorizationFactor LoopVectorizationPlanner::selectVectorizationFactor() {
               : Candidate.Width.getFixedValue();
       LLVM_DEBUG(dbgs() << "LV: Vector loop of width " << VF
                         << " costs: " << (Candidate.Cost / Width));
+      llvm::outs() << "LV: Vector loop of width " << VF
+                        << " costs: " << (Candidate.Cost / Width) << "\n";
       if (VF.isScalable())
         LLVM_DEBUG(dbgs() << " (assuming a minimum vscale of "
                           << AssumedMinimumVscale << ")");
@@ -4567,11 +4620,22 @@ VectorizationFactor LoopVectorizationPlanner::selectVectorizationFactor() {
       }
 
       // If profitable add it to ProfitableVF list.
-      if (isMoreProfitable(Candidate, ScalarCost))
+      // ScalarCostCandidateScalarCost
+      if (isMoreProfitable(Candidate, ScalarCost)){
+        llvm::outs() << "Candidate VF: " << Candidate.Width << " is more profitable than ScalarCost VF: " << ScalarCost.Width << "\n";
+        // ProfitableVFs will be used at selectEpilogueVectorizationFactor
         ProfitableVFs.push_back(Candidate);
+      }
+      
 
-      if (isMoreProfitable(Candidate, ChosenFactor))
+      // ChosenFactorCandidateChosenFactor
+      // ChosenFactor
+      if (isMoreProfitable(Candidate, ChosenFactor)){
+        llvm::outs() << "Update Chosen Factor with Candidate VF: " << Candidate.Width << " and ChosenFactor VF: " << ChosenFactor.Width << "\n";
         ChosenFactor = Candidate;
+      }
+      
+        
     }
   }
 
@@ -4589,7 +4653,12 @@ VectorizationFactor LoopVectorizationPlanner::selectVectorizationFactor() {
                  !isMoreProfitable(ChosenFactor, ScalarCost)) dbgs()
              << "LV: Vectorization seems to be not beneficial, "
              << "but was forced by a user.\n");
-  LLVM_DEBUG(dbgs() << "LV: Selecting VF: " << ChosenFactor.Width << ".\n");
+  // Final Cost
+  unsigned FinalWidth = ChosenFactor.Width.isScalable() ? ChosenFactor.Width.getKnownMinValue() * getVScaleForTuning(OrigLoop, TTI).value_or(1) : ChosenFactor.Width.getFixedValue();
+  InstructionCost FinalCost = ChosenFactor.Cost / FinalWidth;
+
+  LLVM_DEBUG(dbgs() << "LV: Selecting VF: " << ChosenFactor.Width  << " With Cost: " << FinalCost << ".\n");
+  llvm::outs() << "LV: Selecting VF: " << ChosenFactor.Width << " With Cost: " << FinalCost << ".\n";
   return ChosenFactor;
 }
 
@@ -4633,6 +4702,7 @@ bool LoopVectorizationCostModel::isEpilogueVectorizationProfitable(
   // with vectorization factors larger than a certain value.
 
   // Allow the target to opt out entirely.
+  // Base function is true 
   if (!TTI.preferEpilogueVectorization())
     return false;
 
@@ -4641,9 +4711,10 @@ bool LoopVectorizationCostModel::isEpilogueVectorizationProfitable(
   if (TTI.getMaxInterleaveFactor(VF) <= 1)
     return false;
 
+  // ARM will pass the above two conditions, the preferEpilogueVectorization return default true, and getMaxInterleaveFactor(VF) = 2
   unsigned Multiplier = 1;
   if (VF.isScalable())
-    Multiplier = getVScaleForTuning(TheLoop, TTI).value_or(1);
+    Multiplier = getVScaleForTuning(TheLoop, TTI).value_or(1); // ARM return 2, RISCV return 1 or 2 or nullopt
   if ((Multiplier * VF.getKnownMinValue()) >= EpilogueVectorizationMinVF)
     return true;
   return false;
@@ -4652,14 +4723,18 @@ bool LoopVectorizationCostModel::isEpilogueVectorizationProfitable(
 VectorizationFactor LoopVectorizationPlanner::selectEpilogueVectorizationFactor(
     const ElementCount MainLoopVF, unsigned IC) {
   VectorizationFactor Result = VectorizationFactor::Disabled();
+  // Default is enabled.
   if (!EnableEpilogueVectorization) {
     LLVM_DEBUG(dbgs() << "LEV: Epilogue vectorization is disabled.\n");
+    llvm::outs() << "LEV: Epilogue vectorization is disabled.\n";
     return Result;
   }
-
+  // epilogue vectorization
   if (!CM.isScalarEpilogueAllowed()) {
     LLVM_DEBUG(dbgs() << "LEV: Unable to vectorize epilogue because no "
                          "epilogue is allowed.\n");
+    llvm::outs() << "LEV: Unable to vectorize epilogue because no "
+                         "epilogue is allowed.\n";
     return Result;
   }
 
@@ -4668,17 +4743,22 @@ VectorizationFactor LoopVectorizationPlanner::selectEpilogueVectorizationFactor(
   if (!isCandidateForEpilogueVectorization(MainLoopVF)) {
     LLVM_DEBUG(dbgs() << "LEV: Unable to vectorize epilogue because the loop "
                          "is not a supported candidate.\n");
+    llvm::outs() << "LEV: Unable to vectorize epilogue because the loop "
+                         "is not a supported candidate.\n";
     return Result;
   }
 
   if (EpilogueVectorizationForceVF > 1) {
     LLVM_DEBUG(dbgs() << "LEV: Epilogue vectorization factor is forced.\n");
+    llvm::outs() << "LEV: Epilogue vectorization factor is forced.\n";
     ElementCount ForcedEC = ElementCount::getFixed(EpilogueVectorizationForceVF);
     if (hasPlanWithVF(ForcedEC))
       return {ForcedEC, 0, 0};
     else {
       LLVM_DEBUG(dbgs() << "LEV: Epilogue vectorization forced factor is not "
                            "viable.\n");
+      llvm::outs() << "LEV: Epilogue vectorization forced factor is not "
+                           "viable.\n";
       return Result;
     }
   }
@@ -4687,12 +4767,13 @@ VectorizationFactor LoopVectorizationPlanner::selectEpilogueVectorizationFactor(
       OrigLoop->getHeader()->getParent()->hasMinSize()) {
     LLVM_DEBUG(
         dbgs() << "LEV: Epilogue vectorization skipped due to opt for size.\n");
+    llvm::outs() << "LEV: Epilogue vectorization skipped due to opt for size.\n";
     return Result;
   }
 
   if (!CM.isEpilogueVectorizationProfitable(MainLoopVF)) {
-    LLVM_DEBUG(dbgs() << "LEV: Epilogue vectorization is not profitable for "
-                         "this loop\n");
+    LLVM_DEBUG(dbgs() << "LEV: Epilogue vectorization is not profitable for this loop\n");
+    llvm::outs() << "LEV: Epilogue vectorization is not profitable for this loop\n";
     return Result;
   }
 
@@ -4741,9 +4822,13 @@ VectorizationFactor LoopVectorizationPlanner::selectEpilogueVectorizationFactor(
       Result = NextVF;
   }
 
-  if (Result != VectorizationFactor::Disabled())
+  if (Result != VectorizationFactor::Disabled()){
     LLVM_DEBUG(dbgs() << "LEV: Vectorizing epilogue loop with VF = "
                       << Result.Width << "\n");
+    llvm::outs() << "LEV: Vectorizing epilogue loop with VF = "
+                      << Result.Width << " at function: " << OrigLoop->getHeader()->getParent()->getName() << " at line: " << OrigLoop->getLocStr() << "\n";
+  }
+   
   return Result;
 }
 
@@ -4835,7 +4920,7 @@ LoopVectorizationCostModel::selectInterleaveCount(ElementCount VF,
   // 1. If the code has reductions, then we interleave to break the cross
   // iteration dependency.
   // 2. If the loop is really small, then we interleave to reduce the loop
-  // overhead.
+  // overhead.  
   // 3. We don't interleave if we think that we will spill registers to memory
   // due to the increased register pressure.
 
@@ -4885,10 +4970,16 @@ LoopVectorizationCostModel::selectInterleaveCount(ElementCount VF,
   // We also want power of two interleave counts to ensure that the induction
   // variable of the vector loop wraps to zero, when tail is folded by masking;
   // this currently happens when OptForSize, in which case IC is set to 1 above.
+  // IC = (AvailableRegisters - LoopInvariantRegs) / RequiredRegisters
   unsigned IC = UINT_MAX;
-
+  
+  // MaxLocalUsers hold maximun number of concurrent live intervals in the loop.
+  // Live Interval refers to the range of a variable from its definition to its last use.
   for (auto& pair : R.MaxLocalUsers) {
+    // pair.first is the type of the register.
+    // pair.second is the number of registers that are used by the loop.
     unsigned TargetNumRegisters = TTI.getNumberOfRegisters(pair.first);
+    // llvm::outs() << "targetnumregs: " << TargetNumRegisters;
     LLVM_DEBUG(dbgs() << "LV: The target has " << TargetNumRegisters
                       << " registers of "
                       << TTI.getRegisterClassName(pair.first) << " register class\n");
@@ -4903,7 +4994,11 @@ LoopVectorizationCostModel::selectInterleaveCount(ElementCount VF,
     unsigned LoopInvariantRegs = 0;
     if (R.LoopInvariantRegs.find(pair.first) != R.LoopInvariantRegs.end())
       LoopInvariantRegs = R.LoopInvariantRegs[pair.first];
-
+    // 
+    // TargetNumRegisters: 
+    // LoopInvariantRegs: 
+    // MaxLocalUsers: 
+    // bit_floor:  2 
     unsigned TmpIC = llvm::bit_floor((TargetNumRegisters - LoopInvariantRegs) /
                                      MaxLocalUsers);
     // Don't count the induction variable as interleaved.
@@ -4917,16 +5012,18 @@ LoopVectorizationCostModel::selectInterleaveCount(ElementCount VF,
 
   // Clamp the interleave ranges to reasonable counts.
   unsigned MaxInterleaveCount = TTI.getMaxInterleaveFactor(VF);
-
+  llvm::outs() << "maxbefore: " << MaxInterleaveCount << "\n";
   // Check if the user has overridden the max.
-  if (VF.isScalar()) {
-    if (ForceTargetMaxScalarInterleaveFactor.getNumOccurrences() > 0)
+  if (VF.isScalar()) { 
+    // Check if the user has overridden the max.
+    if (ForceTargetMaxScalarInterleaveFactor.getNumOccurrences() > 0) 
       MaxInterleaveCount = ForceTargetMaxScalarInterleaveFactor;
   } else {
+    // Check if the user has overridden the max.
     if (ForceTargetMaxVectorInterleaveFactor.getNumOccurrences() > 0)
       MaxInterleaveCount = ForceTargetMaxVectorInterleaveFactor;
   }
-
+  llvm::outs() << "maxafter: " << MaxInterleaveCount << "\n";
   unsigned EstimatedVF = VF.getKnownMinValue();
   if (VF.isScalable()) {
     if (std::optional<unsigned> VScale = getVScaleForTuning(TheLoop, TTI))
@@ -5488,6 +5585,13 @@ InstructionCost LoopVectorizationCostModel::expectedCost(
   // For each block.
   for (BasicBlock *BB : TheLoop->blocks()) {
     InstructionCost BlockCost;
+    llvm::outs() << "\n-----------------Function that is being costed:'"
+                    << TheLoop->getHeader()->getParent()->getName() << "' from "
+                    << TheLoop->getLocStr() << "-----------------\n";
+    LLVM_DEBUG(dbgs() << "\n-----------------Function that is being costed:'"
+                    << TheLoop->getHeader()->getParent()->getName() << "' from "
+                    << TheLoop->getLocStr() << "-----------------\n");
+    
 
     // For each instruction in the old loop.
     for (Instruction &I : BB->instructionsWithoutDebug()) {
@@ -5495,9 +5599,10 @@ InstructionCost LoopVectorizationCostModel::expectedCost(
       if (ValuesToIgnore.count(&I) ||
           (VF.isVector() && VecValuesToIgnore.count(&I)))
         continue;
+      
+     
 
       InstructionCost C = getInstructionCost(&I, VF);
-
       // Check if we should override the cost.
       if (C.isValid() && ForceTargetInstructionCost.getNumOccurrences() > 0)
         C = InstructionCost(ForceTargetInstructionCost);
@@ -5507,8 +5612,10 @@ InstructionCost LoopVectorizationCostModel::expectedCost(
         Invalid->emplace_back(&I, VF);
 
       BlockCost += C;
+      llvm::outs() << "LV: Found an estimated cost of " << C << " for VF "
+                    << VF << " For instruction: " << I << " of type:" << I.getOpcodeName() << '\n';
       LLVM_DEBUG(dbgs() << "LV: Found an estimated cost of " << C << " for VF "
-                        << VF << " For instruction: " << I << '\n');
+                        << VF << " For instruction: " << I << " of type:" << I.getOpcodeName() << '\n');
     }
 
     // If we are vectorizing a predicated block, it will have been
@@ -5620,33 +5727,72 @@ LoopVectorizationCostModel::getMemInstScalarizationCost(Instruction *I,
 InstructionCost
 LoopVectorizationCostModel::getConsecutiveMemOpCost(Instruction *I,
                                                     ElementCount VF) {
+  ttilog = "";
   Type *ValTy = getLoadStoreType(I);
   auto *VectorTy = cast<VectorType>(ToVectorTy(ValTy, VF));
   Value *Ptr = getLoadStorePointerOperand(I);
   unsigned AS = getLoadStoreAddressSpace(I);
   int ConsecutiveStride = Legal->isConsecutivePtr(ValTy, Ptr);
   enum TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput;
-
   assert((ConsecutiveStride == 1 || ConsecutiveStride == -1) &&
          "Stride should be 1 or -1 for consecutive memory access");
   const Align Alignment = getLoadStoreAlignment(I);
   InstructionCost Cost = 0;
   if (Legal->isMaskRequired(I)) {
-    Cost += TTI.getMaskedMemoryOpCost(I->getOpcode(), VectorTy, Alignment, AS,
-                                      CostKind);
+    InstructionCost MaskedMemoryOpCost = TTI.getMaskedMemoryOpCost(I->getOpcode(), VectorTy, Alignment, AS, CostKind);
+    if (MaskedMemoryOpCost.isValid()) {
+      ttilog += "MaskedMemoryOpCost(" + STR(MaskedMemoryOpCost) + ")";
+    }
+    Cost += MaskedMemoryOpCost;
   } else {
     TTI::OperandValueInfo OpInfo = TTI::getOperandInfo(I->getOperand(0));
-    Cost += TTI.getMemoryOpCost(I->getOpcode(), VectorTy, Alignment, AS,
-                                CostKind, OpInfo, I);
+    InstructionCost MemoryOpCost = TTI.getMemoryOpCost(I->getOpcode(), VectorTy, Alignment, AS, CostKind, OpInfo, I);
+    if (MemoryOpCost.isValid()) {
+      ttilog += "MemoryOpCost(" + STR(MemoryOpCost) + ")";
+    }
+    Cost += MemoryOpCost;
   }
 
   bool Reverse = ConsecutiveStride < 0;
-  if (Reverse)
-    Cost += TTI.getShuffleCost(TargetTransformInfo::SK_Reverse, VectorTy,
+  if (Reverse){
+    InstructionCost ShuffleCost = TTI.getShuffleCost(TargetTransformInfo::SK_Reverse, VectorTy,
                                std::nullopt, CostKind, 0);
+    Cost += ShuffleCost;
+  }
+  llvm::outs() <<"@@ Instruction =>" << *I << " -> Cost: " << Cost << " -> VectorType: " << *VectorTy << " -> ttilog -> " << ttilog << "\n";
   return Cost;
 }
 
+// InstructionCost
+// LoopVectorizationCostModel::getConsecutiveMemOpCost(Instruction *I,
+//                                                     ElementCount VF) {
+//   Type *ValTy = getLoadStoreType(I);
+//   auto *VectorTy = cast<VectorType>(ToVectorTy(ValTy, VF));
+//   Value *Ptr = getLoadStorePointerOperand(I);
+//   unsigned AS = getLoadStoreAddressSpace(I);
+//   int ConsecutiveStride = Legal->isConsecutivePtr(ValTy, Ptr);
+//   enum TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput;
+
+//   assert((ConsecutiveStride == 1 || ConsecutiveStride == -1) &&
+//          "Stride should be 1 or -1 for consecutive memory access");
+//   const Align Alignment = getLoadStoreAlignment(I);
+//   InstructionCost Cost = 0;
+//   if (Legal->isMaskRequired(I)) {
+//     Cost += TTI.getMaskedMemoryOpCost(I->getOpcode(), VectorTy, Alignment, AS,
+//                                       CostKind);
+//   } else {
+//     TTI::OperandValueInfo OpInfo = TTI::getOperandInfo(I->getOperand(0));
+//     Cost += TTI.getMemoryOpCost(I->getOpcode(), VectorTy, Alignment, AS,
+//                                 CostKind, OpInfo, I);
+//   }
+
+//   bool Reverse = ConsecutiveStride < 0;
+//   if (Reverse)
+//     Cost += TTI.getShuffleCost(TargetTransformInfo::SK_Reverse, VectorTy,
+//                                std::nullopt, CostKind, 0);
+//   return Cost;
+// }
+
 InstructionCost
 LoopVectorizationCostModel::getUniformMemOpCost(Instruction *I,
                                                 ElementCount VF) {
@@ -6325,6 +6471,7 @@ LoopVectorizationCostModel::getInstructionCost(Instruction *I,
   if (isUniformAfterVectorization(I, VF))
     VF = ElementCount::getFixed(1);
 
+  // Check if it's better to not vectorize this specific instruction
   if (VF.isVector() && isProfitableToScalarize(I, VF))
     return InstsToScalarize[VF][I];
 
@@ -6336,8 +6483,11 @@ LoopVectorizationCostModel::getInstructionCost(Instruction *I,
       return getInstructionCost(I, ElementCount::getFixed(1)) *
              VF.getKnownMinValue();
   }
-
+  // Get the result type of the instruction
   Type *RetTy = I->getType();
+
+  // Check if we can use a smaller data type (truncate).
+  // Using a smaller type can sometines be cheaper
   if (canTruncateToMinimalBitwidth(I, VF))
     RetTy = IntegerType::get(RetTy->getContext(), MinBWs[I]);
   auto SE = PSE.getSE();
@@ -6559,6 +6709,8 @@ LoopVectorizationCostModel::getInstructionCost(Instruction *I,
     ElementCount Width = VF;
     if (Width.isVector()) {
       InstWidening Decision = getWideningDecision(I, Width);
+      llvm::outs() << "LV: Decision: " << GetDecisionName(Decision) << "\n";
+
       assert(Decision != CM_Unknown &&
              "CM decision should be taken at this point");
       if (getWideningCost(I, VF) == InstructionCost::getInvalid())
@@ -6864,6 +7016,7 @@ LoopVectorizationPlanner::plan(ElementCount UserVF, unsigned UserIC) {
     return std::nullopt;
 
   // Invalidate interleave groups if all blocks of loop will be predicated.
+  // blocks 
   if (CM.blockNeedsPredicationForAnyReason(OrigLoop->getHeader()) &&
       !useMaskedInterleavedAccesses(TTI)) {
     LLVM_DEBUG(
@@ -7288,7 +7441,13 @@ LoopVectorizationPlanner::executePlan(
 
   LLVM_DEBUG(dbgs() << "Executing best plan with VF=" << BestVF
                     << ", UF=" << BestUF << '\n');
+  llvm::outs() << "Executing best plan with VF=" << BestVF
+               << ", UF=" << BestUF << " at function: " << OrigLoop->getHeader()->getParent()->getName() << " at line: " << OrigLoop->getLocStr() << "\n";
+  
   BestVPlan.setName("Final VPlan");
+  llvm::outs() << "================ Final VPlan ================\n";
+  BestVPlan.print(llvm::outs());
+  llvm::outs() << "================ Final VPlan ================\n\n";
   LLVM_DEBUG(BestVPlan.dump());
 
   // Perform the actual loop transformation.
@@ -7711,6 +7870,7 @@ void EpilogueVectorizerEpilogueLoop::printDebugTracesAtEnd() {
   });
 }
 
+///  decision  VF 
 bool LoopVectorizationPlanner::getDecisionAndClampRange(
     const std::function<bool(ElementCount)> &Predicate, VFRange &Range) {
   assert(!Range.isEmpty() && "Trying to test an empty VF range.");
@@ -8322,7 +8482,6 @@ VPRecipeBuilder::tryToCreateWidenRecipe(Instruction *Instr,
 void LoopVectorizationPlanner::buildVPlansWithVPRecipes(ElementCount MinVF,
                                                         ElementCount MaxVF) {
   assert(OrigLoop->isInnermost() && "Inner loop expected.");
-
   auto MaxVFTimes2 = MaxVF * 2;
   for (ElementCount VF = MinVF; ElementCount::isKnownLT(VF, MaxVFTimes2);) {
     VFRange SubRange = {VF, MaxVFTimes2};
@@ -8702,12 +8861,19 @@ LoopVectorizationPlanner::tryToBuildVPlanWithVPRecipes(VFRange &Range) {
   if (useActiveLaneMask(Style)) {
     // TODO: Move checks to VPlanTransforms::addActiveLaneMask once
     // TailFoldingStyle is visible there.
-    bool ForControlFlow = useActiveLaneMaskForControlFlow(Style);
-    bool WithoutRuntimeCheck =
+    bool ForControlFlow = useActiveLaneMaskForControlFlow(Style); // 0
+    bool WithoutRuntimeCheck =                                    // 0
         Style == TailFoldingStyle::DataAndControlFlowWithoutRuntimeCheck;
     VPlanTransforms::addActiveLaneMask(*Plan, ForControlFlow,
                                        WithoutRuntimeCheck);
   }
+  llvm::outs() << "========== Loop: " << OrigLoop->getHeader()->getParent()->getName() << "' from "
+                    << OrigLoop->getLocStr() << "==========\n";
+  llvm::outs() << "========== VPlan for Vector Factor Range: " << Range.Start.getKnownMinValue()
+             << " to " << Range.End.getKnownMinValue() << "==========\n";
+
+  Plan->print(llvm::outs());
+  llvm::outs() << "\n";
   return Plan;
 }
 
@@ -9817,6 +9983,7 @@ bool LoopVectorizePass::processLoop(Loop *L) {
       hasBranchWeightMD(*L->getLoopLatch()->getTerminator());
   GeneratedRTChecks Checks(*PSE.getSE(), DT, LI, TTI,
                            F->getDataLayout(), AddBranchWeights);
+  //  IC  runtime check 
   if (MaybeVF) {
     VF = *MaybeVF;
     // Select the interleave count.
@@ -9850,6 +10017,7 @@ bool LoopVectorizePass::processLoop(Loop *L) {
   // Identify the diagnostic messages that should be produced.
   std::pair<StringRef, std::string> VecDiagMsg, IntDiagMsg;
   bool VectorizeLoop = true, InterleaveLoop = true;
+  //  if statement  VectorizeLoop  InterleaveLoop bool 
   if (VF.Width.isScalar()) {
     LLVM_DEBUG(dbgs() << "LV: Vectorization is possible but not beneficial.\n");
     VecDiagMsg = std::make_pair(
@@ -9930,6 +10098,7 @@ bool LoopVectorizePass::processLoop(Loop *L) {
   }
 
   bool DisableRuntimeUnroll = false;
+  // MDNode means metadata node
   MDNode *OrigLoopID = L->getLoopID();
   {
     using namespace ore;
@@ -9958,6 +10127,7 @@ bool LoopVectorizePass::processLoop(Loop *L) {
       // Consider vectorizing the epilogue too if it's profitable.
       VectorizationFactor EpilogueVF =
           LVP.selectEpilogueVectorizationFactor(VF.Width, IC);
+      llvm::outs() << "LEV: EpilogueVF.Width: " << EpilogueVF.Width << "\n";
       if (EpilogueVF.Width.isVector()) {
 
         // The first pass vectorizes the main loop and creates a scalar epilogue
@@ -10150,6 +10320,7 @@ LoopVectorizeResult LoopVectorizePass::runImpl(
   // Build up a worklist of inner-loops to vectorize. This is necessary as
   // the act of vectorizing or partially unrolling a loop creates new loops
   // and can invalidate iterators across the loops.
+  // loops Loop  Loop  vectorization
   SmallVector<Loop *, 8> Worklist;
 
   for (Loop *L : *LI)
@@ -10229,6 +10400,11 @@ PreservedAnalyses LoopVectorizePass::run(Function &F,
     } else {
       PA.preserveSet<CFGAnalyses>();
     }
+    llvm::outs() << "================================================\n";
+    llvm::outs() << "Loops Vectorized: " << LoopsVectorized << "\n"
+                 << "Loops Analyzed: " << LoopsAnalyzed << "\n"
+                 << "Loops Epilogues Vectorized: " << LoopsEpilogueVectorized << "\n";
+    llvm::outs() << "================================================\n";
     return PA;
 }
 
